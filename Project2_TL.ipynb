{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brilliant-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import everything we need for the project\n",
    "\n",
    "%matplotlib inline\n",
    "import os, time\n",
    "import csv\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix, classification_report # Helps present results as a confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-external",
   "metadata": {},
   "source": [
    "## Writing CSV - ASL Dataset\n",
    "Since this dataset doesn't come with a nice csv, write one ourselves to make loading the data easier later\n",
    "Note: directly setting filepaths to the pre-binarized images so we don't need to perform this operation ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "obvious-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"path_to_file\", \"GT\"]\n",
    "csv_path = \"./data/asl/asl_images.csv\"\n",
    "imagepaths = []\n",
    "fnames = []\n",
    "# these are 400x400 BW images\n",
    "root = \"./data/asl/asl_data/binary_frames_rotated\"\n",
    "\n",
    "for dirname, dirs, files in os.walk(root):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".png\"):\n",
    "            fnames.append(fname)\n",
    "        path = os.path.join(dirname, fname)\n",
    "        if path.endswith(\".png\"):\n",
    "            imagepaths.append(path)\n",
    "\n",
    "gt = [fname.split('_')[0] for fname in fnames]\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for i, fpath in enumerate(imagepaths):\n",
    "        writer.writerow([fpath, gt[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-interaction",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "To facilitate using pytorch to build our cnn, we write a custom DataLoader class. This allows for on-demand loading of images, which are used to train our cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cutting-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslGestureDataset(Dataset):\n",
    "    \"\"\"Custom loader for the Kaggle Hand Detection Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image filepaths and gt classes\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.images_frame.iloc[idx, 0]\n",
    "        #print(img_name)\n",
    "        img = cv2.imread(img_name)\n",
    "        y = (self.images_frame.iloc[idx, 1])\n",
    "        # normalize to 0-26 for classes (missing j and z b/c dynamic)\n",
    "        y = ord(y)- ord('a')\n",
    "        sample = {'image' : img, 'y' : y, 'fname' : img_name}\n",
    "        \n",
    "#         print(sample)\n",
    "        \n",
    "        if len(self.transforms) > 0:\n",
    "            for _, transform in enumerate(self.transforms):\n",
    "                sample = transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "collectible-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Used to rescale an image to a given size. Useful for the CNN\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size after rescaling. If tuple, output is matched to output_size.\n",
    "        If int smaller of width/height is matched to output_size, keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h // w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w // h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        img = cv2.resize(image, (new_h, new_w))\n",
    "        #print(f\"after resize: {img.shape}\")\n",
    "        return {'image' : img, 'y' : y, 'fname' : sample['fname']}\n",
    "    \n",
    "class Recolor(object):\n",
    "    \"\"\"Used to recolor an image using cv2\n",
    "    \n",
    "    Args:\n",
    "        flag (cv2.COLOR_): color to swap to\n",
    "    \"\"\"\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        #print(f\"before recolor: {image.shape}\")\n",
    "        \n",
    "        # cvtColor to gray drops the damned channel dimension but we need it\n",
    "        img_cvt = cv2.cvtColor(image, self.color)\n",
    "        # fucking hack an extra dim to appease pytorch's bitchass\n",
    "        img_cvt = np.expand_dims(img_cvt, axis=-1)\n",
    "        #print(f\"after exansion: {img_cvt.shape}\")\n",
    "        return {'image' : img_cvt, 'y' : y, 'fname' : sample['fname']}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays to pytorch Tensors\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        # swap color axis b/c \n",
    "        # numpy img: H x W x C\n",
    "        # torch img: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image' : torch.from_numpy(image), 'y' : y, 'fname' : sample['fname']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "split-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_class_str(pred):\n",
    "    classes = {0 : \"palm\", 1 : \"L\", 2 : \"fist\", 3 : \"fist_moved\", 4 : \"thumb\", 5 : \"index\", 6 : \"ok\", 7 : \"palm_moved\", 8 : \"c\", 9 : \"down\"}\n",
    "    return classes[pred]\n",
    "\n",
    "def classify_arbitrary_image(model, img):\n",
    "    img_type = type(img)\n",
    "    print(img_type)\n",
    "    if img_type == torch.Tensor:\n",
    "        print(\"tensor\")\n",
    "        img = img.float()\n",
    "        img = img.unsqueeze(1)\n",
    "    elif img_type == np.ndarray:\n",
    "        print(\"np array\")\n",
    "        img = np.expand_dims(img, 1)\n",
    "    else:\n",
    "        print(\"error: something other than a torch.Tensor or an np.ndarray was passed as img\")\n",
    "    prediction = model(img)\n",
    "    prediction = prediction.data.numpy()\n",
    "    y_hat = np.argmax(prediction, axis=1)\n",
    "    return prediction_to_class_str(y_hat[0])\n",
    "\n",
    "def classify_many_images(model, imgs):\n",
    "    # for now, assuming imgs is a list of images that are either np.ndarrays or torch.Tensors\n",
    "    # labels will be given back in order images were given\n",
    "    predictions = []\n",
    "    for img in imgs:\n",
    "        predictions.append(classify_arbitrary_image(model, img))\n",
    "    return predictions\n",
    "\n",
    "def get_model_acc(model, data_loader):\n",
    "    total_samples = 0\n",
    "    total_misclass = 0\n",
    "    all_y = np.array([], dtype=np.uint8)\n",
    "    all_y_hat = np.array([], dtype=np.uint8)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        model = model.cpu()\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        y = sample['y']\n",
    "        y = y.data.numpy()\n",
    "        images = (sample['image'])\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            \n",
    "        images = images.float()\n",
    "        predictions = model(images)\n",
    "        predictions = predictions.cpu()\n",
    "        predictions = predictions.data.numpy()\n",
    "        y_hat = np.argmax(predictions, axis=1)\n",
    "        misclass = np.sum(np.where(y != y_hat, 1, 0))\n",
    "        total_samples += y.shape[0]\n",
    "        total_misclass += misclass\n",
    "        all_y = np.append(all_y, y)\n",
    "        all_y_hat = np.append(all_y_hat, y_hat)\n",
    "        #print(f\"all_y = {all_y}\")\n",
    "        #print(f\"all_y_hat = {all_y_hat}\")\n",
    "        print(f\"Number of Misclassifications = {misclass}\")\n",
    "        print(f\"Sample acc = {(y.shape[0]-misclass)/y.shape[0]*100}\")\n",
    "    overall_acc = (total_samples - total_misclass)/total_samples\n",
    "    print(f\"Overall Accuracy = {overall_acc}\")\n",
    "    return all_y, all_y_hat, overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "quiet-frederick",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HandNNModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (32, 256, 256) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (252, 252, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (121, 121, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (119, 119, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (59, 59, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (57, 57, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (28, 28, 64)\n",
    "        self.fc1 = Linear(28*28*64, 128) # output shape: (28*28*64, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "actual-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslNNModel(Module):\n",
    "    # same structure as HandNNModel, need to change dimensions\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (64, 400, 400) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (496, 496, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (198, 198, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (196, 196, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (98, 98, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (96, 96, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (48, 48, 64)\n",
    "        self.fc1 = Linear(48*48*64, 128) # output shape: (48*48*64, 128)\n",
    "        self.fc2 = Linear(128, 26) # 24 possible output classes, but it goes up to idx 26: CUDA screams otherwise, so here we are\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "under-palestinian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_asl_count_df(y):\n",
    "    counts = np.zeros(26)\n",
    "    y = np.array(y)\n",
    "    for i in range(26):\n",
    "        counts[i] = np.sum(np.where(y == i, 1, 0))\n",
    "    idx = [chr(i) for i in range(97, 123)]\n",
    "    columns=[\"Count\"]\n",
    "    df = pd.DataFrame(counts, index=idx, columns=columns)\n",
    "    return df\n",
    "\n",
    "def y_as_np_arr(dataset):\n",
    "    return np.array([sample['y'] for sample in dataset])\n",
    "\n",
    "\n",
    "def make_asl_train_test_split(dataset, counts_df, split_ratio=0.75, train_csv=\"./data/asl/train_asl.csv\", test_csv=\"./data/asl/test_asl.csv\"):\n",
    "    header = [\"path_to_file\", \"GT\"]\n",
    "    train = []\n",
    "    test = []\n",
    "    np.random.seed(0)\n",
    "    train_counts, test_counts = [int(np.ceil(split_ratio*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])], [int(np.floor((1-split_ratio)*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])]\n",
    "    y = y_as_np_arr(dataset)\n",
    "    for idx in range(counts_df.shape[0]):\n",
    "        #curr_train_selections = []\n",
    "        #curr_test_selections = []\n",
    "        only_class_locs = np.where(y==idx)[0]\n",
    "        train_idxes = np.random.choice(only_class_locs, size=train_counts[idx], replace=False)\n",
    "        for data_idx in only_class_locs:\n",
    "            sample = dataset[data_idx]\n",
    "            if data_idx in train_idxes:\n",
    "                #curr_train_selections.append(sample['fname'])\n",
    "                train.append(sample['fname'])\n",
    "            else:\n",
    "                #curr_test_selections.append(sample['fname'])\n",
    "                test.append(sample['fname'])\n",
    "        #train.append(curr_train_selections)\n",
    "        #test.append(curr_test_selections)\n",
    "#     train_lasts = [fname.split('/')[-1] for fname in train]\n",
    "    train_gt = [os.path.split(fname)[-1].split('_')[0] for fname in train]\n",
    "#     test_lasts = [fname.split('/')[-1] for fname in test]\n",
    "    test_gt = [os.path.split(fname)[-1].split('_')[0] for fname in test]\n",
    "    with open(train_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(train):\n",
    "            writer.writerow([fpath, train_gt[i]])\n",
    "    with open(test_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(test):\n",
    "            writer.writerow([fpath, test_gt[i]])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "#print(test)\n",
    "\n",
    "#lasts = [fname.split('/')[-1] for fname in test]\n",
    "#gt = [fname.split('_')[0] for fname in lasts]\n",
    "#print(len(gt))\n",
    "#train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "#print(len(train_asl_dataset))\n",
    "#plt.imshow(np.squeeze(test[0][0]['image'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "educated-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = Rescale((256,256))\n",
    "recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "to_tensor = ToTensor()\n",
    "transforms = [resize, recolor, to_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adult-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "asl_dataset = AslGestureDataset(csv_file=\"./data/asl/asl_images.csv\", transforms=[recolor, to_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dress-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_as_np_arr(asl_dataset)\n",
    "df = make_asl_count_df(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "becoming-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = make_asl_train_test_split(asl_dataset, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-shirt",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Report (from sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "personal-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "alphabet = [chr(l) for l in range(ord('a'), ord('z') +1)]\n",
    "alphabet_reduced = [chr(l) for l in range(ord('a'), ord('z') +1)]\n",
    "alphabet_reduced.remove(\"j\")\n",
    "alphabet_reduced.remove(\"z\")\n",
    "\n",
    "def make_stats(y, y_hat, num_classes=10):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    cm_df = pd.DataFrame(cm, columns=[str(i) for i in range(num_classes)])\n",
    "    report = classification_report(y, y_hat)\n",
    "    if num_classes == 24:\n",
    "        cm_df.index = alphabet_reduced\n",
    "        cm_df.columns = alphabet_reduced\n",
    "    else:\n",
    "        cm_df.index = alphabet[:num_classes]\n",
    "        cm_df.columns = alphabet[:num_classes]\n",
    "#     print(cm_df.to_markdown())\n",
    "    for cl, lt in enumerate(alphabet):\n",
    "        cl_str = f' {cl} '\n",
    "        lt_str = f' {lt} '\n",
    "        lt_str = ' '*(len(cl_str) - len(lt_str)) + lt_str\n",
    "        report = report.replace(cl_str, lt_str)\n",
    "        \n",
    "    return cm_df, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-gamma",
   "metadata": {},
   "source": [
    "## Train the model using the Custom Dataloader\n",
    "Below, we will actually train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "instructional-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "grand-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 48\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    params = {'batch_size' : batch, 'shuffle': True, 'num_workers' : 0, 'pin_memory' : True}\n",
    "else:\n",
    "    params = {'batch_size' : batch, 'shuffle': True, 'num_workers' : 0}\n",
    "def train_model(model, train_data_loader, test_data_loader, max_epochs, use_cuda, save_path='trained_model.pkl', save=True):\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    print(model.eval())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"start epoch {epoch}\")\n",
    "        running_loss_train = 0.0\n",
    "        count_train = 0\n",
    "        epoch_start_train = time.time()\n",
    "        for i, data in enumerate(train_data_loader):\n",
    "            imgs, labels = data['image'], data['y']\n",
    "            # move to GPU\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            imgs = imgs.float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item()\n",
    "            count_train = i + 1\n",
    "            if i % 50 == 0:\n",
    "                print(f\"epoch {epoch} {count_train * batch}: train running loss = {running_loss_train/count_train}\")\n",
    "                print(f\"epoch {epoch} {count_train * batch}: train testing runtime = {time.time() - epoch_start_train}\")\n",
    "        epoch_end_train = time.time()\n",
    "        print(f\"epoch {epoch}  {count_train * batch}: Train Final loss = {running_loss_train/count_train}\")\n",
    "        train_loss.append(running_loss_train/count_train)\n",
    "        print(f\"epoch {epoch}  {count_train * batch}: Train runtime = {epoch_end_train - epoch_start_train}\")\n",
    "        # ====================Test data=================\n",
    "        running_loss_test = 0.0\n",
    "        count_test = 0\n",
    "        epoch_start_test = time.time()\n",
    "        for i, data in enumerate(test_data_loader):\n",
    "            imgs, labels = data['image'], data['y']\n",
    "            # move to GPU\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            imgs = imgs.float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss_test += loss.item()\n",
    "            count_test = i + 1\n",
    "            if i % 50 == 0:\n",
    "                print(f\"epoch {epoch} {count_test * batch}: testing running loss = {running_loss_test/count_test}\")\n",
    "                print(f\"epoch {epoch} {count_test * batch}: testing runtime = {time.time() - epoch_start_test}\")\n",
    "        epoch_end_test = time.time()\n",
    "        print(f\"epoch {epoch} {count_test * batch}: Testing Final loss = {running_loss_test/count_test}\")\n",
    "        test_loss.append(running_loss_test/count_test)\n",
    "        print(f\"epoch {epoch} {count_test * batch}: Testing runtime = {epoch_end_test - epoch_start_test}\")\n",
    "    end = time.time()\n",
    "    print(f\"Total training time = {end - start}\")\n",
    "\n",
    "    # make sure to save the model so we don't need to train again\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-variety",
   "metadata": {},
   "source": [
    "## Train our Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "structured-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "test_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/test_asl.csv\", transforms=[recolor, to_tensor])\n",
    "test_data_loader = DataLoader(test_asl_dataset, **params)\n",
    "train_data_loader = DataLoader(train_asl_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "prepared-python",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_epochs = 400 \n",
    "# asl_model = AslNNModel()\n",
    "\n",
    "# save_path='./data/asl/asl_train_no_tl_same_model_tmp.pkl'\n",
    "# train_model(asl_model, train_data_loader, max_epochs, use_cuda, save_path='./data/asl/asl_train_no_tl_same_model.pkl')\n",
    "# asl_train_all_y, asl_train_all_y_hat, asl_train_acc = get_model_acc(asl_model, train_data_loader)\n",
    "# cm_df, report = make_stats(asl_train_all_y, asl_train_all_y_hat, num_classes=24)\n",
    "# print(cm_df.to_markdown())\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "characteristic-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asl_test_all_y, asl_test_all_y_hat, asl_test_acc = get_model_acc(asl_model, test_data_loader)\n",
    "# cm_df, report = make_stats(asl_test_all_y, asl_test_all_y_hat, num_classes=24)\n",
    "# print(cm_df.to_markdown())\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "collectible-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = AslNNModel()\n",
    "# loaded_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-right",
   "metadata": {},
   "source": [
    "## Using AlexNet\n",
    "This has already been implemented in pytorch, so we are exploring using this CNN model with our datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-wireless",
   "metadata": {},
   "source": [
    "### Baseline: Literally just loading AlexNet and making predicitions\n",
    "To have a baseline comparison, we will first just use AlexNet (pretrained on ImageNet dataset) to make predictions. We don't expect this to do well at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "awful-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "alexnet_model = torchvision.models.alexnet(pretrained=True)\n",
    "# update model to have a single layer input, not 3 layer (AlexNet expects 3 channel RGB images as input)\n",
    "alexnet_model.features[0] = Conv2d(1, 64, kernel_size=(11,11), stride=(4,4), padding=(2,2))\n",
    "# update model to have 26 possible output classes, one per letter\n",
    "alexnet_model.classifier[6] = torch.nn.Linear(4096, 26)\n",
    "print(alexnet_model.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "interim-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_asl_dataset))\n",
    "# alexnet_train_all_y, alexnet_train_all_y_hat, alexnet_train_acc = get_model_acc(alexnet_model, train_data_loader)\n",
    "# cm_df, report = make_stats(alexnet_train_all_y, alexnet_train_all_y_hat, num_classes=24)\n",
    "# print(cm_df.to_markdown())\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-enhancement",
   "metadata": {},
   "source": [
    "### Second Approach: Transfer Learning from the Pretrained AlexNet model \n",
    "This won't be a full train from scratch, but techinically transfer learning from the Pretrained AlexNet model.\n",
    "We will train AlexNet on our ASL dataset, starting from the already trained AlexNet model on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "renewable-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "opening-cartoon",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "start epoch 0\n",
      "epoch 0 48: train running loss = 4.16281795501709\n",
      "epoch 0 48: train testing runtime = 0.1849968433380127\n",
      "epoch 0  720: Train Final loss = 3.220088736216227\n",
      "epoch 0  720: Train runtime = 2.618997573852539\n",
      "epoch 0 48: testing running loss = 2.808217763900757\n",
      "epoch 0 48: testing runtime = 0.11299967765808105\n",
      "epoch 0 240: Testing Final loss = 2.578148078918457\n",
      "epoch 0 240: Testing runtime = 0.5559995174407959\n",
      "start epoch 1\n",
      "epoch 1 48: train running loss = 2.5109703540802\n",
      "epoch 1 48: train testing runtime = 0.16400456428527832\n",
      "epoch 1  720: Train Final loss = 1.9120055198669434\n",
      "epoch 1  720: Train runtime = 2.5220015048980713\n",
      "epoch 1 48: testing running loss = 1.7185534238815308\n",
      "epoch 1 48: testing runtime = 0.10799837112426758\n",
      "epoch 1 240: Testing Final loss = 1.7164358377456665\n",
      "epoch 1 240: Testing runtime = 0.5529992580413818\n",
      "start epoch 2\n",
      "epoch 2 48: train running loss = 1.4805928468704224\n",
      "epoch 2 48: train testing runtime = 0.16899943351745605\n",
      "epoch 2  720: Train Final loss = 1.3939913113911946\n",
      "epoch 2  720: Train runtime = 2.501000165939331\n",
      "epoch 2 48: testing running loss = 1.0120635032653809\n",
      "epoch 2 48: testing runtime = 0.10699868202209473\n",
      "epoch 2 240: Testing Final loss = 1.1160125255584716\n",
      "epoch 2 240: Testing runtime = 0.543999195098877\n",
      "start epoch 3\n",
      "epoch 3 48: train running loss = 1.159688115119934\n",
      "epoch 3 48: train testing runtime = 0.17199993133544922\n",
      "epoch 3  720: Train Final loss = 0.7811725556850433\n",
      "epoch 3  720: Train runtime = 2.524000883102417\n",
      "epoch 3 48: testing running loss = 0.7271955609321594\n",
      "epoch 3 48: testing runtime = 0.11099910736083984\n",
      "epoch 3 240: Testing Final loss = 1.0117994785308837\n",
      "epoch 3 240: Testing runtime = 0.5530006885528564\n",
      "start epoch 4\n",
      "epoch 4 48: train running loss = 0.7813987731933594\n",
      "epoch 4 48: train testing runtime = 0.1699986457824707\n",
      "epoch 4  720: Train Final loss = 0.7082874218622843\n",
      "epoch 4  720: Train runtime = 2.5649983882904053\n",
      "epoch 4 48: testing running loss = 0.8767910003662109\n",
      "epoch 4 48: testing runtime = 0.11600160598754883\n",
      "epoch 4 240: Testing Final loss = 0.6588474214076996\n",
      "epoch 4 240: Testing runtime = 0.5650010108947754\n",
      "start epoch 5\n",
      "epoch 5 48: train running loss = 0.4301571547985077\n",
      "epoch 5 48: train testing runtime = 0.17200136184692383\n",
      "epoch 5  720: Train Final loss = 0.40252388616402945\n",
      "epoch 5  720: Train runtime = 2.6129987239837646\n",
      "epoch 5 48: testing running loss = 0.770743191242218\n",
      "epoch 5 48: testing runtime = 0.12100005149841309\n",
      "epoch 5 240: Testing Final loss = 0.6560379147529602\n",
      "epoch 5 240: Testing runtime = 0.5869970321655273\n",
      "start epoch 6\n",
      "epoch 6 48: train running loss = 0.3453352451324463\n",
      "epoch 6 48: train testing runtime = 0.1739978790283203\n",
      "epoch 6  720: Train Final loss = 0.2569795648256938\n",
      "epoch 6  720: Train runtime = 2.6019980907440186\n",
      "epoch 6 48: testing running loss = 0.3442573547363281\n",
      "epoch 6 48: testing runtime = 0.12000012397766113\n",
      "epoch 6 240: Testing Final loss = 0.475895357131958\n",
      "epoch 6 240: Testing runtime = 0.564000129699707\n",
      "start epoch 7\n",
      "epoch 7 48: train running loss = 0.14250381290912628\n",
      "epoch 7 48: train testing runtime = 0.16699957847595215\n",
      "epoch 7  720: Train Final loss = 0.19737087885538737\n",
      "epoch 7  720: Train runtime = 2.5640013217926025\n",
      "epoch 7 48: testing running loss = 0.6729333400726318\n",
      "epoch 7 48: testing runtime = 0.10999798774719238\n",
      "epoch 7 240: Testing Final loss = 0.4834984093904495\n",
      "epoch 7 240: Testing runtime = 0.5529980659484863\n",
      "start epoch 8\n",
      "epoch 8 48: train running loss = 0.25745341181755066\n",
      "epoch 8 48: train testing runtime = 0.16300034523010254\n",
      "epoch 8  720: Train Final loss = 0.14780302699655296\n",
      "epoch 8  720: Train runtime = 2.559551954269409\n",
      "epoch 8 48: testing running loss = 0.4658724367618561\n",
      "epoch 8 48: testing runtime = 0.11500048637390137\n",
      "epoch 8 240: Testing Final loss = 0.3545439660549164\n",
      "epoch 8 240: Testing runtime = 0.5520002841949463\n",
      "start epoch 9\n",
      "epoch 9 48: train running loss = 0.06574369221925735\n",
      "epoch 9 48: train testing runtime = 0.1660020351409912\n",
      "epoch 9  720: Train Final loss = 0.1808158462246259\n",
      "epoch 9  720: Train runtime = 2.5400009155273438\n",
      "epoch 9 48: testing running loss = 0.2553679943084717\n",
      "epoch 9 48: testing runtime = 0.1120004653930664\n",
      "epoch 9 240: Testing Final loss = 0.6839982509613037\n",
      "epoch 9 240: Testing runtime = 0.556999921798706\n",
      "start epoch 10\n",
      "epoch 10 48: train running loss = 0.24492119252681732\n",
      "epoch 10 48: train testing runtime = 0.167999267578125\n",
      "epoch 10  720: Train Final loss = 0.08982382342219353\n",
      "epoch 10  720: Train runtime = 2.7069990634918213\n",
      "epoch 10 48: testing running loss = 0.25682878494262695\n",
      "epoch 10 48: testing runtime = 0.11299967765808105\n",
      "epoch 10 240: Testing Final loss = 0.32767058312892916\n",
      "epoch 10 240: Testing runtime = 0.5679998397827148\n",
      "start epoch 11\n",
      "epoch 11 48: train running loss = 0.03224431723356247\n",
      "epoch 11 48: train testing runtime = 0.17600035667419434\n",
      "epoch 11  720: Train Final loss = 0.04081739435593287\n",
      "epoch 11  720: Train runtime = 2.5560028553009033\n",
      "epoch 11 48: testing running loss = 0.20231769979000092\n",
      "epoch 11 48: testing runtime = 0.10699915885925293\n",
      "epoch 11 240: Testing Final loss = 0.283822326362133\n",
      "epoch 11 240: Testing runtime = 0.5560004711151123\n",
      "start epoch 12\n",
      "epoch 12 48: train running loss = 0.03145987167954445\n",
      "epoch 12 48: train testing runtime = 0.16699862480163574\n",
      "epoch 12  720: Train Final loss = 0.01830221648948888\n",
      "epoch 12  720: Train runtime = 2.5439982414245605\n",
      "epoch 12 48: testing running loss = 0.2704603374004364\n",
      "epoch 12 48: testing runtime = 0.10800027847290039\n",
      "epoch 12 240: Testing Final loss = 0.31466577649116517\n",
      "epoch 12 240: Testing runtime = 0.5670032501220703\n",
      "start epoch 13\n",
      "epoch 13 48: train running loss = 0.005953750107437372\n",
      "epoch 13 48: train testing runtime = 0.1719977855682373\n",
      "epoch 13  720: Train Final loss = 0.016762821019316712\n",
      "epoch 13  720: Train runtime = 2.554366111755371\n",
      "epoch 13 48: testing running loss = 0.4402771294116974\n",
      "epoch 13 48: testing runtime = 0.10699987411499023\n",
      "epoch 13 240: Testing Final loss = 0.3731513500213623\n",
      "epoch 13 240: Testing runtime = 0.5650012493133545\n",
      "start epoch 14\n",
      "epoch 14 48: train running loss = 0.01322185155004263\n",
      "epoch 14 48: train testing runtime = 0.16699767112731934\n",
      "epoch 14  720: Train Final loss = 0.010276775799381237\n",
      "epoch 14  720: Train runtime = 2.5990147590637207\n",
      "epoch 14 48: testing running loss = 0.3199073076248169\n",
      "epoch 14 48: testing runtime = 0.14799880981445312\n",
      "epoch 14 240: Testing Final loss = 0.3121709883213043\n",
      "epoch 14 240: Testing runtime = 0.6799948215484619\n",
      "start epoch 15\n",
      "epoch 15 48: train running loss = 0.0022144641261547804\n",
      "epoch 15 48: train testing runtime = 0.1741189956665039\n",
      "epoch 15  720: Train Final loss = 0.010573845744753877\n",
      "epoch 15  720: Train runtime = 2.6492624282836914\n",
      "epoch 15 48: testing running loss = 0.16379861533641815\n",
      "epoch 15 48: testing runtime = 0.11099982261657715\n",
      "epoch 15 240: Testing Final loss = 0.23475254625082015\n",
      "epoch 15 240: Testing runtime = 0.5570030212402344\n",
      "start epoch 16\n",
      "epoch 16 48: train running loss = 0.008045965805649757\n",
      "epoch 16 48: train testing runtime = 0.1809990406036377\n",
      "epoch 16  720: Train Final loss = 0.006922343245241791\n",
      "epoch 16  720: Train runtime = 2.6209967136383057\n",
      "epoch 16 48: testing running loss = 0.028317132964730263\n",
      "epoch 16 48: testing runtime = 0.1099998950958252\n",
      "epoch 16 240: Testing Final loss = 0.29557490535080433\n",
      "epoch 16 240: Testing runtime = 0.5494225025177002\n",
      "start epoch 17\n",
      "epoch 17 48: train running loss = 0.005056824069470167\n",
      "epoch 17 48: train testing runtime = 0.1681530475616455\n",
      "epoch 17  720: Train Final loss = 0.00427197345222036\n",
      "epoch 17  720: Train runtime = 2.6165590286254883\n",
      "epoch 17 48: testing running loss = 0.273666113615036\n",
      "epoch 17 48: testing runtime = 0.1190330982208252\n",
      "epoch 17 240: Testing Final loss = 0.273330757021904\n",
      "epoch 17 240: Testing runtime = 0.5870335102081299\n",
      "start epoch 18\n",
      "epoch 18 48: train running loss = 0.0006823784206062555\n",
      "epoch 18 48: train testing runtime = 0.18199849128723145\n",
      "epoch 18  720: Train Final loss = 0.0019943005715807277\n",
      "epoch 18  720: Train runtime = 2.6969985961914062\n",
      "epoch 18 48: testing running loss = 0.2034735530614853\n",
      "epoch 18 48: testing runtime = 0.12900280952453613\n",
      "epoch 18 240: Testing Final loss = 0.24855862855911254\n",
      "epoch 18 240: Testing runtime = 0.5810067653656006\n",
      "start epoch 19\n",
      "epoch 19 48: train running loss = 0.0004707530315499753\n",
      "epoch 19 48: train testing runtime = 0.17799687385559082\n",
      "epoch 19  720: Train Final loss = 0.0012109412249022473\n",
      "epoch 19  720: Train runtime = 2.6919984817504883\n",
      "epoch 19 48: testing running loss = 0.13031254708766937\n",
      "epoch 19 48: testing runtime = 0.1159968376159668\n",
      "epoch 19 240: Testing Final loss = 0.24811281710863115\n",
      "epoch 19 240: Testing runtime = 0.5799970626831055\n",
      "start epoch 20\n",
      "epoch 20 48: train running loss = 0.0004883167566731572\n",
      "epoch 20 48: train testing runtime = 0.1810014247894287\n",
      "epoch 20  720: Train Final loss = 0.0008103000426975389\n",
      "epoch 20  720: Train runtime = 2.805004358291626\n",
      "epoch 20 48: testing running loss = 0.13334344327449799\n",
      "epoch 20 48: testing runtime = 0.11899971961975098\n",
      "epoch 20 240: Testing Final loss = 0.25818575024604795\n",
      "epoch 20 240: Testing runtime = 0.5769996643066406\n",
      "start epoch 21\n",
      "epoch 21 48: train running loss = 0.0007188037852756679\n",
      "epoch 21 48: train testing runtime = 0.17800164222717285\n",
      "epoch 21  720: Train Final loss = 0.0007335276168305427\n",
      "epoch 21  720: Train runtime = 2.646573066711426\n",
      "epoch 21 48: testing running loss = 0.06439319998025894\n",
      "epoch 21 48: testing runtime = 0.108001708984375\n",
      "epoch 21 240: Testing Final loss = 0.2554206341505051\n",
      "epoch 21 240: Testing runtime = 0.5740594863891602\n",
      "start epoch 22\n",
      "epoch 22 48: train running loss = 0.0011709561804309487\n",
      "epoch 22 48: train testing runtime = 0.17800068855285645\n",
      "epoch 22  720: Train Final loss = 0.0006297768016035358\n",
      "epoch 22  720: Train runtime = 2.689000129699707\n",
      "epoch 22 48: testing running loss = 0.09572120755910873\n",
      "epoch 22 48: testing runtime = 0.11000347137451172\n",
      "epoch 22 240: Testing Final loss = 0.25901494175195694\n",
      "epoch 22 240: Testing runtime = 0.5789999961853027\n",
      "start epoch 23\n",
      "epoch 23 48: train running loss = 0.0006532596307806671\n",
      "epoch 23 48: train testing runtime = 0.17200016975402832\n",
      "epoch 23  720: Train Final loss = 0.0005762642550204571\n",
      "epoch 23  720: Train runtime = 2.6199982166290283\n",
      "epoch 23 48: testing running loss = 0.12831521034240723\n",
      "epoch 23 48: testing runtime = 0.11100053787231445\n",
      "epoch 23 240: Testing Final loss = 0.25843723565340043\n",
      "epoch 23 240: Testing runtime = 0.5550026893615723\n",
      "start epoch 24\n",
      "epoch 24 48: train running loss = 0.0009678730275481939\n",
      "epoch 24 48: train testing runtime = 0.17399883270263672\n",
      "epoch 24  720: Train Final loss = 0.0005367332206030066\n",
      "epoch 24  720: Train runtime = 2.5590431690216064\n",
      "epoch 24 48: testing running loss = 0.3393581807613373\n",
      "epoch 24 48: testing runtime = 0.11599397659301758\n",
      "epoch 24 240: Testing Final loss = 0.2590662032365799\n",
      "epoch 24 240: Testing runtime = 0.5709962844848633\n",
      "start epoch 25\n",
      "epoch 25 48: train running loss = 0.0002267898671561852\n",
      "epoch 25 48: train testing runtime = 0.1889972686767578\n",
      "epoch 25  720: Train Final loss = 0.0005011201516026631\n",
      "epoch 25  720: Train runtime = 2.7609963417053223\n",
      "epoch 25 48: testing running loss = 0.3453162610530853\n",
      "epoch 25 48: testing runtime = 0.11400914192199707\n",
      "epoch 25 240: Testing Final loss = 0.26001768112182616\n",
      "epoch 25 240: Testing runtime = 0.5690004825592041\n",
      "start epoch 26\n",
      "epoch 26 48: train running loss = 0.0005103774019517004\n",
      "epoch 26 48: train testing runtime = 0.1639997959136963\n",
      "epoch 26  720: Train Final loss = 0.00046681923946986597\n",
      "epoch 26  720: Train runtime = 2.557000160217285\n",
      "epoch 26 48: testing running loss = 0.28138160705566406\n",
      "epoch 26 48: testing runtime = 0.11100077629089355\n",
      "epoch 26 240: Testing Final loss = 0.26168349385261536\n",
      "epoch 26 240: Testing runtime = 0.5619990825653076\n",
      "start epoch 27\n",
      "epoch 27 48: train running loss = 0.0003691546153277159\n",
      "epoch 27 48: train testing runtime = 0.17100119590759277\n",
      "epoch 27  720: Train Final loss = 0.00043796690491338573\n",
      "epoch 27  720: Train runtime = 2.6230013370513916\n",
      "epoch 27 48: testing running loss = 0.30208390951156616\n",
      "epoch 27 48: testing runtime = 0.11299848556518555\n",
      "epoch 27 240: Testing Final loss = 0.26411515325307844\n",
      "epoch 27 240: Testing runtime = 0.555997371673584\n",
      "Total training time = 89.0659077167511\n"
     ]
    }
   ],
   "source": [
    "train_model(alexnet_model, train_data_loader, test_data_loader, max_epochs, use_cuda, save_path='./Models/Asl_AdamNet_training.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "arranged-tumor",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Number of Misclassifications = 0\n",
      "Sample acc = 100.0\n",
      "Overall Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "alexnet_tl_train_all_y, alexnet_tl_train_all_y_hat, alexnet_tl_train_acc = get_model_acc(alexnet_model, train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fleet-restaurant",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Misclassifications = 4\n",
      "Sample acc = 91.66666666666666\n",
      "Number of Misclassifications = 3\n",
      "Sample acc = 93.75\n",
      "Number of Misclassifications = 5\n",
      "Sample acc = 89.58333333333334\n",
      "Number of Misclassifications = 2\n",
      "Sample acc = 95.83333333333334\n",
      "Number of Misclassifications = 5\n",
      "Sample acc = 89.58333333333334\n",
      "Overall Accuracy = 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, alexnet_tl_test_acc = get_model_acc(alexnet_model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "periodic-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   a |   b |   c |   d |   e |   f |   g |   h |   i |   k |   l |   m |   n |   o |   p |   q |   r |   s |   t |   u |   v |   w |   x |   y |\n",
      "|:---|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|\n",
      "| a  |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| b  |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| c  |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| d  |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| e  |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| f  |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| g  |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| h  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| i  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| k  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| l  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| m  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| n  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| o  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| p  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| q  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| r  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| s  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| t  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |   0 |\n",
      "| u  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |   0 |\n",
      "| v  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |   0 |\n",
      "| w  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |   0 |\n",
      "| x  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |   0 |\n",
      "| y  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  30 |\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       1.00      1.00      1.00        30\n",
      "           b       1.00      1.00      1.00        30\n",
      "           c       1.00      1.00      1.00        30\n",
      "           d       1.00      1.00      1.00        30\n",
      "           e       1.00      1.00      1.00        30\n",
      "           f       1.00      1.00      1.00        30\n",
      "           g       1.00      1.00      1.00        30\n",
      "           h       1.00      1.00      1.00        30\n",
      "           i       1.00      1.00      1.00        30\n",
      "           k       1.00      1.00      1.00        30\n",
      "           l       1.00      1.00      1.00        30\n",
      "           m       1.00      1.00      1.00        30\n",
      "           n       1.00      1.00      1.00        30\n",
      "           o       1.00      1.00      1.00        30\n",
      "           p       1.00      1.00      1.00        30\n",
      "           q       1.00      1.00      1.00        30\n",
      "           r       1.00      1.00      1.00        30\n",
      "           s       1.00      1.00      1.00        30\n",
      "           t       1.00      1.00      1.00        30\n",
      "           u       1.00      1.00      1.00        30\n",
      "           v       1.00      1.00      1.00        30\n",
      "           w       1.00      1.00      1.00        30\n",
      "           x       1.00      1.00      1.00        30\n",
      "           y       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00       720\n",
      "   macro avg       1.00      1.00      1.00       720\n",
      "weighted avg       1.00      1.00      1.00       720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_df, report = make_stats(alexnet_tl_train_all_y, alexnet_tl_train_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "helpful-liberia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   a |   b |   c |   d |   e |   f |   g |   h |   i |   k |   l |   m |   n |   o |   p |   q |   r |   s |   t |   u |   v |   w |   x |   y |\n",
      "|:---|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|\n",
      "| a  |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| b  |   0 |   9 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| c  |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| d  |   0 |   0 |   0 |   9 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |\n",
      "| e  |   0 |   0 |   0 |   0 |   8 |   0 |   0 |   0 |   0 |   0 |   0 |   2 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| f  |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| g  |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| h  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| i  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| k  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| l  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| m  |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   7 |   1 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| n  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   9 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |\n",
      "| o  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| p  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| q  |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   9 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| r  |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   9 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |\n",
      "| s  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   5 |   4 |   0 |   0 |   0 |   0 |   0 |\n",
      "| t  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   3 |   6 |   0 |   0 |   0 |   0 |   0 |\n",
      "| u  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |   0 |\n",
      "| v  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |   0 |\n",
      "| w  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |   0 |\n",
      "| x  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |   0 |\n",
      "| y  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |  10 |\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       1.00      1.00      1.00        10\n",
      "           b       1.00      0.90      0.95        10\n",
      "           c       0.91      1.00      0.95        10\n",
      "           d       1.00      0.90      0.95        10\n",
      "           e       0.89      0.80      0.84        10\n",
      "           f       0.83      1.00      0.91        10\n",
      "           g       1.00      1.00      1.00        10\n",
      "           h       1.00      1.00      1.00        10\n",
      "           i       1.00      1.00      1.00        10\n",
      "           k       1.00      1.00      1.00        10\n",
      "           l       1.00      1.00      1.00        10\n",
      "           m       0.78      0.70      0.74        10\n",
      "           n       0.75      0.90      0.82        10\n",
      "           o       1.00      1.00      1.00        10\n",
      "           p       1.00      1.00      1.00        10\n",
      "           q       1.00      0.90      0.95        10\n",
      "           r       1.00      0.90      0.95        10\n",
      "           s       0.56      0.50      0.53        10\n",
      "           t       0.55      0.60      0.57        10\n",
      "           u       1.00      1.00      1.00        10\n",
      "           v       0.91      1.00      0.95        10\n",
      "           w       1.00      1.00      1.00        10\n",
      "           x       1.00      1.00      1.00        10\n",
      "           y       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       240\n",
      "   macro avg       0.92      0.92      0.92       240\n",
      "weighted avg       0.92      0.92      0.92       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_df, report = make_stats(alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-toner",
   "metadata": {},
   "source": [
    "### Third Approach: Transfer Learning from the Kaggle Dataset\n",
    "For comparison to the TL trained AlexNet model, we will also be retraining the same style CNN as part 1 of our project, but using TL from our first model.\n",
    "In other words, we will be starting from the pretrained model on the Kaggle Dataset, and training a new model to recognize the ASL alphabet\n",
    "JK I'm not going through this headache - would basically need to redo every layer to match asl model to actually get it working, too much of a pita so not even gonna try\n",
    "We're just gonna say too hard in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "republican-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = np.arange(len(train_loss))\n",
    "# plot lines\n",
    "plt.plot(iterations, train_loss, label = \"train loss\")\n",
    "plt.plot(iterations, test_loss, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-upper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
