{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import everything we need for the project\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "This project uses the [Hand Gesture Recognition Database](https://www.kaggle.com/gti-upm/leapgestrecog/version/1) (citation below) available on Kaggle. It contains 20000 images with different hands and hand gestures. There is a total of 10 hand gestures of 10 different people presented in the dataset. There are 5 female subjects and 5 male subjects.\n",
    "The images were captured using the Leap Motion hand tracking device.\n",
    "\n",
    ">Hand Gesture | Label used\n",
    ">--- | ---\n",
    "> Thumb down | 0\n",
    "> Palm (Horizontal) | 1\n",
    "> L | 2\n",
    "> Fist (Horizontal) | 3\n",
    "> Fist (Vertical) | 4\n",
    "> Thumbs up | 5\n",
    "> Index | 6\n",
    "> OK | 7\n",
    "> Palm (Vertical) | 8\n",
    "> C | 9\n",
    "\n",
    "Table 1 - Classification used for every hand gesture.\n",
    "\n",
    "\n",
    "T. Mantecón, C.R. del Blanco, F. Jaureguizar, N. García, “Hand Gesture Recognition using Infrared Imagery Provided by Leap Motion Controller”, Int. Conf. on Advanced Concepts for Intelligent Vision Systems, ACIVS 2016, Lecce, Italy, pp. 47-57, 24-27 Oct. 2016. (doi: 10.1007/978-3-319-48680-2_5)  \n",
    "\n",
    "Overview:\n",
    "- Load images\n",
    "- Some validation\n",
    "- Preparing the images for training\n",
    "- Use of train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip images, ignore this cell if files are already in the workspace\n",
    "#!unzip leapGestRecog.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What needs to be fixed\n",
    "I'm tired, so here's a list of what needs to be fixed\n",
    "- read only from `leapgestrecog`: as far as I can tell, these are the same, so read these files only\n",
    "- modify the DataSet for this to also include the labels, since it currently doesn't, and it'll make training easier\n",
    "- actually train the model: will be super easy and like 15 lines of code once the rest is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing CSV\n",
    "Since this dataset doesn't come with a nice csv, write one ourselves to make loading the data easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "['./leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0156.png', './leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0031.png', './leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0011.png', './leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0144.png', './leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0038.png']\n"
     ]
    }
   ],
   "source": [
    "header = [\"path_to_file\", \"class/GT\"]\n",
    "csv_path = \"kaggle_images.csv\"\n",
    "# We need to get all the paths for the images to later load them\n",
    "imagepaths = []\n",
    "root = \"./leapgestrecog\"\n",
    "\n",
    "for dirname, dirs, files in os.walk(root):\n",
    "    for fname in files:\n",
    "        path = os.path.join(dirname, fname)\n",
    "        if path.endswith(\".png\"):\n",
    "            imagepaths.append(path)\n",
    "# Go through all the files and subdirectories inside a folder and save path to images inside list\n",
    "\"\"\"\n",
    "for root, dirs, files in os.walk(\".\", topdown=False): \n",
    "  for name in files:\n",
    "    path = os.path.join(root, name)\n",
    "    if path.endswith(\"png\"): # We want only the images\n",
    "      imagepaths.append(path)\n",
    "\"\"\"\n",
    "\n",
    "print(len(imagepaths)) # If > 0, then a PNG image was loaded\n",
    "print(imagepaths[:5])\n",
    "categories = [fpath.split(\"/\")[4] for _, fpath in enumerate(imagepaths)]\n",
    "gt = [category.split(\"_\")[0] for _, category in enumerate(categories)]\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for i, fpath in enumerate(imagepaths):\n",
    "        writer.writerow([fpath, gt[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "To facilitate using pytorch to build our cnn, we write a custom DataLoader class. This allows for on-demand loading of images, which are used to train our cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleHandDetectionDataset(Dataset):\n",
    "    \"\"\"Custom loader for the Kaggle Hand Detection Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image filepaths and gt classes\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file)\n",
    "        #print(self.images_frame)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.images_frame.iloc[idx, 0]\n",
    "        img = cv2.imread(img_name)\n",
    "        y = int(self.images_frame.iloc[idx, 1])-1\n",
    "        sample = {'image' : img, 'y' : y}\n",
    "        \n",
    "        if len(self.transforms) > 0:\n",
    "            for _, transform in enumerate(self.transforms):\n",
    "                sample = transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Used to rescale an image to a given size. Useful for the CNN\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size after rescaling. If tuple, output is matched to output_size.\n",
    "        If int smaller of width/height is matched to output_size, keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h // w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w // h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        img = cv2.resize(image, (new_h, new_w))\n",
    "        #print(f\"after resize: {img.shape}\")\n",
    "        return {'image' : img, 'y' : y}\n",
    "    \n",
    "class Recolor(object):\n",
    "    \"\"\"Used to recolor an image using cv2\n",
    "    \n",
    "    Args:\n",
    "        flag (cv2.COLOR_): color to swap to\n",
    "    \"\"\"\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        #print(f\"before recolor: {image.shape}\")\n",
    "        \n",
    "        # cvtColor to gray drops the damned channel dimension but we need it\n",
    "        img_cvt = cv2.cvtColor(image, self.color)\n",
    "        # fucking hack an extra dim to appease pytorch's bitchass\n",
    "        img_cvt = np.expand_dims(img_cvt, axis=-1)\n",
    "        #print(f\"after exansion: {img_cvt.shape}\")\n",
    "        return {'image' : img_cvt, 'y' : y}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays to pytorch Tensors\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        # swap color axis b/c \n",
    "        # numpy img: H x W x C\n",
    "        # torch img: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image' : torch.from_numpy(image), 'y' : y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNNModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (32, 256, 256) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (252, 252, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (121, 121, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (119, 119, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (59, 59, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (57, 57, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (28, 28, 64)\n",
    "        self.fc1 = Linear(30*30*64, 128) # output shape: (28*28*64, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([1, 256, 256])\n",
      "0\n",
      "{'image': tensor([[[ 6,  6,  7,  ...,  5,  5,  4],\n",
      "         [ 7,  6,  5,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ...,  3,  4,  5],\n",
      "         ...,\n",
      "         [ 5,  5,  6,  ...,  3,  4,  4],\n",
      "         [ 4,  4,  4,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ..., 34,  7, 13]]], dtype=torch.uint8), 'y': 0}\n"
     ]
    }
   ],
   "source": [
    "resize = Rescale((256,256))\n",
    "recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "to_tensor = ToTensor()\n",
    "transforms = [resize, recolor, to_tensor]\n",
    "\n",
    "hand_dataset = KaggleHandDetectionDataset(csv_file=\"kaggle_images.csv\", transforms=transforms)\n",
    "df = pd.read_csv(\"kaggle_images.csv\")\n",
    "\n",
    "#print(hand_dataset.images_frame)\n",
    "#print(\"images frame above\")\n",
    "#print(len(hand_dataset))\n",
    "sample = hand_dataset[0]\n",
    "print()\n",
    "print(sample['image'].shape)\n",
    "print(sample['y'])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6,  6,  7,  ...,  5,  5,  4],\n",
      "         [ 7,  6,  5,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ...,  3,  4,  5],\n",
      "         ...,\n",
      "         [ 5,  5,  6,  ...,  3,  4,  4],\n",
      "         [ 4,  4,  4,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ..., 34,  7, 13]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ed0471c8970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# complains b/c the gpu owns it, not the cpu since it's a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#cv2.imshow('recolored', as_gray['image'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'mat'"
     ]
    }
   ],
   "source": [
    "sample = hand_dataset[0]\n",
    "img = sample['image']\n",
    "#recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "#as_gray = recolor(sample)\n",
    "#scale = Rescale((256, 256))\n",
    "#tns = ToTensor()\n",
    "#as_tns = tns(sample)\n",
    "#print(type(as_tns['image']))\n",
    "#new_img = scale(sample)\n",
    "print(sample['image'])\n",
    "# complains b/c the gpu owns it, not the cpu since it's a tensor\n",
    "cv2.imshow('original', sample['image'])\n",
    "cv2.waitKey(2000)\n",
    "#cv2.imshow('recolored', as_gray['image'])\n",
    "#cv2.waitKey(2000)\n",
    "#cv2.imshow('rescaled', new_img['image'])\n",
    "#cv2.waitKey(3000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 1, 256, 256])\n",
      "1 torch.Size([4, 1, 256, 256])\n",
      "2 torch.Size([4, 1, 256, 256])\n",
      "3 torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(hand_dataset, batch_size=4)\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size())\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using the Custom Dataloader\n",
    "Below, we will actually train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-84539c9594ff>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(X)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eceef3724790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "params = {'batch_size' : 32, 'shuffle': True, 'num_workers' : 8}\n",
    "\n",
    "max_epochs = 100\n",
    "data_loader = DataLoader(hand_dataset, **params)\n",
    "model = HandNNModel()\n",
    "\n",
    "# batch_size, num_channels, w, h\n",
    "random_data = torch.rand((1, 1, 256, 256))\n",
    "\n",
    "result = model(random_data)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "imgs, labels = (next(iter(data_loader)))\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        imgs, labels = data['image'], data['y']\n",
    "        imgs = imgs.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        if i % 2000 == 1999:\n",
    "            print(f\"[{epoch+1, i +1, running_loss/2000}]\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
