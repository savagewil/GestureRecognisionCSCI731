{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import everything we need for the project\n",
    "\n",
    "%matplotlib inline\n",
    "import os, time\n",
    "import csv\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix, classification_report # Helps present results as a confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "This project uses the [Hand Gesture Recognition Database](https://www.kaggle.com/gti-upm/leapgestrecog/version/1) (citation below) available on Kaggle. It contains 20000 images with different hands and hand gestures. There is a total of 10 hand gestures of 10 different people presented in the dataset. There are 5 female subjects and 5 male subjects.\n",
    "The images were captured using the Leap Motion hand tracking device.\n",
    "\n",
    ">Hand Gesture | Label used\n",
    ">--- | ---\n",
    "> Thumb down | 0\n",
    "> Palm (Horizontal) | 1\n",
    "> L | 2\n",
    "> Fist (Horizontal) | 3\n",
    "> Fist (Vertical) | 4\n",
    "> Thumbs up | 5\n",
    "> Index | 6\n",
    "> OK | 7\n",
    "> Palm (Vertical) | 8\n",
    "> C | 9\n",
    "\n",
    "Table 1 - Classification used for every hand gesture.\n",
    "\n",
    "\n",
    "T. Mantecón, C.R. del Blanco, F. Jaureguizar, N. García, “Hand Gesture Recognition using Infrared Imagery Provided by Leap Motion Controller”, Int. Conf. on Advanced Concepts for Intelligent Vision Systems, ACIVS 2016, Lecce, Italy, pp. 47-57, 24-27 Oct. 2016. (doi: 10.1007/978-3-319-48680-2_5)  \n",
    "\n",
    "Overview:\n",
    "- Load images\n",
    "- Some validation\n",
    "- Preparing the images for training\n",
    "- Use of train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip images, ignore this cell if files are already in the workspace\n",
    "#!unzip leapGestRecog.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing CSV - Kaggle Dataset\n",
    "Since this dataset doesn't come with a nice csv, write one ourselves to make loading the data easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "header = [\"path_to_file\", \"class/GT\"]\n",
    "csv_path = \"kaggle_images.csv\"\n",
    "# We need to get all the paths for the images to later load them\n",
    "imagepaths = []\n",
    "root = \"./leapgestrecog\"\n",
    "\n",
    "for dirname, dirs, files in os.walk(root):\n",
    "    for fname in files:\n",
    "        path = os.path.join(dirname, fname)\n",
    "        if path.endswith(\".png\"):\n",
    "            imagepaths.append(path)\n",
    "\n",
    "print(len(imagepaths)) # If > 0, then a PNG image was loaded\n",
    "categories = [fpath.split(\"/\")[4] for _, fpath in enumerate(imagepaths)]\n",
    "gt = [category.split(\"_\")[0] for _, category in enumerate(categories)]\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for i, fpath in enumerate(imagepaths):\n",
    "        writer.writerow([fpath, gt[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing CSV - Kaggle Dataset\n",
    "Since this dataset doesn't come with a nice csv, write one ourselves to make loading the data easier later\n",
    "Note: directly setting filepaths to the pre-binarized images so we don't need to perform this operation ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd09736dc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXM0lEQVR4nO3de5BU9Zn/8fcDzAxyGy4qQSARxWABIWRwFShLTTaJqJWgibtgGePPEvhFo+VWkhUoq1Y2VVtZNlEg8YKIRrTkoriIpDY/g7ffBk1URBgckAEWiQwICKIMt3Gmn/2jD2MPc2W6e053fz+vqm/NmW/fnsMwnznn9OnzmLsjIuHqFHcBIhIvhYBI4BQCIoFTCIgETiEgEjiFgEjgshYCZjbBzLaY2TYzm5Gt1xGR9Fg2zhMws85AJfAdYBfwNnCDu2/K+IuJSFqytSVwMbDN3f/H3WuApcDELL2WiKShS5aedyDwYcr3u4BLmruzmem0RZHs+9jdzzp1Mlsh0CozmwZMi+v1RQK0s6nJbIVAFTA45ftB0Vw9d18ALABtCYjEKVvHBN4GLjCzIWZWDEwGXsjSa4lIGrKyJeDutWZ2B/Ai0Bl43N0rsvFaIpKerLxFeNpFaHdApCO84+4XnTqpMwZFAqcQEAmcQkAkcAoBkcApBEQCpxAQCZxCQCRwCgGRwCkERAKnEBAJnEJAJHAKAZHAKQREAqcQEAmcQkAkcAoBkcApBEQCpxAQCVxa1xg0sw+Aw0AdUOvuF5lZX2AZcC7wAfCP7v5JemWKSLZkYkvgm+4+OuXaZTOAl939AuDl6HsRyVHZ2B2YCCyKlhcB12bhNUQkQ9INAQf+ZGbvRB2FAPq7+55o+SOgf5qvISJZlG7fgUvdvcrMzgZWm9n7qTe6uzd3OXG1IRPJDWltCbh7VfR1H7CCZDfivWY2ACD6uq+Zxy5w94uaug66iHScdoeAmXU3s54nl4HvAu+RbDd2c3S3m4GV6RYpItmTzu5Af2CFmZ18nsXu/v/M7G3gGTO7lWQX1H9Mv0wRyRa1IRMJh9qQiUhjCgGRwCkERAKnEBAJnEJAJHAKAZHAKQREAqcQEAmcQkAkcAoBkcApBEQCpxAQCZxCQCRwCgGRwCkERAKnEBAJnEJAJHAKAZHAKQREAtdqCJjZ42a2z8zeS5nra2arzWxr9LVPNG9m9lsz22Zm5WZWls3iRSR9bdkSeAKYcMpcc/0GrwIuiMY04OHMlCki2dJqCLj7fwMHT5lurt/gROBJT/or0PtkIxIRyU3tPSbQXL/BgcCHKffbFc01YmbTzGytma1tZw0ikgHp9iJssd9gK49bACwA9R0QiVN7twSa6zdYBQxOud+gaE5EclR7Q6C5foMvAD+O3iUYC3yastsgIrnI3VscwBJgD/A5yX38W4F+JN8V2Aq8BPSN7mvAg8B2YCNwUWvPHz3ONTQ0sj7WNvX7p16EIuFQL0IRaUwhIBI4hYBI4BQCIoFTCIgETiEgEjiFgEjgFAIigVMIiAROISASOIWASOAUAiKBUwiIBE4hIBI4hYBI4BQCIoFTCIgETiEgErj2tiGbZWZVZrY+Glen3DYzakO2xcyuzFbhIpIZ7W1DBjDH3UdH478AzGw4MBkYET3mITPrnKliRSTz2tuGrDkTgaXufsLddwDbgIvTqE9EsiydYwJ3RJ2HHz/ZlRi1IRPJO+0NgYeB84HRJHsS3He6T+DuC9z9oqYugSwiHaddIeDue929zt0TwKN8scmvNmQieaZdIXBKu/HrgJPvHLwATDazEjMbAlwAvJVeiSKSTa12JTazJcAVwJlmtgu4F7jCzEaTbG30AfB/Ady9wsyeATYBtcBP3b0uK5WLSEaoDZlIONSGTEQaUwiIBE4hIBI4hYBI4BQCIoFTCIgETiEgEjiFgEjgFAIigVMIiAROISASOIWASOAUAiKBUwiIBE4hIBI4hYBI4BQCIoFTCIgEri1tyAab2atmtsnMKszsrmi+r5mtNrOt0dc+0byZ2W+jVmTlZlaW7ZUQkfZr9UKjJC8Y+nN3X2dmPYF3zGw18H+Al939381sBjADmA5cRfIqwxcAl5DsUXBJNoqX3NS9e3fOPPPMBnOffvophw4diqcgaVGrIeDue0g2GMHdD5vZZpJdhSaSvAoxwCLgNZIhMBF40pNXMP2rmfU2swHR80iO69mzJ507N98+srS0lNtuu63F+wwbNozvfve7DebWrVvH/Pnzee655zhy5EjG6pX0tWVLoJ6ZnQt8A3gT6J/yi/0R0D9abq4VWYMQMLNpwLTTLzm3mRlFRUVpP08ikaC2trbdj+/UqRNdujT88Y4fP54xY8a0+Lhbb72VL33pSy0+b69evTCz06pn3LhxlJWVceeddzJv3jyWLVvG559/flrPIdnR5kuOm1kP4P8D/+bu/2lmh9y9d8rtn7h7HzP7A/Dv7r4mmn8ZmO7uzfYcLKRLjk+ZMoV77rkn7efZuXMnCxYsaDT/pz/9iY8//rjFx44ePZopU6ZwzTXXNJjv06cPpaWladeWrhMnTlBRUcHs2bN5/vnnqampibukUDR5yfE2hYCZFQF/AF509/ujuS3AFe6+J+pI9Jq7DzOzR6LlJafer4Xnz/sQGDRoEI8++iiXXnopPXr0yMpruDuVlZUcPXq0fq6uro777ruvPhgmTZrE9773Pfr379/c0+SM2tpaKioqePvtt3nooYd499134y6p0DUZArh7iwMw4Elg7inzvwZmRMszgP+Ilq8B/hg9bizwVhtew/N9XHfddX706FGX9tm/f78//vjjPnz48Nh/lgU81npTv39NTXrDX9BLoycoB9ZH42qgH/AysBV4CejrX4TGg8B2YCNwURteI+5/nLRHp06dfM2aNR35e1NwEomEf/bZZ/7YY4/5V7/6Ve/cuXPsP9cCG02GgNqQZdA111zDihUrMnJgMGTuzvHjx3nqqae4//772b17N4cPH467rELQ/mMC2VYoIdClSxdeeeUVxo8f3+JbaNI27s7hw4f58MMPmTt3Lk899RQnTpyIu6x81r5jAh0xiH8zKWOjf//+PnPmTK+srPREIpGd7eYA1dTU+KZNm/zmm2/2bt26xf5zztOh3YGONGjQICZNmsQtt9zCiBEj4i6nYCQSCTZt2sSvf/1rVq1axSeffBJ3SflEuwNxOOecc7j++uu5/fbbGTZsWNzlFJR33nmHrVu3Mnv2bMrLy0kkEnGXlOu0OxDnOPPMM/3OO+/0HTt2eF1dXba2moOTSCT82LFjvnjxYh81apQXFRXF/rPO4aHdgVxQWlrKj370o/pTem+55RYGDRrU4D7FxcX07NkzjvLylrvz+eefs2TJEubOnUtFRYVOS25MuwO5qHv37o3O8R82bBg//OEPG8yNGjWKK664osGcmVFcXHza5/EXuurqapYtW8b8+fMpLy/XaclfUAjksx49etCnT58GcyUlJfz85z+ne/fu9XPdunVj4sSJdOrU8FIRZhZcWBw8eJBPPvmE++67jyVLluijzAqBMBQVFXHhhRc2+oWfNGkSZWUNr+8yZswYzjrrrI4sLxbuyc9czJkzhxUrVrBv3764S4qLQkAa+vrXv07fvn0bzA0dOpSpU6dSUlLCyJEjG21R5LuNGzfyyCOPsGLFCnbv3h13OR1NISBt9+Uvf5nKykpKSkriLiXj3J2Kigo2btzIvHnzWL9+fShnIjYZAoUV8yJtYGaMHDmSyZMn8+c//5lFixYxbty4ggy8tlAISLBOXgVq0qRJrF69mt///vdcfvnljd6tKXQKARGSb9XecMMNrFy5kmXLljV6O7aQKQREUpSWlvKDH/yA559/npUrV3L22WfHXVLWKQREmlBaWsr3v/99XnrpJaZOnVpw75KkKtw1E8mAr33ta8ybN4+pU6cW7DUiFAIirTjjjDOYO3cuDz/8MOeee27c5WRcOm3IZplZlZmtj8bVKY+ZGbUh22JmV2ZzBSQ7LrvssoL9y9ceXbt2ZcqUKSxfvpyvfOUrcZeTWU19tNAbfsx3AFAWLfcEKoHhwCzgF03cfziwASgBhpC84GjnVl4j7o9YaqSMPn366KKpLXjzzTd94sSJHp3klk+jyY8St7ol4O573H1dtHwYONmGrDkTgaXufsLddwDbgItbex3JHVdeeSXjx4+Pu4ycdfHFF7N06VKmT59eEAcMT2sNTmlDBnBH1Hn48ZNdiWm+DZnkgV69enHXXXcF94nD09W1a1dmzZrF3XffnfcnF7U5BKI2ZM8B/+Tun5HsNnw+MJpkn8H7TueFzWyama01s2bbk0nHmzBhApdcoibSbVFSUsIvf/lLli9fzsiRI+Mup/2a2kc4dQBFwIvAz5q5/VzgvWh5JjAz5bYXgXGtPH/c+0oa4CUlJb5u3bos71EXnkQi4ZWVlT5ixIjYf4atjPYdE7DkduFjwGaP+hBG8wNS7nYd8F60/AIw2cxKzGwIcAHwVmuvI/G79tprGTVqVNxl5B0zY+jQoSxfvpxp06bl3+5BU8ngDf9KN9eG7CmSbcbKSf7iD0h5zD0k3xXYAlzVhteIOyGDH0VFRf7uu+922F/PQlVTU+OzZ8/O1RZqutCoNO+mm25i4cKFFBcXx11K3jtw4ACXXXYZmzZtiruUU+l6AtJYcXExs2bN4t5771UAZEi/fv24/fbb82a3QFsCATvnnHP46U9/yvTp03V2YIYdOXKEMWPGsGXLlrhLSaUtAflCWVkZa9asYcaMGQqALOjWrRu/+MUv8uLfViEQmOLiYu644w6effZZhgwZUhBnvOUiM+PGG29k6NChcZfSKv0PCMxNN93EnDlzOO+88+IupeB17dqV6dOnx11GqxQCgWmq45Fkx86dOykpKWHw4MFxl9Ii/W8QSZO7s2HDBo4dO8YTTzzB5s2bAdi9ezfbt2+PubrWKQQCc+jQIWpqavR2YIYkEgmefvppbrvtNo4cORJ3Oe2i3YHALF26lF27dsVdRkGoq6vj6aef5ic/+UneBgBoSyA4uXBeSD5yd44fP467U1VVxdKlS6muruaBBx7g6NGjcZeXFoVAYNydRCKRPGdc1wxolbtz5MgRFi9ezG9+8xuqq6upqanhwIEDcZeWOU19oKCjB/F/sCKoMXToUH/ooYd87969Gf8ATSE5cOCAL1y40IcNG5aPlxJraugDRNLQiBEjGDJkCHfffTeDBw8uyCvptsfevXtZtWoVv/vd7ygvL4+7nExq8rTh2LcCXFsCOTHOP/98v/fee/3YsWPZ+9Oa4/bs2eMPPPCAjx49OvafR5aGtgSkZWbG2rVrKSsri7uUDnXw4EGefPJJHn30UTZv3lzIB0+b3BLQgUGp519smRUUj47sJxKJ+rm//OUvvP766wAsXryY7du3U1dXF1eJsVIISL3W3i1IJBK89tprGTkL7pvf/CZDhgzBzNL6EJNH73ak2r9/P6tWrWpwnwcffJB9+/bVz1VXV1NdXd3u1y0kCgGpN2HCBIYPH97s7dOmTePZZ5/ls88+S/u1Bg4cSK9evbj88su56qqrOPvssxk7dmyrj6usrOT999+v//7jjz/m/vvvbxAEx44d44MPPki7xlC0GgJm1hX4b5IdhboAy9393ugiokuBfsA7wE3uXmNmJcCTwBjgADDJ3T/IUv2SQb169eKMM85o9vadO3dmJAAAqqqqqKqqYvPmzcyfP5/evXszYsQI+vXrV/85fHdnzpw5fPTRR/WP27VrFzt37sxIDRJp6mihNzxyb0CPaLmIZOORscAzwORofj5wW7R8OzA/Wp4MLGvDa8R91FQDfNKkSS0ePf/2t78de40aaY12tyFzdz+581QUDQe+BSyP5hcB10bLE6PviW7/e9OpaSI5q01HZMyss5mtB/YBq0leTvyQu9dGd0ltNVbfhiy6/VOSuwwikoPaFALuXufuo4FBJJuLXpjuC6sNWe7Zvn07+/fvj7sM6WCn9d6Mux8CXgXGAb3N7OSBxUFAVbRcBQwGiG4vJXmA8NTnWuDuFzV18oLEY+3atfztb3+LuwzpYG1pQ3aWmfWOls8AvkOyPfmrwPXR3W4GVkbLL0TfE93+ihfiGSgiBaItWwIDgFfNrBx4G1jt7n8ApgM/M7NtJPf5H4vu/xjQL5r/GTAj82VLtixatKj1O0lBafU8AXcvB77RxPz/kDw+cOr8ceAfMlKddLgcbJ0lWabLi0kDdXV11NbWNpqvra0N9tz6QqcQkAbeeOMN1qxZ02DO3VmzZk39B26ksCgEpIGamhpqamoazf/qV79qcl7yn0JAGtmxY0f9R4oTiQQLFy7UVkAha+28/o4YxH9OtUbKGD16tCcSifqr7QwcODD2mjQyMtr32QEJ19GjR7nxxhupqqpq/c6St3Q9AWlkz549bNiwgZ07d/LGG2/EXY5kmUJAGtm7dy+vv/46q1at4vjx43GXI1mmC41Kk/r168fBgwcL8pqDAdOFRqXtCqrDjrRIBwZFAqcQEAmcQkAkcAoBkcApBEQCpxAQCZxCQCRwCgGRwLXlQqNdzewtM9tgZhVm9q/R/BNmtsPM1kdjdDRvZvZbM9tmZuVmFlafa5E805YzBk8A33L3ajMrAtaY2R+j2/7Z3Zefcv+rgAuicQnwcPRVRHJQOm3ImjMReDJ63F9J9icYkH6pIpIN7WpD5u5vRjf9W7TJPyfqRgwpbcgiqS3KRCTHtKsNmZmNBGaSbEf2d0Bfkn0I2kxtyERyQ3vbkE1w9z3RJv8J4Pd80YOgvg1ZJLVFWepzqQ2ZSA5obxuy90/u50dtx68F3ose8gLw4+hdgrHAp+6+Jwu1i0gGtOXdgQHAIjPrTDI0nnH3P5jZK2Z2FmDAeuAn0f3/C7ga2AYcBW7JeNUikjG6spBIOJq8spDOGBQJnEJAJHAKAZHAKQREAqcQEAmcQkAkcAoBkcApBEQCpxAQCZxCQCRwCgGRwCkERAKnEBAJnEJAJHAKAZHAKQREAqcQEAmcQkAkcAoBkcApBEQCpxAQCZxCQCRwbek70BGqgS1xF5ElZwIfx11EFhTqekHhrttXmprMlRDYUqjtyMxsbSGuW6GuFxT2ujVFuwMigVMIiAQuV0JgQdwFZFGhrluhrhcU9ro1khO9CEUkPrmyJSAiMYk9BMxsgpltMbNtZjYj7npOl5k9bmb7zOy9lLm+ZrbazLZGX/tE82Zmv43WtdzMyuKrvGVmNtjMXjWzTWZWYWZ3RfN5vW5m1tXM3jKzDdF6/Ws0P8TM3ozqX2ZmxdF8SfT9tuj2c2NdgWxw99gG0BnYDpwHFAMbgOFx1tSOdbgMKAPeS5n7D2BGtDwDmB0tXw38ETBgLPBm3PW3sF4DgLJouSdQCQzP93WL6usRLRcBb0b1PgNMjubnA7dFy7cD86PlycCyuNch4/8mMf9AxgEvpnw/E5gZ9z9KO9bj3FNCYAswIFoeQPI8CIBHgBuaul+uD2Al8J1CWjegG7AOuITkyUFdovn6/5fAi8C4aLlLdD+Lu/ZMjrh3BwYCH6Z8vyuay3f93X1PtPwR0D9azsv1jTaBv0Hyr2ber5uZdTaz9cA+YDXJrdFD7l4b3SW19vr1im7/FOjXoQVnWdwhUPA8+Sckb9+CMbMewHPAP7n7Z6m35eu6uXudu48GBgEXAxfGW1G84g6BKmBwyveDorl8t9fMBgBEX/dF83m1vmZWRDIAnnb3/4ymC2LdANz9EPAqyc3/3mZ28jT61Nrr1yu6vRQ40LGVZlfcIfA2cEF0ZLaY5IGXF2KuKRNeAG6Olm8muT99cv7H0ZH0scCnKZvWOcXMDHgM2Ozu96fclNfrZmZnmVnvaPkMksc5NpMMg+uju526XifX93rglWgLqHDEfVCC5FHlSpL7ZffEXU876l8C7AE+J7kveSvJfcaXga3AS0Df6L4GPBit60bgorjrb2G9LiW5qV8OrI/G1fm+bsAo4N1ovd4D/iWaPw94C9gGPAuURPNdo++3RbefF/c6ZHrojEGRwMW9OyAiMVMIiAROISASOIWASOAUAiKBUwiIBE4hIBI4hYBI4P4XVTJlgBMJ60MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "header = [\"path_to_file\", \"GT\"]\n",
    "csv_path = \"./data/asl/asl_images.csv\"\n",
    "imagepaths = []\n",
    "fnames = []\n",
    "# these are 400x400 BW images\n",
    "root = \"./data/asl/asl_data/binary_frames\"\n",
    "\n",
    "for dirname, dirs, files in os.walk(root):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".png\"):\n",
    "            fnames.append(fname)\n",
    "        path = os.path.join(dirname, fname)\n",
    "        if path.endswith(\".png\"):\n",
    "            imagepaths.append(path)\n",
    "\n",
    "gt = [fname.split('_')[0] for fname in fnames]\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for i, fpath in enumerate(imagepaths):\n",
    "        writer.writerow([fpath, gt[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL Helpers\n",
    "Mostly used to create a somewhat balanced train/test split, and to visualize counts of every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AslGestureDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e91750c9cfbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0masl_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAslGestureDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/asl/asl_images.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_as_np_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masl_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_asl_count_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AslGestureDataset' is not defined"
     ]
    }
   ],
   "source": [
    "def make_asl_count_df(y):\n",
    "    counts = np.zeros(26)\n",
    "    y = np.array(y)\n",
    "    for i in range(26):\n",
    "        counts[i] = np.sum(np.where(y == i, 1, 0))\n",
    "    idx = [chr(i) for i in range(97, 123)]\n",
    "    columns=[\"Count\"]\n",
    "    df = pd.DataFrame(counts, index=idx, columns=columns)\n",
    "    return df\n",
    "\n",
    "def y_as_np_arr(dataset):\n",
    "    return np.array([sample['y'] for sample in dataset])\n",
    "\n",
    "\n",
    "def make_asl_train_test_split(dataset, counts_df, split_ratio=0.75, train_csv=\"./data/asl/train_asl.csv\", test_csv=\"./data/asl/test_asl.csv\"):\n",
    "    header = [\"path_to_file\", \"GT\"]\n",
    "    train = []\n",
    "    test = []\n",
    "    np.random.seed(0)\n",
    "    train_counts, test_counts = [int(np.ceil(split_ratio*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])], [int(np.floor((1-split_ratio)*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])]\n",
    "    y = y_as_np_arr(dataset)\n",
    "    for idx in range(counts_df.shape[0]):\n",
    "        #curr_train_selections = []\n",
    "        #curr_test_selections = []\n",
    "        only_class_locs = np.where(y==idx)[0]\n",
    "        train_idxes = np.random.choice(only_class_locs, size=train_counts[idx], replace=False)\n",
    "        for data_idx in only_class_locs:\n",
    "            sample = dataset[data_idx]\n",
    "            if data_idx in train_idxes:\n",
    "                #curr_train_selections.append(sample['fname'])\n",
    "                train.append(sample['fname'])\n",
    "            else:\n",
    "                #curr_test_selections.append(sample['fname'])\n",
    "                test.append(sample['fname'])\n",
    "        #train.append(curr_train_selections)\n",
    "        #test.append(curr_test_selections)\n",
    "    train_lasts = [fname.split('/')[-1] for fname in train]\n",
    "    train_gt = [fname.split('_')[0] for fname in train_lasts]\n",
    "    test_lasts = [fname.split('/')[-1] for fname in test]\n",
    "    test_gt = [fname.split('_')[0] for fname in test_lasts]\n",
    "    with open(train_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(train):\n",
    "            writer.writerow([fpath, train_gt[i]])\n",
    "    with open(test_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(test):\n",
    "            writer.writerow([fpath, test_gt[i]])\n",
    "    return train, test\n",
    "\n",
    "asl_dataset = AslGestureDataset(csv_file=\"./data/asl/asl_images.csv\", transforms=[recolor, to_tensor])\n",
    "y = y_as_np_arr(asl_dataset)\n",
    "df = make_asl_count_df(y)\n",
    "#train, test = make_asl_train_test_split(asl_dataset, df)\n",
    "#print(test)\n",
    "\n",
    "lasts = [fname.split('/')[-1] for fname in test]\n",
    "gt = [fname.split('_')[0] for fname in lasts]\n",
    "print(len(gt))\n",
    "train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "print(len(train_asl_dataset))\n",
    "#plt.imshow(np.squeeze(test[0][0]['image'].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "To facilitate using pytorch to build our cnn, we write a custom DataLoader class. This allows for on-demand loading of images, which are used to train our cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleHandDetectionDataset(Dataset):\n",
    "    \"\"\"Custom loader for the Kaggle Hand Detection Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image filepaths and gt classes\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file)\n",
    "        #print(self.images_frame)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.images_frame.iloc[idx, 0]\n",
    "        img = cv2.imread(img_name)\n",
    "        y = int(self.images_frame.iloc[idx, 1])-1\n",
    "        sample = {'image' : img, 'y' : y, 'fname' : img_name}\n",
    "        \n",
    "        if len(self.transforms) > 0:\n",
    "            for _, transform in enumerate(self.transforms):\n",
    "                sample = transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslGestureDataset(Dataset):\n",
    "    \"\"\"Custom loader for the Kaggle Hand Detection Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image filepaths and gt classes\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.images_frame.iloc[idx, 0]\n",
    "        #print(img_name)\n",
    "        img = cv2.imread(img_name)\n",
    "        y = (self.images_frame.iloc[idx, 1])\n",
    "        # normalize to 0-26 for classes (missing j and z b/c dynamic)\n",
    "        y = ord(y)- ord('a')\n",
    "        sample = {'image' : img, 'y' : y, 'fname' : img_name}\n",
    "        \n",
    "        if len(self.transforms) > 0:\n",
    "            for _, transform in enumerate(self.transforms):\n",
    "                sample = transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Used to rescale an image to a given size. Useful for the CNN\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size after rescaling. If tuple, output is matched to output_size.\n",
    "        If int smaller of width/height is matched to output_size, keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h // w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w // h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        img = cv2.resize(image, (new_h, new_w))\n",
    "        #print(f\"after resize: {img.shape}\")\n",
    "        return {'image' : img, 'y' : y, 'fname' : sample['fname']}\n",
    "    \n",
    "class Recolor(object):\n",
    "    \"\"\"Used to recolor an image using cv2\n",
    "    \n",
    "    Args:\n",
    "        flag (cv2.COLOR_): color to swap to\n",
    "    \"\"\"\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        #print(f\"before recolor: {image.shape}\")\n",
    "        \n",
    "        # cvtColor to gray drops the damned channel dimension but we need it\n",
    "        img_cvt = cv2.cvtColor(image, self.color)\n",
    "        # fucking hack an extra dim to appease pytorch's bitchass\n",
    "        img_cvt = np.expand_dims(img_cvt, axis=-1)\n",
    "        #print(f\"after exansion: {img_cvt.shape}\")\n",
    "        return {'image' : img_cvt, 'y' : y, 'fname' : sample['fname']}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays to pytorch Tensors\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        # swap color axis b/c \n",
    "        # numpy img: H x W x C\n",
    "        # torch img: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image' : torch.from_numpy(image), 'y' : y, 'fname' : sample['fname']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_class_str(pred):\n",
    "    classes = {0 : \"palm\", 1 : \"L\", 2 : \"fist\", 3 : \"fist_moved\", 4 : \"thumb\", 5 : \"index\", 6 : \"ok\", 7 : \"palm_moved\", 8 : \"c\", 9 : \"down\"}\n",
    "    return classes[pred]\n",
    "\n",
    "def classify_arbitrary_image(model, img):\n",
    "    img_type = type(img)\n",
    "    print(img_type)\n",
    "    if img_type == torch.Tensor:\n",
    "        print(\"tensor\")\n",
    "        img = img.float()\n",
    "        img = img.unsqueeze(1)\n",
    "    elif img_type == np.ndarray:\n",
    "        print(\"np array\")\n",
    "        img = np.expand_dims(img, 1)\n",
    "    else:\n",
    "        print(\"error: something other than a torch.Tensor or an np.ndarray was passed as img\")\n",
    "    prediction = model(img)\n",
    "    prediction = prediction.data.numpy()\n",
    "    y_hat = np.argmax(prediction, axis=1)\n",
    "    return prediction_to_class_str(y_hat[0])\n",
    "\n",
    "def classify_many_images(model, imgs):\n",
    "    # for now, assuming imgs is a list of images that are either np.ndarrays or torch.Tensors\n",
    "    # labels will be given back in order images were given\n",
    "    predictions = []\n",
    "    for img in imgs:\n",
    "        predictions.append(classify_arbitrary_image(model, img))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HandNNModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (32, 256, 256) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (252, 252, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (121, 121, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (119, 119, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (59, 59, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (57, 57, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (28, 28, 64)\n",
    "        self.fc1 = Linear(28*28*64, 128) # output shape: (28*28*64, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslNNModel(Module):\n",
    "    # same structure as HandNNModel, need to change dimensions\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (64, 400, 400) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (496, 496, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (198, 198, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (196, 196, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (98, 98, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (96, 96, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (48, 48, 64)\n",
    "        self.fc1 = Linear(48*48*64, 128) # output shape: (48*48*64, 128)\n",
    "        self.fc2 = Linear(128, 26) # 24 possible output classes, but it goes up to idx 26: CUDA screams otherwise, so here we are\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "\n",
      "torch.Size([1, 400, 400])\n",
      "23\n",
      "{'image': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'y': 23, 'fname': './data/asl/asl_data/binary_frames/x_6-38.png'}\n",
      "\n",
      "torch.Size([1, 256, 256])\n",
      "0\n",
      "{'image': tensor([[[ 6,  6,  7,  ...,  5,  5,  4],\n",
      "         [ 7,  6,  5,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ...,  3,  4,  5],\n",
      "         ...,\n",
      "         [ 5,  5,  6,  ...,  3,  4,  4],\n",
      "         [ 4,  4,  4,  ...,  6,  4,  5],\n",
      "         [ 5,  5,  4,  ..., 34,  7, 13]]], dtype=torch.uint8), 'y': 0, 'fname': './leapgestrecog/leapGestRecog/02/01_palm/frame_02_01_0156.png'}\n"
     ]
    }
   ],
   "source": [
    "resize = Rescale((256,256))\n",
    "recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "to_tensor = ToTensor()\n",
    "transforms = [resize, recolor, to_tensor]\n",
    "\n",
    "hand_dataset = KaggleHandDetectionDataset(csv_file=\"kaggle_images.csv\", transforms=transforms)\n",
    "\n",
    "# for some reason, even with the binarization, there's 3 channels, but we only want one - so, use recolor\n",
    "asl_dataset = AslGestureDataset(csv_file=\"./data/asl/asl_images.csv\", transforms=[recolor, to_tensor])\n",
    "train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "print(len(train_asl_dataset))\n",
    "df = pd.read_csv(\"kaggle_images.csv\")\n",
    "\n",
    "sample = asl_dataset[0]\n",
    "print()\n",
    "print(sample['image'].shape)\n",
    "print(sample['y'])\n",
    "print(sample)\n",
    "sample = hand_dataset[0]\n",
    "print()\n",
    "print(sample['image'].shape)\n",
    "print(sample['y'])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsample = hand_dataset[0]\\nimg = sample['image']\\n#recolor = Recolor(cv2.COLOR_BGR2GRAY)\\n#as_gray = recolor(sample)\\n#scale = Rescale((256, 256))\\n#tns = ToTensor()\\n#as_tns = tns(sample)\\n#print(type(as_tns['image']))\\n#new_img = scale(sample)\\nprint(sample['image'])\\n# complains b/c the gpu owns it, not the cpu since it's a tensor\\ncv2.imshow('original', sample['image'])\\ncv2.waitKey(2000)\\n#cv2.imshow('recolored', as_gray['image'])\\n#cv2.waitKey(2000)\\n#cv2.imshow('rescaled', new_img['image'])\\n#cv2.waitKey(3000)\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sample = asl_dataset[0]\n",
    "print()\n",
    "print(sample['image'].shape)\n",
    "print(sample['y'])\n",
    "print(sample)\n",
    "sample = hand_dataset[0]\n",
    "img = sample['image']\n",
    "#recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "#as_gray = recolor(sample)\n",
    "#scale = Rescale((256, 256))\n",
    "#tns = ToTensor()\n",
    "#as_tns = tns(sample)\n",
    "#print(type(as_tns['image']))\n",
    "#new_img = scale(sample)\n",
    "print(sample['image'])\n",
    "# complains b/c the gpu owns it, not the cpu since it's a tensor\n",
    "cv2.imshow('original', sample['image'])\n",
    "cv2.waitKey(2000)\n",
    "#cv2.imshow('recolored', as_gray['image'])\n",
    "#cv2.waitKey(2000)\n",
    "#cv2.imshow('rescaled', new_img['image'])\n",
    "#cv2.waitKey(3000)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/asl/asl_data/binary_frames/x_6-38.png\n",
      "./data/asl/asl_data/binary_frames/p_7-57.png\n",
      "./data/asl/asl_data/binary_frames/g_2-38.png\n",
      "./data/asl/asl_data/binary_frames/q_8-38.png\n",
      "0 torch.Size([4, 1, 400, 400])\n",
      "./data/asl/asl_data/binary_frames/p_4-57.png\n",
      "./data/asl/asl_data/binary_frames/h_4-38.png\n",
      "./data/asl/asl_data/binary_frames/s_1-19.png\n",
      "./data/asl/asl_data/binary_frames/l_3-76.png\n",
      "1 torch.Size([4, 1, 400, 400])\n",
      "./data/asl/asl_data/binary_frames/w_4-38.png\n",
      "./data/asl/asl_data/binary_frames/a_4-19.png\n",
      "./data/asl/asl_data/binary_frames/c_6-57.png\n",
      "./data/asl/asl_data/binary_frames/d_1-95.png\n",
      "2 torch.Size([4, 1, 400, 400])\n",
      "./data/asl/asl_data/binary_frames/l_5-95.png\n",
      "./data/asl/asl_data/binary_frames/n_7-38.png\n",
      "./data/asl/asl_data/binary_frames/h_8-76.png\n",
      "./data/asl/asl_data/binary_frames/s_1-57.png\n",
      "3 torch.Size([4, 1, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(asl_dataset, batch_size=4)\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size())\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using the Custom Dataloader\n",
    "Below, we will actually train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1a7815147b0d>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: running loss = 3.23928165435791\n",
      "epoch 0: running loss = 36.01439380645752\n",
      "epoch 0 runtime = 3.74021053314209\n",
      "start epoch 1\n",
      "epoch 1: running loss = 3.238759994506836\n",
      "epoch 1: running loss = 35.957186698913574\n",
      "epoch 1 runtime = 3.7243735790252686\n",
      "start epoch 2\n",
      "epoch 2: running loss = 3.1965620517730713\n",
      "epoch 2: running loss = 35.86404800415039\n",
      "epoch 2 runtime = 3.7241523265838623\n",
      "start epoch 3\n",
      "epoch 3: running loss = 3.2804267406463623\n",
      "epoch 3: running loss = 36.104058027267456\n",
      "epoch 3 runtime = 3.690035820007324\n",
      "start epoch 4\n",
      "epoch 4: running loss = 3.2594709396362305\n",
      "epoch 4: running loss = 36.10472846031189\n",
      "epoch 4 runtime = 3.726332426071167\n",
      "start epoch 5\n",
      "epoch 5: running loss = 3.238759994506836\n",
      "epoch 5: running loss = 35.895745277404785\n",
      "epoch 5 runtime = 3.711392641067505\n",
      "start epoch 6\n",
      "epoch 6: running loss = 3.2801475524902344\n",
      "epoch 6: running loss = 35.917240619659424\n",
      "epoch 6 runtime = 3.7042717933654785\n",
      "start epoch 7\n",
      "epoch 7: running loss = 3.2386934757232666\n",
      "epoch 7: running loss = 35.95892834663391\n",
      "epoch 7 runtime = 3.72097110748291\n",
      "start epoch 8\n",
      "epoch 8: running loss = 3.321972608566284\n",
      "epoch 8: running loss = 36.02027249336243\n",
      "epoch 8 runtime = 3.7007787227630615\n",
      "start epoch 9\n",
      "epoch 9: running loss = 3.280093193054199\n",
      "epoch 9: running loss = 36.042208671569824\n",
      "epoch 9 runtime = 3.728198528289795\n",
      "start epoch 10\n",
      "epoch 10: running loss = 3.1970787048339844\n",
      "epoch 10: running loss = 36.02200794219971\n",
      "epoch 10 runtime = 3.6970598697662354\n",
      "start epoch 11\n",
      "epoch 11: running loss = 3.280297040939331\n",
      "epoch 11: running loss = 35.979987382888794\n",
      "epoch 11 runtime = 3.7128489017486572\n",
      "start epoch 12\n",
      "epoch 12: running loss = 3.301259994506836\n",
      "epoch 12: running loss = 35.938169717788696\n",
      "epoch 12 runtime = 3.7228708267211914\n",
      "start epoch 13\n",
      "epoch 13: running loss = 3.25931453704834\n",
      "epoch 13: running loss = 35.93857431411743\n",
      "epoch 13 runtime = 3.7269346714019775\n",
      "start epoch 14\n",
      "epoch 14: running loss = 3.3012592792510986\n",
      "epoch 14: running loss = 35.958712339401245\n",
      "epoch 14 runtime = 3.712580919265747\n",
      "start epoch 15\n",
      "epoch 15: running loss = 3.217926263809204\n",
      "epoch 15: running loss = 35.89649772644043\n",
      "epoch 15 runtime = 3.70985746383667\n",
      "start epoch 16\n",
      "epoch 16: running loss = 3.280426263809204\n",
      "epoch 16: running loss = 35.875901222229004\n",
      "epoch 16 runtime = 3.7131168842315674\n",
      "start epoch 17\n",
      "epoch 17: running loss = 3.1762571334838867\n",
      "epoch 17: running loss = 35.75106120109558\n",
      "epoch 17 runtime = 3.7328782081604004\n",
      "start epoch 18\n",
      "epoch 18: running loss = 3.259587287902832\n",
      "epoch 18: running loss = 35.91718792915344\n",
      "epoch 18 runtime = 3.731391668319702\n",
      "start epoch 19\n",
      "epoch 19: running loss = 3.301105499267578\n",
      "epoch 19: running loss = 35.834065437316895\n",
      "epoch 19 runtime = 3.716817617416382\n",
      "start epoch 20\n",
      "epoch 20: running loss = 3.1763181686401367\n",
      "epoch 20: running loss = 35.834232568740845\n",
      "epoch 20 runtime = 3.729891061782837\n",
      "start epoch 21\n",
      "epoch 21: running loss = 3.2595930099487305\n",
      "epoch 21: running loss = 36.03179144859314\n",
      "epoch 21 runtime = 3.7427003383636475\n",
      "start epoch 22\n",
      "epoch 22: running loss = 3.3011858463287354\n",
      "epoch 22: running loss = 36.00042009353638\n",
      "epoch 22 runtime = 3.7207224369049072\n",
      "start epoch 23\n",
      "epoch 23: running loss = 3.3010406494140625\n",
      "epoch 23: running loss = 35.89799213409424\n",
      "epoch 23 runtime = 3.717193365097046\n",
      "start epoch 24\n",
      "epoch 24: running loss = 3.238755226135254\n",
      "epoch 24: running loss = 35.96961045265198\n",
      "epoch 24 runtime = 3.7318732738494873\n",
      "start epoch 25\n",
      "epoch 25: running loss = 3.3220937252044678\n",
      "epoch 25: running loss = 36.000781297683716\n",
      "epoch 25 runtime = 3.7292685508728027\n",
      "start epoch 26\n",
      "epoch 26: running loss = 3.238759994506836\n",
      "epoch 26: running loss = 35.98051595687866\n",
      "epoch 26 runtime = 3.7399890422821045\n",
      "start epoch 27\n",
      "epoch 27: running loss = 3.2595932483673096\n",
      "epoch 27: running loss = 36.04290223121643\n",
      "epoch 27 runtime = 3.688868761062622\n",
      "start epoch 28\n",
      "epoch 28: running loss = 3.1764495372772217\n",
      "epoch 28: running loss = 36.00153350830078\n",
      "epoch 28 runtime = 3.714073896408081\n",
      "start epoch 29\n",
      "epoch 29: running loss = 3.2804267406463623\n",
      "epoch 29: running loss = 35.98031449317932\n",
      "epoch 29 runtime = 3.743384599685669\n",
      "start epoch 30\n",
      "epoch 30: running loss = 3.301260232925415\n",
      "epoch 30: running loss = 35.93791103363037\n",
      "epoch 30 runtime = 3.718919038772583\n",
      "start epoch 31\n",
      "epoch 31: running loss = 3.3010590076446533\n",
      "epoch 31: running loss = 36.08364725112915\n",
      "epoch 31 runtime = 3.71492338180542\n",
      "start epoch 32\n",
      "epoch 32: running loss = 3.259580373764038\n",
      "epoch 32: running loss = 35.93809628486633\n",
      "epoch 32 runtime = 3.6861202716827393\n",
      "start epoch 33\n",
      "epoch 33: running loss = 3.2803428173065186\n",
      "epoch 33: running loss = 35.89689350128174\n",
      "epoch 33 runtime = 3.6947062015533447\n",
      "start epoch 34\n",
      "epoch 34: running loss = 3.2595860958099365\n",
      "epoch 34: running loss = 35.959182262420654\n",
      "epoch 34 runtime = 3.735583543777466\n",
      "start epoch 35\n",
      "epoch 35: running loss = 3.238759994506836\n",
      "epoch 35: running loss = 35.81757068634033\n",
      "epoch 35 runtime = 3.6884560585021973\n",
      "start epoch 36\n",
      "epoch 36: running loss = 3.2387325763702393\n",
      "epoch 36: running loss = 35.979713439941406\n",
      "epoch 36 runtime = 3.7101902961730957\n",
      "start epoch 37\n",
      "epoch 37: running loss = 3.238759994506836\n",
      "epoch 37: running loss = 35.896998167037964\n",
      "epoch 37 runtime = 3.7138404846191406\n",
      "start epoch 38\n",
      "epoch 38: running loss = 3.301259994506836\n",
      "epoch 38: running loss = 35.89631533622742\n",
      "epoch 38 runtime = 3.7218568325042725\n",
      "start epoch 39\n",
      "epoch 39: running loss = 3.2387590408325195\n",
      "epoch 39: running loss = 35.93794798851013\n",
      "epoch 39 runtime = 3.739840269088745\n",
      "start epoch 40\n",
      "epoch 40: running loss = 3.2595932483673096\n",
      "epoch 40: running loss = 35.93852758407593\n",
      "epoch 40 runtime = 3.722579002380371\n",
      "start epoch 41\n",
      "epoch 41: running loss = 3.259514093399048\n",
      "epoch 41: running loss = 35.834428548812866\n",
      "epoch 41 runtime = 3.7254648208618164\n",
      "start epoch 42\n",
      "epoch 42: running loss = 3.1970930099487305\n",
      "epoch 42: running loss = 35.77093529701233\n",
      "epoch 42 runtime = 3.7135257720947266\n",
      "start epoch 43\n",
      "epoch 43: running loss = 3.301260232925415\n",
      "epoch 43: running loss = 35.95894455909729\n",
      "epoch 43 runtime = 3.713770627975464\n",
      "start epoch 44\n",
      "epoch 44: running loss = 3.259592294692993\n",
      "epoch 44: running loss = 35.875800371170044\n",
      "epoch 44 runtime = 3.6908326148986816\n",
      "start epoch 45\n",
      "epoch 45: running loss = 3.2179267406463623\n",
      "epoch 45: running loss = 35.81328105926514\n",
      "epoch 45 runtime = 3.731560468673706\n",
      "start epoch 46\n",
      "epoch 46: running loss = 3.2595932483673096\n",
      "epoch 46: running loss = 35.77160334587097\n",
      "epoch 46 runtime = 3.7149410247802734\n",
      "start epoch 47\n",
      "epoch 47: running loss = 3.238759994506836\n",
      "epoch 47: running loss = 35.896451473236084\n",
      "epoch 47 runtime = 3.719402551651001\n",
      "start epoch 48\n",
      "epoch 48: running loss = 3.280409574508667\n",
      "epoch 48: running loss = 35.98032474517822\n",
      "epoch 48 runtime = 3.7395992279052734\n",
      "start epoch 49\n",
      "epoch 49: running loss = 3.2595932483673096\n",
      "epoch 49: running loss = 35.89687180519104\n",
      "epoch 49 runtime = 3.6955363750457764\n",
      "start epoch 50\n",
      "epoch 50: running loss = 3.2804269790649414\n",
      "epoch 50: running loss = 35.81335115432739\n",
      "epoch 50 runtime = 3.7352774143218994\n",
      "start epoch 51\n",
      "epoch 51: running loss = 3.2387592792510986\n",
      "epoch 51: running loss = 35.91763520240784\n",
      "epoch 51 runtime = 3.7202231884002686\n",
      "start epoch 52\n",
      "epoch 52: running loss = 3.2804267406463623\n",
      "epoch 52: running loss = 35.91731071472168\n",
      "epoch 52 runtime = 3.7024643421173096\n",
      "start epoch 53\n",
      "epoch 53: running loss = 3.2804267406463623\n",
      "epoch 53: running loss = 35.93885040283203\n",
      "epoch 53 runtime = 3.706129789352417\n",
      "start epoch 54\n",
      "epoch 54: running loss = 3.259591817855835\n",
      "epoch 54: running loss = 35.93867468833923\n",
      "epoch 54 runtime = 3.7173686027526855\n",
      "start epoch 55\n",
      "epoch 55: running loss = 3.3007333278656006\n",
      "epoch 55: running loss = 35.91717576980591\n",
      "epoch 55 runtime = 3.735196352005005\n",
      "start epoch 56\n",
      "epoch 56: running loss = 3.238759994506836\n",
      "epoch 56: running loss = 35.91755747795105\n",
      "epoch 56 runtime = 3.7528469562530518\n",
      "start epoch 57\n",
      "epoch 57: running loss = 3.2179267406463623\n",
      "epoch 57: running loss = 35.87596583366394\n",
      "epoch 57 runtime = 3.7414515018463135\n",
      "start epoch 58\n",
      "epoch 58: running loss = 3.3012592792510986\n",
      "epoch 58: running loss = 35.85542011260986\n",
      "epoch 58 runtime = 3.700497627258301\n",
      "start epoch 59\n",
      "epoch 59: running loss = 3.1970932483673096\n",
      "epoch 59: running loss = 35.81355547904968\n",
      "epoch 59 runtime = 3.6995906829833984\n",
      "start epoch 60\n",
      "epoch 60: running loss = 3.2801597118377686\n",
      "epoch 60: running loss = 35.83364939689636\n",
      "epoch 60 runtime = 3.702690839767456\n",
      "start epoch 61\n",
      "epoch 61: running loss = 3.238759994506836\n",
      "epoch 61: running loss = 35.959152698516846\n",
      "epoch 61 runtime = 3.7272462844848633\n",
      "start epoch 62\n",
      "epoch 62: running loss = 3.3220937252044678\n",
      "epoch 62: running loss = 35.792285680770874\n",
      "epoch 62 runtime = 3.7206335067749023\n",
      "start epoch 63\n",
      "epoch 63: running loss = 3.2594215869903564\n",
      "epoch 63: running loss = 35.95880150794983\n",
      "epoch 63 runtime = 3.7337732315063477\n",
      "start epoch 64\n",
      "epoch 64: running loss = 3.2595932483673096\n",
      "epoch 64: running loss = 35.830777406692505\n",
      "epoch 64 runtime = 3.7245986461639404\n",
      "start epoch 65\n",
      "epoch 65: running loss = 3.301224708557129\n",
      "epoch 65: running loss = 35.91630816459656\n",
      "epoch 65 runtime = 3.705806016921997\n",
      "start epoch 66\n",
      "epoch 66: running loss = 3.3011562824249268\n",
      "epoch 66: running loss = 36.04267477989197\n",
      "epoch 66 runtime = 3.734492301940918\n",
      "start epoch 67\n",
      "epoch 67: running loss = 3.2593328952789307\n",
      "epoch 67: running loss = 35.895784854888916\n",
      "epoch 67 runtime = 3.7378110885620117\n",
      "start epoch 68\n",
      "epoch 68: running loss = 3.301260232925415\n",
      "epoch 68: running loss = 35.917123556137085\n",
      "epoch 68 runtime = 3.7093558311462402\n",
      "start epoch 69\n",
      "epoch 69: running loss = 3.280418634414673\n",
      "epoch 69: running loss = 35.97879886627197\n",
      "epoch 69 runtime = 3.7545318603515625\n",
      "start epoch 70\n",
      "epoch 70: running loss = 3.2804269790649414\n",
      "epoch 70: running loss = 36.08423590660095\n",
      "epoch 70 runtime = 3.7066452503204346\n",
      "start epoch 71\n",
      "epoch 71: running loss = 3.30122447013855\n",
      "epoch 71: running loss = 35.95889091491699\n",
      "epoch 71 runtime = 3.7333874702453613\n",
      "start epoch 72\n",
      "epoch 72: running loss = 3.2595932483673096\n",
      "epoch 72: running loss = 35.95956087112427\n",
      "epoch 72 runtime = 3.7225964069366455\n",
      "start epoch 73\n",
      "epoch 73: running loss = 3.3012287616729736\n",
      "epoch 73: running loss = 36.022034645080566\n",
      "epoch 73 runtime = 3.7080416679382324\n",
      "start epoch 74\n",
      "epoch 74: running loss = 3.217926263809204\n",
      "epoch 74: running loss = 35.95870280265808\n",
      "epoch 74 runtime = 3.698120594024658\n",
      "start epoch 75\n",
      "epoch 75: running loss = 3.301260232925415\n",
      "epoch 75: running loss = 36.01590919494629\n",
      "epoch 75 runtime = 3.716132640838623\n",
      "start epoch 76\n",
      "epoch 76: running loss = 3.2185709476470947\n",
      "epoch 76: running loss = 36.018144845962524\n",
      "epoch 76 runtime = 3.712611198425293\n",
      "start epoch 77\n",
      "epoch 77: running loss = 3.2380876541137695\n",
      "epoch 77: running loss = 35.85421109199524\n",
      "epoch 77 runtime = 3.723524332046509\n",
      "start epoch 78\n",
      "epoch 78: running loss = 3.2804267406463623\n",
      "epoch 78: running loss = 35.98001003265381\n",
      "epoch 78 runtime = 3.7409608364105225\n",
      "start epoch 79\n",
      "epoch 79: running loss = 3.301259994506836\n",
      "epoch 79: running loss = 35.896764516830444\n",
      "epoch 79 runtime = 3.713275671005249\n",
      "start epoch 80\n",
      "epoch 80: running loss = 3.3010661602020264\n",
      "epoch 80: running loss = 35.87517523765564\n",
      "epoch 80 runtime = 3.7222740650177\n",
      "start epoch 81\n",
      "epoch 81: running loss = 3.280426025390625\n",
      "epoch 81: running loss = 35.97804069519043\n",
      "epoch 81 runtime = 3.7027647495269775\n",
      "start epoch 82\n",
      "epoch 82: running loss = 3.2595932483673096\n",
      "epoch 82: running loss = 35.77228403091431\n",
      "epoch 82 runtime = 3.700613260269165\n",
      "start epoch 83\n",
      "epoch 83: running loss = 3.2384462356567383\n",
      "epoch 83: running loss = 35.878690004348755\n",
      "epoch 83 runtime = 3.688774824142456\n",
      "start epoch 84\n",
      "epoch 84: running loss = 3.238759994506836\n",
      "epoch 84: running loss = 35.85473299026489\n",
      "epoch 84 runtime = 3.686835289001465\n",
      "start epoch 85\n",
      "epoch 85: running loss = 3.2800190448760986\n",
      "epoch 85: running loss = 35.810208559036255\n",
      "epoch 85 runtime = 3.708336353302002\n",
      "start epoch 86\n",
      "epoch 86: running loss = 3.238485336303711\n",
      "epoch 86: running loss = 35.70904016494751\n",
      "epoch 86 runtime = 3.7148690223693848\n",
      "start epoch 87\n",
      "epoch 87: running loss = 3.238473653793335\n",
      "epoch 87: running loss = 35.8802707195282\n",
      "epoch 87 runtime = 3.7521438598632812\n",
      "start epoch 88\n",
      "epoch 88: running loss = 3.2803170680999756\n",
      "epoch 88: running loss = 35.855685234069824\n",
      "epoch 88 runtime = 3.7336585521698\n",
      "start epoch 89\n",
      "epoch 89: running loss = 3.2595930099487305\n",
      "epoch 89: running loss = 35.834367752075195\n",
      "epoch 89 runtime = 3.7189249992370605\n",
      "start epoch 90\n",
      "epoch 90: running loss = 3.238759994506836\n",
      "epoch 90: running loss = 35.66706895828247\n",
      "epoch 90 runtime = 3.701068639755249\n",
      "start epoch 91\n",
      "epoch 91: running loss = 3.2595927715301514\n",
      "epoch 91: running loss = 35.687955141067505\n",
      "epoch 91 runtime = 3.719036817550659\n",
      "start epoch 92\n",
      "epoch 92: running loss = 3.1970818042755127\n",
      "epoch 92: running loss = 35.73038935661316\n",
      "epoch 92 runtime = 3.7039098739624023\n",
      "start epoch 93\n",
      "epoch 93: running loss = 3.300873041152954\n",
      "epoch 93: running loss = 35.812721967697144\n",
      "epoch 93 runtime = 3.742933511734009\n",
      "start epoch 94\n",
      "epoch 94: running loss = 3.259592294692993\n",
      "epoch 94: running loss = 35.750547647476196\n",
      "epoch 94 runtime = 3.6988930702209473\n",
      "start epoch 95\n",
      "epoch 95: running loss = 3.2803843021392822\n",
      "epoch 95: running loss = 35.81360745429993\n",
      "epoch 95 runtime = 3.6978063583374023\n",
      "start epoch 96\n",
      "epoch 96: running loss = 3.2595932483673096\n",
      "epoch 96: running loss = 35.77185654640198\n",
      "epoch 96 runtime = 3.699312210083008\n",
      "start epoch 97\n",
      "epoch 97: running loss = 3.217926263809204\n",
      "epoch 97: running loss = 35.771528244018555\n",
      "epoch 97 runtime = 3.717529296875\n",
      "start epoch 98\n",
      "epoch 98: running loss = 3.3220298290252686\n",
      "epoch 98: running loss = 35.81338953971863\n",
      "epoch 98 runtime = 3.741844892501831\n",
      "start epoch 99\n",
      "epoch 99: running loss = 3.2595932483673096\n",
      "epoch 99: running loss = 35.8344988822937\n",
      "epoch 99 runtime = 3.716759204864502\n",
      "start epoch 100\n",
      "epoch 100: running loss = 3.259592056274414\n",
      "epoch 100: running loss = 35.751261711120605\n",
      "epoch 100 runtime = 3.690326452255249\n",
      "start epoch 101\n",
      "epoch 101: running loss = 3.2595932483673096\n",
      "epoch 101: running loss = 35.709322452545166\n",
      "epoch 101 runtime = 3.72052264213562\n",
      "start epoch 102\n",
      "epoch 102: running loss = 3.2595932483673096\n",
      "epoch 102: running loss = 35.792900800704956\n",
      "epoch 102 runtime = 3.7227540016174316\n",
      "start epoch 103\n",
      "epoch 103: running loss = 3.238759756088257\n",
      "epoch 103: running loss = 35.85547852516174\n",
      "epoch 103 runtime = 3.704519033432007\n",
      "start epoch 104\n",
      "epoch 104: running loss = 3.2595930099487305\n",
      "epoch 104: running loss = 35.66773748397827\n",
      "epoch 104 runtime = 3.686424493789673\n",
      "start epoch 105\n",
      "epoch 105: running loss = 3.238759994506836\n",
      "epoch 105: running loss = 35.77161455154419\n",
      "epoch 105 runtime = 3.846607208251953\n",
      "start epoch 106\n",
      "epoch 106: running loss = 3.238759994506836\n",
      "epoch 106: running loss = 35.70926117897034\n",
      "epoch 106 runtime = 3.708265542984009\n",
      "start epoch 107\n",
      "epoch 107: running loss = 3.3220937252044678\n",
      "epoch 107: running loss = 35.813809871673584\n",
      "epoch 107 runtime = 3.721940040588379\n",
      "start epoch 108\n",
      "epoch 108: running loss = 3.25954270362854\n",
      "epoch 108: running loss = 35.89675235748291\n",
      "epoch 108 runtime = 3.699312686920166\n",
      "start epoch 109\n",
      "epoch 109: running loss = 3.1554126739501953\n",
      "epoch 109: running loss = 35.647178173065186\n",
      "epoch 109 runtime = 3.7165160179138184\n",
      "start epoch 110\n",
      "epoch 110: running loss = 3.238759994506836\n",
      "epoch 110: running loss = 35.79302525520325\n",
      "epoch 110 runtime = 3.6890716552734375\n",
      "start epoch 111\n",
      "epoch 111: running loss = 3.2595932483673096\n",
      "epoch 111: running loss = 35.79289937019348\n",
      "epoch 111 runtime = 3.708211898803711\n",
      "start epoch 112\n",
      "epoch 112: running loss = 3.238759994506836\n",
      "epoch 112: running loss = 35.8138370513916\n",
      "epoch 112 runtime = 3.7271244525909424\n",
      "start epoch 113\n",
      "epoch 113: running loss = 3.301259994506836\n",
      "epoch 113: running loss = 35.85536003112793\n",
      "epoch 113 runtime = 3.7178187370300293\n",
      "start epoch 114\n",
      "epoch 114: running loss = 3.1762123107910156\n",
      "epoch 114: running loss = 35.79292368888855\n",
      "epoch 114 runtime = 3.7307052612304688\n",
      "start epoch 115\n",
      "epoch 115: running loss = 3.300999402999878\n",
      "epoch 115: running loss = 35.751020193099976\n",
      "epoch 115 runtime = 3.7236597537994385\n",
      "start epoch 116\n",
      "epoch 116: running loss = 3.2804267406463623\n",
      "epoch 116: running loss = 35.66786456108093\n",
      "epoch 116 runtime = 3.7280240058898926\n",
      "start epoch 117\n",
      "epoch 117: running loss = 3.2179267406463623\n",
      "epoch 117: running loss = 35.7305269241333\n",
      "epoch 117 runtime = 3.7379772663116455\n",
      "start epoch 118\n",
      "epoch 118: running loss = 3.3219945430755615\n",
      "epoch 118: running loss = 36.00125598907471\n",
      "epoch 118 runtime = 3.7155075073242188\n",
      "start epoch 119\n",
      "epoch 119: running loss = 3.301259994506836\n",
      "epoch 119: running loss = 35.813093185424805\n",
      "epoch 119 runtime = 3.7468550205230713\n",
      "start epoch 120\n",
      "epoch 120: running loss = 3.2179267406463623\n",
      "epoch 120: running loss = 35.8134081363678\n",
      "epoch 120 runtime = 3.7274460792541504\n",
      "start epoch 121\n",
      "epoch 121: running loss = 3.238759994506836\n",
      "epoch 121: running loss = 35.70961785316467\n",
      "epoch 121 runtime = 3.715822219848633\n",
      "start epoch 122\n",
      "epoch 122: running loss = 3.2595832347869873\n",
      "epoch 122: running loss = 35.73009967803955\n",
      "epoch 122 runtime = 3.702181339263916\n",
      "start epoch 123\n",
      "epoch 123: running loss = 3.238759994506836\n",
      "epoch 123: running loss = 35.64707922935486\n",
      "epoch 123 runtime = 3.7295682430267334\n",
      "start epoch 124\n",
      "epoch 124: running loss = 3.280179977416992\n",
      "epoch 124: running loss = 35.875285148620605\n",
      "epoch 124 runtime = 3.7250282764434814\n",
      "start epoch 125\n",
      "epoch 125: running loss = 3.176259756088257\n",
      "epoch 125: running loss = 35.77206254005432\n",
      "epoch 125 runtime = 3.7439143657684326\n",
      "start epoch 126\n",
      "epoch 126: running loss = 3.2595932483673096\n",
      "epoch 126: running loss = 35.81364417076111\n",
      "epoch 126 runtime = 3.6990067958831787\n",
      "start epoch 127\n",
      "epoch 127: running loss = 3.2387619018554688\n",
      "epoch 127: running loss = 35.8346803188324\n",
      "epoch 127 runtime = 3.6858339309692383\n",
      "start epoch 128\n",
      "epoch 128: running loss = 3.2595932483673096\n",
      "epoch 128: running loss = 35.7302360534668\n",
      "epoch 128 runtime = 3.7164952754974365\n",
      "start epoch 129\n",
      "epoch 129: running loss = 3.259465456008911\n",
      "epoch 129: running loss = 35.709282875061035\n",
      "epoch 129 runtime = 3.6963322162628174\n",
      "start epoch 130\n",
      "epoch 130: running loss = 3.259467840194702\n",
      "epoch 130: running loss = 35.81313228607178\n",
      "epoch 130 runtime = 3.7042934894561768\n",
      "start epoch 131\n",
      "epoch 131: running loss = 3.259565591812134\n",
      "epoch 131: running loss = 35.895503759384155\n",
      "epoch 131 runtime = 3.7183051109313965\n",
      "start epoch 132\n",
      "epoch 132: running loss = 3.1970062255859375\n",
      "epoch 132: running loss = 35.77176547050476\n",
      "epoch 132 runtime = 3.73075532913208\n",
      "start epoch 133\n",
      "epoch 133: running loss = 3.2804195880889893\n",
      "epoch 133: running loss = 35.729188680648804\n",
      "epoch 133 runtime = 3.714982032775879\n",
      "start epoch 134\n",
      "epoch 134: running loss = 3.259368896484375\n",
      "epoch 134: running loss = 35.729562759399414\n",
      "epoch 134 runtime = 3.7399423122406006\n",
      "start epoch 135\n",
      "epoch 135: running loss = 3.2804267406463623\n",
      "epoch 135: running loss = 35.79252767562866\n",
      "epoch 135 runtime = 3.704566478729248\n",
      "start epoch 136\n",
      "epoch 136: running loss = 3.2595930099487305\n",
      "epoch 136: running loss = 35.95869183540344\n",
      "epoch 136 runtime = 3.7384910583496094\n",
      "start epoch 137\n",
      "epoch 137: running loss = 3.3220741748809814\n",
      "epoch 137: running loss = 35.83407926559448\n",
      "epoch 137 runtime = 3.7164032459259033\n",
      "start epoch 138\n",
      "epoch 138: running loss = 3.217926263809204\n",
      "epoch 138: running loss = 35.81310272216797\n",
      "epoch 138 runtime = 3.712480068206787\n",
      "start epoch 139\n",
      "epoch 139: running loss = 3.238759994506836\n",
      "epoch 139: running loss = 35.73011374473572\n",
      "epoch 139 runtime = 3.7159416675567627\n",
      "start epoch 140\n",
      "epoch 140: running loss = 3.280294418334961\n",
      "epoch 140: running loss = 35.79252862930298\n",
      "epoch 140 runtime = 3.6918492317199707\n",
      "start epoch 141\n",
      "epoch 141: running loss = 3.2595932483673096\n",
      "epoch 141: running loss = 35.6878080368042\n",
      "epoch 141 runtime = 3.734128475189209\n",
      "start epoch 142\n",
      "epoch 142: running loss = 3.176259756088257\n",
      "epoch 142: running loss = 35.6670184135437\n",
      "epoch 142 runtime = 3.731077194213867\n",
      "start epoch 143\n",
      "epoch 143: running loss = 3.2595913410186768\n",
      "epoch 143: running loss = 35.667699337005615\n",
      "epoch 143 runtime = 3.7544004917144775\n",
      "start epoch 144\n",
      "epoch 144: running loss = 3.2387592792510986\n",
      "epoch 144: running loss = 35.77156186103821\n",
      "epoch 144 runtime = 3.703416347503662\n",
      "start epoch 145\n",
      "epoch 145: running loss = 3.2384731769561768\n",
      "epoch 145: running loss = 35.72970700263977\n",
      "epoch 145 runtime = 3.7333767414093018\n",
      "start epoch 146\n",
      "epoch 146: running loss = 3.25927472114563\n",
      "epoch 146: running loss = 35.87440848350525\n",
      "epoch 146 runtime = 3.6998157501220703\n",
      "start epoch 147\n",
      "epoch 147: running loss = 3.2595784664154053\n",
      "epoch 147: running loss = 35.771461486816406\n",
      "epoch 147 runtime = 3.729412317276001\n",
      "start epoch 148\n",
      "epoch 148: running loss = 3.3011081218719482\n",
      "epoch 148: running loss = 35.771255016326904\n",
      "epoch 148 runtime = 3.710052490234375\n",
      "start epoch 149\n",
      "epoch 149: running loss = 3.301260232925415\n",
      "epoch 149: running loss = 35.77248406410217\n",
      "epoch 149 runtime = 3.6988372802734375\n",
      "start epoch 150\n",
      "epoch 150: running loss = 3.2179267406463623\n",
      "epoch 150: running loss = 35.70903968811035\n",
      "epoch 150 runtime = 3.721536159515381\n",
      "start epoch 151\n",
      "epoch 151: running loss = 3.2595932483673096\n",
      "epoch 151: running loss = 35.79245185852051\n",
      "epoch 151 runtime = 3.7178120613098145\n",
      "start epoch 152\n",
      "epoch 152: running loss = 3.2593066692352295\n",
      "epoch 152: running loss = 35.62545323371887\n",
      "epoch 152 runtime = 3.7156670093536377\n",
      "start epoch 153\n",
      "epoch 153: running loss = 3.2804267406463623\n",
      "epoch 153: running loss = 35.813305616378784\n",
      "epoch 153 runtime = 3.716402292251587\n",
      "start epoch 154\n",
      "epoch 154: running loss = 3.1970932483673096\n",
      "epoch 154: running loss = 35.667580127716064\n",
      "epoch 154 runtime = 3.7437267303466797\n",
      "start epoch 155\n",
      "epoch 155: running loss = 3.2800962924957275\n",
      "epoch 155: running loss = 35.75094246864319\n",
      "epoch 155 runtime = 3.733893632888794\n",
      "start epoch 156\n",
      "epoch 156: running loss = 3.238759994506836\n",
      "epoch 156: running loss = 35.792943239212036\n",
      "epoch 156 runtime = 3.703977108001709\n",
      "start epoch 157\n",
      "epoch 157: running loss = 3.2804250717163086\n",
      "epoch 157: running loss = 35.959397077560425\n",
      "epoch 157 runtime = 3.7284674644470215\n",
      "start epoch 158\n",
      "epoch 158: running loss = 3.2804267406463623\n",
      "epoch 158: running loss = 35.750818967819214\n",
      "epoch 158 runtime = 3.689751625061035\n",
      "start epoch 159\n",
      "epoch 159: running loss = 3.2594072818756104\n",
      "epoch 159: running loss = 35.9172203540802\n",
      "epoch 159 runtime = 3.702735424041748\n",
      "start epoch 160\n",
      "epoch 160: running loss = 3.238759994506836\n",
      "epoch 160: running loss = 35.771631717681885\n",
      "epoch 160 runtime = 3.7283501625061035\n",
      "start epoch 161\n",
      "epoch 161: running loss = 3.280410051345825\n",
      "epoch 161: running loss = 35.79260325431824\n",
      "epoch 161 runtime = 3.7155845165252686\n",
      "start epoch 162\n",
      "epoch 162: running loss = 3.2595932483673096\n",
      "epoch 162: running loss = 35.6878125667572\n",
      "epoch 162 runtime = 3.761120080947876\n",
      "start epoch 163\n",
      "epoch 163: running loss = 3.2385590076446533\n",
      "epoch 163: running loss = 35.750104904174805\n",
      "epoch 163 runtime = 3.688204526901245\n",
      "start epoch 164\n",
      "epoch 164: running loss = 3.2804267406463623\n",
      "epoch 164: running loss = 35.8135039806366\n",
      "epoch 164 runtime = 3.6976864337921143\n",
      "start epoch 165\n",
      "epoch 165: running loss = 3.2803914546966553\n",
      "epoch 165: running loss = 35.77198529243469\n",
      "epoch 165 runtime = 3.7024731636047363\n",
      "start epoch 166\n",
      "epoch 166: running loss = 3.238757848739624\n",
      "epoch 166: running loss = 35.7721426486969\n",
      "epoch 166 runtime = 3.728461503982544\n",
      "start epoch 167\n",
      "epoch 167: running loss = 3.2804269790649414\n",
      "epoch 167: running loss = 35.66792130470276\n",
      "epoch 167 runtime = 3.6926581859588623\n",
      "start epoch 168\n",
      "epoch 168: running loss = 3.2595932483673096\n",
      "epoch 168: running loss = 35.917763471603394\n",
      "epoch 168 runtime = 3.708588123321533\n",
      "start epoch 169\n",
      "epoch 169: running loss = 3.238759994506836\n",
      "epoch 169: running loss = 35.71608328819275\n",
      "epoch 169 runtime = 3.6996142864227295\n",
      "start epoch 170\n",
      "epoch 170: running loss = 3.2595932483673096\n",
      "epoch 170: running loss = 35.77536702156067\n",
      "epoch 170 runtime = 3.700547695159912\n",
      "start epoch 171\n",
      "epoch 171: running loss = 3.1970932483673096\n",
      "epoch 171: running loss = 35.623420000076294\n",
      "epoch 171 runtime = 3.7081048488616943\n",
      "start epoch 172\n",
      "epoch 172: running loss = 3.280116081237793\n",
      "epoch 172: running loss = 35.547929763793945\n",
      "epoch 172 runtime = 3.7049720287323\n",
      "start epoch 173\n",
      "epoch 173: running loss = 3.2592947483062744\n",
      "epoch 173: running loss = 35.562798261642456\n",
      "epoch 173 runtime = 3.7131972312927246\n",
      "start epoch 174\n",
      "epoch 174: running loss = 3.2178125381469727\n",
      "epoch 174: running loss = 35.626004457473755\n",
      "epoch 174 runtime = 3.6982808113098145\n",
      "start epoch 175\n",
      "epoch 175: running loss = 3.2803914546966553\n",
      "epoch 175: running loss = 35.687501668930054\n",
      "epoch 175 runtime = 3.7521004676818848\n",
      "start epoch 176\n",
      "epoch 176: running loss = 3.1762590408325195\n",
      "epoch 176: running loss = 35.64096450805664\n",
      "epoch 176 runtime = 3.7152390480041504\n",
      "start epoch 177\n",
      "epoch 177: running loss = 3.238698720932007\n",
      "epoch 177: running loss = 35.66729497909546\n",
      "epoch 177 runtime = 3.7491586208343506\n",
      "start epoch 178\n",
      "epoch 178: running loss = 3.238759994506836\n",
      "epoch 178: running loss = 35.60472321510315\n",
      "epoch 178 runtime = 3.70971941947937\n",
      "start epoch 179\n",
      "epoch 179: running loss = 3.2801990509033203\n",
      "epoch 179: running loss = 35.68768668174744\n",
      "epoch 179 runtime = 3.7409534454345703\n",
      "start epoch 180\n",
      "epoch 180: running loss = 3.23852276802063\n",
      "epoch 180: running loss = 35.729543685913086\n",
      "epoch 180 runtime = 3.7313075065612793\n",
      "start epoch 181\n",
      "epoch 181: running loss = 3.2595794200897217\n",
      "epoch 181: running loss = 35.6664023399353\n",
      "epoch 181 runtime = 3.70119047164917\n",
      "start epoch 182\n",
      "epoch 182: running loss = 3.3008203506469727\n",
      "epoch 182: running loss = 35.6878707408905\n",
      "epoch 182 runtime = 3.7396883964538574\n",
      "start epoch 183\n",
      "epoch 183: running loss = 3.2179224491119385\n",
      "epoch 183: running loss = 35.66665077209473\n",
      "epoch 183 runtime = 3.687284469604492\n",
      "start epoch 184\n",
      "epoch 184: running loss = 3.2386913299560547\n",
      "epoch 184: running loss = 35.68788194656372\n",
      "epoch 184 runtime = 3.713266372680664\n",
      "start epoch 185\n",
      "epoch 185: running loss = 3.1970880031585693\n",
      "epoch 185: running loss = 35.604488134384155\n",
      "epoch 185 runtime = 3.7200100421905518\n",
      "start epoch 186\n",
      "epoch 186: running loss = 3.238759994506836\n",
      "epoch 186: running loss = 35.58289837837219\n",
      "epoch 186 runtime = 3.6974985599517822\n",
      "start epoch 187\n",
      "epoch 187: running loss = 3.2594451904296875\n",
      "epoch 187: running loss = 35.770522594451904\n",
      "epoch 187 runtime = 3.7033591270446777\n",
      "start epoch 188\n",
      "epoch 188: running loss = 3.2804203033447266\n",
      "epoch 188: running loss = 35.708083152770996\n",
      "epoch 188 runtime = 3.7355241775512695\n",
      "start epoch 189\n",
      "epoch 189: running loss = 3.238663673400879\n",
      "epoch 189: running loss = 35.6875205039978\n",
      "epoch 189 runtime = 3.7094368934631348\n",
      "start epoch 190\n",
      "epoch 190: running loss = 3.259589433670044\n",
      "epoch 190: running loss = 35.72941589355469\n",
      "epoch 190 runtime = 3.733266830444336\n",
      "start epoch 191\n",
      "epoch 191: running loss = 3.3012168407440186\n",
      "epoch 191: running loss = 35.66731929779053\n",
      "epoch 191 runtime = 3.753283739089966\n",
      "start epoch 192\n",
      "epoch 192: running loss = 3.259587049484253\n",
      "epoch 192: running loss = 35.68784141540527\n",
      "epoch 192 runtime = 3.707848310470581\n",
      "start epoch 193\n",
      "epoch 193: running loss = 3.217902183532715\n",
      "epoch 193: running loss = 35.60378694534302\n",
      "epoch 193 runtime = 3.711658477783203\n",
      "start epoch 194\n",
      "epoch 194: running loss = 3.2797939777374268\n",
      "epoch 194: running loss = 35.56164002418518\n",
      "epoch 194 runtime = 3.7472097873687744\n",
      "start epoch 195\n",
      "epoch 195: running loss = 3.280407667160034\n",
      "epoch 195: running loss = 35.60521101951599\n",
      "epoch 195 runtime = 3.749404191970825\n",
      "start epoch 196\n",
      "epoch 196: running loss = 3.259434700012207\n",
      "epoch 196: running loss = 35.59815287590027\n",
      "epoch 196 runtime = 3.757388114929199\n",
      "start epoch 197\n",
      "epoch 197: running loss = 3.217768669128418\n",
      "epoch 197: running loss = 35.60311245918274\n",
      "epoch 197 runtime = 3.7129111289978027\n",
      "start epoch 198\n",
      "epoch 198: running loss = 3.300987958908081\n",
      "epoch 198: running loss = 35.51995372772217\n",
      "epoch 198 runtime = 3.7287445068359375\n",
      "start epoch 199\n",
      "epoch 199: running loss = 3.2179548740386963\n",
      "epoch 199: running loss = 35.56239128112793\n",
      "epoch 199 runtime = 3.7172820568084717\n",
      "start epoch 200\n",
      "epoch 200: running loss = 3.2387571334838867\n",
      "epoch 200: running loss = 35.58265209197998\n",
      "epoch 200 runtime = 3.7440435886383057\n",
      "start epoch 201\n",
      "epoch 201: running loss = 3.155061721801758\n",
      "epoch 201: running loss = 35.58295798301697\n",
      "epoch 201 runtime = 3.7296786308288574\n",
      "start epoch 202\n",
      "epoch 202: running loss = 3.2384417057037354\n",
      "epoch 202: running loss = 35.583064794540405\n",
      "epoch 202 runtime = 3.7419705390930176\n",
      "start epoch 203\n",
      "epoch 203: running loss = 3.259155035018921\n",
      "epoch 203: running loss = 35.66591143608093\n",
      "epoch 203 runtime = 3.72627854347229\n",
      "start epoch 204\n",
      "epoch 204: running loss = 3.280329942703247\n",
      "epoch 204: running loss = 35.64651560783386\n",
      "epoch 204 runtime = 3.7725064754486084\n",
      "start epoch 205\n",
      "epoch 205: running loss = 3.301260232925415\n",
      "epoch 205: running loss = 35.52073073387146\n",
      "epoch 205 runtime = 3.709205389022827\n",
      "start epoch 206\n",
      "epoch 206: running loss = 3.1762514114379883\n",
      "epoch 206: running loss = 35.43712282180786\n",
      "epoch 206 runtime = 3.7873098850250244\n",
      "start epoch 207\n",
      "epoch 207: running loss = 3.1970930099487305\n",
      "epoch 207: running loss = 35.646404504776\n",
      "epoch 207 runtime = 3.756105899810791\n",
      "start epoch 208\n",
      "epoch 208: running loss = 3.2590839862823486\n",
      "epoch 208: running loss = 35.66600441932678\n",
      "epoch 208 runtime = 3.7649035453796387\n",
      "start epoch 209\n",
      "epoch 209: running loss = 3.238656997680664\n",
      "epoch 209: running loss = 35.500505208969116\n",
      "epoch 209 runtime = 3.7113306522369385\n",
      "start epoch 210\n",
      "epoch 210: running loss = 3.25948166847229\n",
      "epoch 210: running loss = 35.56298613548279\n",
      "epoch 210 runtime = 3.7313485145568848\n",
      "start epoch 211\n",
      "epoch 211: running loss = 3.238759994506836\n",
      "epoch 211: running loss = 35.666032791137695\n",
      "epoch 211 runtime = 3.7500386238098145\n",
      "start epoch 212\n",
      "epoch 212: running loss = 3.2177202701568604\n",
      "epoch 212: running loss = 35.52094507217407\n",
      "epoch 212 runtime = 3.769335985183716\n",
      "start epoch 213\n",
      "epoch 213: running loss = 3.259589910507202\n",
      "epoch 213: running loss = 35.4997820854187\n",
      "epoch 213 runtime = 3.748779773712158\n",
      "start epoch 214\n",
      "epoch 214: running loss = 3.2383596897125244\n",
      "epoch 214: running loss = 35.56257367134094\n",
      "epoch 214 runtime = 3.727036237716675\n",
      "start epoch 215\n",
      "epoch 215: running loss = 3.1759893894195557\n",
      "epoch 215: running loss = 35.499385356903076\n",
      "epoch 215 runtime = 3.749396324157715\n",
      "start epoch 216\n",
      "epoch 216: running loss = 3.196841239929199\n",
      "epoch 216: running loss = 35.47894763946533\n",
      "epoch 216 runtime = 3.7461249828338623\n",
      "start epoch 217\n",
      "epoch 217: running loss = 3.2595913410186768\n",
      "epoch 217: running loss = 35.541696548461914\n",
      "epoch 217 runtime = 3.7610220909118652\n",
      "start epoch 218\n",
      "epoch 218: running loss = 3.1970436573028564\n",
      "epoch 218: running loss = 35.5823073387146\n",
      "epoch 218 runtime = 3.733621597290039\n",
      "start epoch 219\n",
      "epoch 219: running loss = 3.2793266773223877\n",
      "epoch 219: running loss = 35.51947283744812\n",
      "epoch 219 runtime = 3.730623483657837\n",
      "start epoch 220\n",
      "epoch 220: running loss = 3.175999402999878\n",
      "epoch 220: running loss = 35.58199214935303\n",
      "epoch 220 runtime = 3.721893787384033\n",
      "start epoch 221\n",
      "epoch 221: running loss = 3.2804267406463623\n",
      "epoch 221: running loss = 35.54080128669739\n",
      "epoch 221 runtime = 3.7485010623931885\n",
      "start epoch 222\n",
      "epoch 222: running loss = 3.2590982913970947\n",
      "epoch 222: running loss = 35.602779388427734\n",
      "epoch 222 runtime = 3.758931875228882\n",
      "start epoch 223\n",
      "epoch 223: running loss = 3.2799251079559326\n",
      "epoch 223: running loss = 35.56151342391968\n",
      "epoch 223 runtime = 3.7673745155334473\n",
      "start epoch 224\n",
      "epoch 224: running loss = 3.2804059982299805\n",
      "epoch 224: running loss = 35.581942319869995\n",
      "epoch 224 runtime = 3.7316577434539795\n",
      "start epoch 225\n",
      "epoch 225: running loss = 3.2592782974243164\n",
      "epoch 225: running loss = 35.51993918418884\n",
      "epoch 225 runtime = 3.7508108615875244\n",
      "start epoch 226\n",
      "epoch 226: running loss = 3.2179253101348877\n",
      "epoch 226: running loss = 35.49935269355774\n",
      "epoch 226 runtime = 3.7398877143859863\n",
      "start epoch 227\n",
      "epoch 227: running loss = 3.2174625396728516\n",
      "epoch 227: running loss = 35.54094338417053\n",
      "epoch 227 runtime = 3.758516550064087\n",
      "start epoch 228\n",
      "epoch 228: running loss = 3.238754987716675\n",
      "epoch 228: running loss = 35.436158895492554\n",
      "epoch 228 runtime = 3.7132341861724854\n",
      "start epoch 229\n",
      "epoch 229: running loss = 3.1760025024414062\n",
      "epoch 229: running loss = 35.58207559585571\n",
      "epoch 229 runtime = 3.7270374298095703\n",
      "start epoch 230\n",
      "epoch 230: running loss = 3.1969082355499268\n",
      "epoch 230: running loss = 35.560452699661255\n",
      "epoch 230 runtime = 3.7240240573883057\n",
      "start epoch 231\n",
      "epoch 231: running loss = 3.2173500061035156\n",
      "epoch 231: running loss = 35.541253089904785\n",
      "epoch 231 runtime = 3.746415615081787\n",
      "start epoch 232\n",
      "epoch 232: running loss = 3.19657039642334\n",
      "epoch 232: running loss = 35.62435460090637\n",
      "epoch 232 runtime = 3.754593849182129\n",
      "start epoch 233\n",
      "epoch 233: running loss = 3.217926263809204\n",
      "epoch 233: running loss = 35.604034662246704\n",
      "epoch 233 runtime = 3.7677533626556396\n",
      "start epoch 234\n",
      "epoch 234: running loss = 3.092784881591797\n",
      "epoch 234: running loss = 35.45774292945862\n",
      "epoch 234 runtime = 3.7378625869750977\n",
      "start epoch 235\n",
      "epoch 235: running loss = 3.2382800579071045\n",
      "epoch 235: running loss = 35.47810745239258\n",
      "epoch 235 runtime = 3.7404749393463135\n",
      "start epoch 236\n",
      "epoch 236: running loss = 3.2385826110839844\n",
      "epoch 236: running loss = 35.70639085769653\n",
      "epoch 236 runtime = 3.7078402042388916\n",
      "start epoch 237\n",
      "epoch 237: running loss = 3.1969738006591797\n",
      "epoch 237: running loss = 35.64412260055542\n",
      "epoch 237 runtime = 3.7086105346679688\n",
      "start epoch 238\n",
      "epoch 238: running loss = 3.238271474838257\n",
      "epoch 238: running loss = 35.66509771347046\n",
      "epoch 238 runtime = 3.7308802604675293\n",
      "start epoch 239\n",
      "epoch 239: running loss = 3.3010714054107666\n",
      "epoch 239: running loss = 35.5820198059082\n",
      "epoch 239 runtime = 3.737271308898926\n",
      "start epoch 240\n",
      "epoch 240: running loss = 3.217780351638794\n",
      "epoch 240: running loss = 35.58275747299194\n",
      "epoch 240 runtime = 3.7386398315429688\n",
      "start epoch 241\n",
      "epoch 241: running loss = 3.217606544494629\n",
      "epoch 241: running loss = 35.62433385848999\n",
      "epoch 241 runtime = 3.7171149253845215\n",
      "start epoch 242\n",
      "epoch 242: running loss = 3.3007638454437256\n",
      "epoch 242: running loss = 35.54071855545044\n",
      "epoch 242 runtime = 3.745051860809326\n",
      "start epoch 243\n",
      "epoch 243: running loss = 3.2176036834716797\n",
      "epoch 243: running loss = 35.5822651386261\n",
      "epoch 243 runtime = 3.7138142585754395\n",
      "start epoch 244\n",
      "epoch 244: running loss = 3.1970930099487305\n",
      "epoch 244: running loss = 35.56194806098938\n",
      "epoch 244 runtime = 3.722500801086426\n",
      "start epoch 245\n",
      "epoch 245: running loss = 3.25958514213562\n",
      "epoch 245: running loss = 35.49988842010498\n",
      "epoch 245 runtime = 3.7303895950317383\n",
      "start epoch 246\n",
      "epoch 246: running loss = 3.2384426593780518\n",
      "epoch 246: running loss = 35.686415910720825\n",
      "epoch 246 runtime = 3.7374250888824463\n",
      "start epoch 247\n",
      "epoch 247: running loss = 3.2179267406463623\n",
      "epoch 247: running loss = 35.624518632888794\n",
      "epoch 247 runtime = 3.7657952308654785\n",
      "start epoch 248\n",
      "epoch 248: running loss = 3.2384283542633057\n",
      "epoch 248: running loss = 35.66594839096069\n",
      "epoch 248 runtime = 3.7171378135681152\n",
      "start epoch 249\n",
      "epoch 249: running loss = 3.2595927715301514\n",
      "epoch 249: running loss = 35.70758366584778\n",
      "epoch 249 runtime = 3.7307231426239014\n",
      "start epoch 250\n",
      "epoch 250: running loss = 3.259279489517212\n",
      "epoch 250: running loss = 35.707305669784546\n",
      "epoch 250 runtime = 3.770534038543701\n",
      "start epoch 251\n",
      "epoch 251: running loss = 3.2595932483673096\n",
      "epoch 251: running loss = 35.5191912651062\n",
      "epoch 251 runtime = 3.757648468017578\n",
      "start epoch 252\n",
      "epoch 252: running loss = 3.238454818725586\n",
      "epoch 252: running loss = 35.665629863739014\n",
      "epoch 252 runtime = 3.748721122741699\n",
      "start epoch 253\n",
      "epoch 253: running loss = 3.238467216491699\n",
      "epoch 253: running loss = 35.541037797927856\n",
      "epoch 253 runtime = 3.7264671325683594\n",
      "start epoch 254\n",
      "epoch 254: running loss = 3.1970930099487305\n",
      "epoch 254: running loss = 35.60306477546692\n",
      "epoch 254 runtime = 3.7625174522399902\n",
      "start epoch 255\n",
      "epoch 255: running loss = 3.259169578552246\n",
      "epoch 255: running loss = 35.49852466583252\n",
      "epoch 255 runtime = 3.7554380893707275\n",
      "start epoch 256\n",
      "epoch 256: running loss = 3.2586517333984375\n",
      "epoch 256: running loss = 35.498823404312134\n",
      "epoch 256 runtime = 3.734818935394287\n",
      "start epoch 257\n",
      "epoch 257: running loss = 3.197092056274414\n",
      "epoch 257: running loss = 35.478216886520386\n",
      "epoch 257 runtime = 3.729919195175171\n",
      "start epoch 258\n",
      "epoch 258: running loss = 3.2590529918670654\n",
      "epoch 258: running loss = 35.54067635536194\n",
      "epoch 258 runtime = 3.7510392665863037\n",
      "start epoch 259\n",
      "epoch 259: running loss = 3.2178869247436523\n",
      "epoch 259: running loss = 35.5400927066803\n",
      "epoch 259 runtime = 3.736253261566162\n",
      "start epoch 260\n",
      "epoch 260: running loss = 3.2179248332977295\n",
      "epoch 260: running loss = 35.581366300582886\n",
      "epoch 260 runtime = 3.7117245197296143\n",
      "start epoch 261\n",
      "epoch 261: running loss = 3.2175559997558594\n",
      "epoch 261: running loss = 35.49855136871338\n",
      "epoch 261 runtime = 3.7611706256866455\n",
      "start epoch 262\n",
      "epoch 262: running loss = 3.1137468814849854\n",
      "epoch 262: running loss = 35.477131843566895\n",
      "epoch 262 runtime = 3.7353756427764893\n",
      "start epoch 263\n",
      "epoch 263: running loss = 3.1970930099487305\n",
      "epoch 263: running loss = 35.60408163070679\n",
      "epoch 263 runtime = 3.7319791316986084\n",
      "start epoch 264\n",
      "epoch 264: running loss = 3.2387571334838867\n",
      "epoch 264: running loss = 35.520894050598145\n",
      "epoch 264 runtime = 3.722080945968628\n",
      "start epoch 265\n",
      "epoch 265: running loss = 3.259493827819824\n",
      "epoch 265: running loss = 35.599913597106934\n",
      "epoch 265 runtime = 3.7539401054382324\n",
      "start epoch 266\n",
      "epoch 266: running loss = 3.21791934967041\n",
      "epoch 266: running loss = 35.52000665664673\n",
      "epoch 266 runtime = 3.7481865882873535\n",
      "start epoch 267\n",
      "epoch 267: running loss = 3.217923879623413\n",
      "epoch 267: running loss = 35.58209753036499\n",
      "epoch 267 runtime = 3.7216594219207764\n",
      "start epoch 268\n",
      "epoch 268: running loss = 3.2595860958099365\n",
      "epoch 268: running loss = 35.706870555877686\n",
      "epoch 268 runtime = 3.7425007820129395\n",
      "start epoch 269\n",
      "epoch 269: running loss = 3.2384378910064697\n",
      "epoch 269: running loss = 35.582847118377686\n",
      "epoch 269 runtime = 3.7559328079223633\n",
      "start epoch 270\n",
      "epoch 270: running loss = 3.196758508682251\n",
      "epoch 270: running loss = 35.58283448219299\n",
      "epoch 270 runtime = 3.7666404247283936\n",
      "start epoch 271\n",
      "epoch 271: running loss = 3.217926263809204\n",
      "epoch 271: running loss = 35.47933268547058\n",
      "epoch 271 runtime = 3.740280866622925\n",
      "start epoch 272\n",
      "epoch 272: running loss = 3.1968820095062256\n",
      "epoch 272: running loss = 35.5401725769043\n",
      "epoch 272 runtime = 3.7668886184692383\n",
      "start epoch 273\n",
      "epoch 273: running loss = 3.2386608123779297\n",
      "epoch 273: running loss = 35.68630003929138\n",
      "epoch 273 runtime = 3.7405669689178467\n",
      "start epoch 274\n",
      "epoch 274: running loss = 3.196795701980591\n",
      "epoch 274: running loss = 35.58192563056946\n",
      "epoch 274 runtime = 3.7109951972961426\n",
      "start epoch 275\n",
      "epoch 275: running loss = 3.175950050354004\n",
      "epoch 275: running loss = 35.624003887176514\n",
      "epoch 275 runtime = 3.7307136058807373\n",
      "start epoch 276\n",
      "epoch 276: running loss = 3.2387592792510986\n",
      "epoch 276: running loss = 35.6034677028656\n",
      "epoch 276 runtime = 3.7606890201568604\n",
      "start epoch 277\n",
      "epoch 277: running loss = 3.217694044113159\n",
      "epoch 277: running loss = 35.56197190284729\n",
      "epoch 277 runtime = 3.7546157836914062\n",
      "start epoch 278\n",
      "epoch 278: running loss = 3.2179243564605713\n",
      "epoch 278: running loss = 35.68693828582764\n",
      "epoch 278 runtime = 3.75935697555542\n",
      "start epoch 279\n",
      "epoch 279: running loss = 3.2592670917510986\n",
      "epoch 279: running loss = 35.62433624267578\n",
      "epoch 279 runtime = 3.73994779586792\n",
      "start epoch 280\n",
      "epoch 280: running loss = 3.217780351638794\n",
      "epoch 280: running loss = 35.6656014919281\n",
      "epoch 280 runtime = 3.775822877883911\n",
      "start epoch 281\n",
      "epoch 281: running loss = 3.2386410236358643\n",
      "epoch 281: running loss = 35.562861919403076\n",
      "epoch 281 runtime = 3.7575466632843018\n",
      "start epoch 282\n",
      "epoch 282: running loss = 3.0928966999053955\n",
      "epoch 282: running loss = 35.537787437438965\n",
      "epoch 282 runtime = 3.7388782501220703\n",
      "start epoch 283\n",
      "epoch 283: running loss = 3.2802953720092773\n",
      "epoch 283: running loss = 35.644853830337524\n",
      "epoch 283 runtime = 3.7563703060150146\n",
      "start epoch 284\n",
      "epoch 284: running loss = 3.1759090423583984\n",
      "epoch 284: running loss = 35.47647500038147\n",
      "epoch 284 runtime = 3.7445361614227295\n",
      "start epoch 285\n",
      "epoch 285: running loss = 3.2800731658935547\n",
      "epoch 285: running loss = 35.65107870101929\n",
      "epoch 285 runtime = 3.7387452125549316\n",
      "start epoch 286\n",
      "epoch 286: running loss = 3.2387535572052\n",
      "epoch 286: running loss = 35.49939584732056\n",
      "epoch 286 runtime = 3.716954469680786\n",
      "start epoch 287\n",
      "epoch 287: running loss = 3.238353729248047\n",
      "epoch 287: running loss = 35.58361601829529\n",
      "epoch 287 runtime = 3.755495548248291\n",
      "start epoch 288\n",
      "epoch 288: running loss = 3.2595818042755127\n",
      "epoch 288: running loss = 35.70786809921265\n",
      "epoch 288 runtime = 3.713040351867676\n",
      "start epoch 289\n",
      "epoch 289: running loss = 3.2589991092681885\n",
      "epoch 289: running loss = 35.53972673416138\n",
      "epoch 289 runtime = 3.728760242462158\n",
      "start epoch 290\n",
      "epoch 290: running loss = 3.238506317138672\n",
      "epoch 290: running loss = 35.519673347473145\n",
      "epoch 290 runtime = 3.710012197494507\n",
      "start epoch 291\n",
      "epoch 291: running loss = 3.1970880031585693\n",
      "epoch 291: running loss = 35.43653655052185\n",
      "epoch 291 runtime = 3.707467794418335\n",
      "start epoch 292\n",
      "epoch 292: running loss = 3.238525152206421\n",
      "epoch 292: running loss = 35.52031850814819\n",
      "epoch 292 runtime = 3.745408773422241\n",
      "start epoch 293\n",
      "epoch 293: running loss = 3.1969330310821533\n",
      "epoch 293: running loss = 35.52186560630798\n",
      "epoch 293 runtime = 3.7222447395324707\n",
      "start epoch 294\n",
      "epoch 294: running loss = 3.301069974899292\n",
      "epoch 294: running loss = 35.51982593536377\n",
      "epoch 294 runtime = 3.751103639602661\n",
      "start epoch 295\n",
      "epoch 295: running loss = 3.280390977859497\n",
      "epoch 295: running loss = 35.52022862434387\n",
      "epoch 295 runtime = 3.74358868598938\n",
      "start epoch 296\n",
      "epoch 296: running loss = 3.30106258392334\n",
      "epoch 296: running loss = 35.64511513710022\n",
      "epoch 296 runtime = 3.727328300476074\n",
      "start epoch 297\n",
      "epoch 297: running loss = 3.238455057144165\n",
      "epoch 297: running loss = 35.6447594165802\n",
      "epoch 297 runtime = 3.7405664920806885\n",
      "start epoch 298\n",
      "epoch 298: running loss = 3.237898588180542\n",
      "epoch 298: running loss = 35.58293890953064\n",
      "epoch 298 runtime = 3.7530102729797363\n",
      "start epoch 299\n",
      "epoch 299: running loss = 3.238745927810669\n",
      "epoch 299: running loss = 35.60330152511597\n",
      "epoch 299 runtime = 3.7260427474975586\n",
      "start epoch 300\n",
      "epoch 300: running loss = 3.2587249279022217\n",
      "epoch 300: running loss = 35.51928353309631\n",
      "epoch 300 runtime = 3.752838373184204\n",
      "start epoch 301\n",
      "epoch 301: running loss = 3.2801036834716797\n",
      "epoch 301: running loss = 35.66537642478943\n",
      "epoch 301 runtime = 3.716735363006592\n",
      "start epoch 302\n",
      "epoch 302: running loss = 3.2172353267669678\n",
      "epoch 302: running loss = 35.53949522972107\n",
      "epoch 302 runtime = 3.727304220199585\n",
      "start epoch 303\n",
      "epoch 303: running loss = 3.2595927715301514\n",
      "epoch 303: running loss = 35.54080772399902\n",
      "epoch 303 runtime = 3.763233184814453\n",
      "start epoch 304\n",
      "epoch 304: running loss = 3.217902421951294\n",
      "epoch 304: running loss = 35.53987669944763\n",
      "epoch 304 runtime = 3.726815938949585\n",
      "start epoch 305\n",
      "epoch 305: running loss = 3.25917649269104\n",
      "epoch 305: running loss = 35.560465812683105\n",
      "epoch 305 runtime = 3.739386796951294\n",
      "start epoch 306\n",
      "epoch 306: running loss = 3.2178609371185303\n",
      "epoch 306: running loss = 35.60326838493347\n",
      "epoch 306 runtime = 3.715561866760254\n",
      "start epoch 307\n",
      "epoch 307: running loss = 3.2590789794921875\n",
      "epoch 307: running loss = 35.64386034011841\n",
      "epoch 307 runtime = 3.752532958984375\n",
      "start epoch 308\n",
      "epoch 308: running loss = 3.2179195880889893\n",
      "epoch 308: running loss = 35.56069588661194\n",
      "epoch 308 runtime = 3.7441909313201904\n",
      "start epoch 309\n",
      "epoch 309: running loss = 3.2178332805633545\n",
      "epoch 309: running loss = 35.54021215438843\n",
      "epoch 309 runtime = 3.729114532470703\n",
      "start epoch 310\n",
      "epoch 310: running loss = 3.2595932483673096\n",
      "epoch 310: running loss = 35.49980711936951\n",
      "epoch 310 runtime = 3.7221763134002686\n",
      "start epoch 311\n",
      "epoch 311: running loss = 3.1970930099487305\n",
      "epoch 311: running loss = 35.60443592071533\n",
      "epoch 311 runtime = 3.7402584552764893\n",
      "start epoch 312\n",
      "epoch 312: running loss = 3.2593190670013428\n",
      "epoch 312: running loss = 35.52167272567749\n",
      "epoch 312 runtime = 3.7092370986938477\n",
      "start epoch 313\n",
      "epoch 313: running loss = 3.280426025390625\n",
      "epoch 313: running loss = 35.64596104621887\n",
      "epoch 313 runtime = 3.7234573364257812\n",
      "start epoch 314\n",
      "epoch 314: running loss = 3.197092294692993\n",
      "epoch 314: running loss = 35.54211115837097\n",
      "epoch 314 runtime = 3.7307004928588867\n",
      "start epoch 315\n",
      "epoch 315: running loss = 3.2179181575775146\n",
      "epoch 315: running loss = 35.438398599624634\n",
      "epoch 315 runtime = 3.763324499130249\n",
      "start epoch 316\n",
      "epoch 316: running loss = 3.1762304306030273\n",
      "epoch 316: running loss = 35.60390496253967\n",
      "epoch 316 runtime = 3.755755662918091\n",
      "start epoch 317\n",
      "epoch 317: running loss = 3.155426263809204\n",
      "epoch 317: running loss = 35.58317160606384\n",
      "epoch 317 runtime = 3.7580268383026123\n",
      "start epoch 318\n",
      "epoch 318: running loss = 3.1969382762908936\n",
      "epoch 318: running loss = 35.6040415763855\n",
      "epoch 318 runtime = 3.7533881664276123\n",
      "start epoch 319\n",
      "epoch 319: running loss = 3.2179219722747803\n",
      "epoch 319: running loss = 35.56145668029785\n",
      "epoch 319 runtime = 3.722365140914917\n",
      "start epoch 320\n",
      "epoch 320: running loss = 3.2804267406463623\n",
      "epoch 320: running loss = 35.644222259521484\n",
      "epoch 320 runtime = 3.7450475692749023\n",
      "start epoch 321\n",
      "epoch 321: running loss = 3.238757848739624\n",
      "epoch 321: running loss = 35.436946630477905\n",
      "epoch 321 runtime = 3.751431465148926\n",
      "start epoch 322\n",
      "epoch 322: running loss = 3.196716070175171\n",
      "epoch 322: running loss = 35.54135274887085\n",
      "epoch 322 runtime = 3.7372543811798096\n",
      "start epoch 323\n",
      "epoch 323: running loss = 3.197033166885376\n",
      "epoch 323: running loss = 35.58124279975891\n",
      "epoch 323 runtime = 3.752711296081543\n",
      "start epoch 324\n",
      "epoch 324: running loss = 3.2176380157470703\n",
      "epoch 324: running loss = 35.47833967208862\n",
      "epoch 324 runtime = 3.7596335411071777\n",
      "start epoch 325\n",
      "epoch 325: running loss = 3.259361505508423\n",
      "epoch 325: running loss = 35.562018394470215\n",
      "epoch 325 runtime = 3.7196273803710938\n",
      "start epoch 326\n",
      "epoch 326: running loss = 3.217634439468384\n",
      "epoch 326: running loss = 35.70692014694214\n",
      "epoch 326 runtime = 3.739471912384033\n",
      "start epoch 327\n",
      "epoch 327: running loss = 3.259491205215454\n",
      "epoch 327: running loss = 35.47745490074158\n",
      "epoch 327 runtime = 3.724433660507202\n",
      "start epoch 328\n",
      "epoch 328: running loss = 3.197028398513794\n",
      "epoch 328: running loss = 35.58168983459473\n",
      "epoch 328 runtime = 3.7311573028564453\n",
      "start epoch 329\n",
      "epoch 329: running loss = 3.2595930099487305\n",
      "epoch 329: running loss = 35.64472055435181\n",
      "epoch 329 runtime = 3.7250609397888184\n",
      "start epoch 330\n",
      "epoch 330: running loss = 3.1550216674804688\n",
      "epoch 330: running loss = 35.583176612854004\n",
      "epoch 330 runtime = 3.7248101234436035\n",
      "start epoch 331\n",
      "epoch 331: running loss = 3.2387590408325195\n",
      "epoch 331: running loss = 35.58166027069092\n",
      "epoch 331 runtime = 3.7134718894958496\n",
      "start epoch 332\n",
      "epoch 332: running loss = 3.155425786972046\n",
      "epoch 332: running loss = 35.56140470504761\n",
      "epoch 332 runtime = 3.7098429203033447\n",
      "start epoch 333\n",
      "epoch 333: running loss = 3.2384393215179443\n",
      "epoch 333: running loss = 35.62321186065674\n",
      "epoch 333 runtime = 3.748164176940918\n",
      "start epoch 334\n",
      "epoch 334: running loss = 3.2595441341400146\n",
      "epoch 334: running loss = 35.70690941810608\n",
      "epoch 334 runtime = 3.7122480869293213\n",
      "start epoch 335\n",
      "epoch 335: running loss = 3.258854866027832\n",
      "epoch 335: running loss = 35.49878764152527\n",
      "epoch 335 runtime = 3.7454781532287598\n",
      "start epoch 336\n",
      "epoch 336: running loss = 3.2179250717163086\n",
      "epoch 336: running loss = 35.81189942359924\n",
      "epoch 336 runtime = 3.733184576034546\n",
      "start epoch 337\n",
      "epoch 337: running loss = 3.2384440898895264\n",
      "epoch 337: running loss = 35.58145046234131\n",
      "epoch 337 runtime = 3.72532320022583\n",
      "start epoch 338\n",
      "epoch 338: running loss = 3.280315399169922\n",
      "epoch 338: running loss = 35.64397430419922\n",
      "epoch 338 runtime = 3.73443603515625\n",
      "start epoch 339\n",
      "epoch 339: running loss = 3.2387187480926514\n",
      "epoch 339: running loss = 35.43601965904236\n",
      "epoch 339 runtime = 3.7559714317321777\n",
      "start epoch 340\n",
      "epoch 340: running loss = 3.238511323928833\n",
      "epoch 340: running loss = 35.60286211967468\n",
      "epoch 340 runtime = 3.781301259994507\n",
      "start epoch 341\n",
      "epoch 341: running loss = 3.2595837116241455\n",
      "epoch 341: running loss = 35.582475900650024\n",
      "epoch 341 runtime = 3.729443073272705\n",
      "start epoch 342\n",
      "epoch 342: running loss = 3.2593019008636475\n",
      "epoch 342: running loss = 35.561280727386475\n",
      "epoch 342 runtime = 3.7574362754821777\n",
      "start epoch 343\n",
      "epoch 343: running loss = 3.2179267406463623\n",
      "epoch 343: running loss = 35.56123495101929\n",
      "epoch 343 runtime = 3.745116949081421\n",
      "start epoch 344\n",
      "epoch 344: running loss = 3.1958515644073486\n",
      "epoch 344: running loss = 35.45681691169739\n",
      "epoch 344 runtime = 3.763211488723755\n",
      "start epoch 345\n",
      "epoch 345: running loss = 3.280111312866211\n",
      "epoch 345: running loss = 35.62384653091431\n",
      "epoch 345 runtime = 3.729641914367676\n",
      "start epoch 346\n",
      "epoch 346: running loss = 3.2176363468170166\n",
      "epoch 346: running loss = 35.56129765510559\n",
      "epoch 346 runtime = 3.726210594177246\n",
      "start epoch 347\n",
      "epoch 347: running loss = 3.155426263809204\n",
      "epoch 347: running loss = 35.41545867919922\n",
      "epoch 347 runtime = 3.743316411972046\n",
      "start epoch 348\n",
      "epoch 348: running loss = 3.2798779010772705\n",
      "epoch 348: running loss = 35.5197536945343\n",
      "epoch 348 runtime = 3.7429566383361816\n",
      "start epoch 349\n",
      "epoch 349: running loss = 3.134420394897461\n",
      "epoch 349: running loss = 35.62292551994324\n",
      "epoch 349 runtime = 3.7519423961639404\n",
      "start epoch 350\n",
      "epoch 350: running loss = 3.237848997116089\n",
      "epoch 350: running loss = 35.560625076293945\n",
      "epoch 350 runtime = 3.7463760375976562\n",
      "start epoch 351\n",
      "epoch 351: running loss = 3.2803192138671875\n",
      "epoch 351: running loss = 35.60262322425842\n",
      "epoch 351 runtime = 3.7387466430664062\n",
      "start epoch 352\n",
      "epoch 352: running loss = 3.279911756515503\n",
      "epoch 352: running loss = 35.49770164489746\n",
      "epoch 352 runtime = 3.7277729511260986\n",
      "start epoch 353\n",
      "epoch 353: running loss = 3.238754987716675\n",
      "epoch 353: running loss = 35.60237693786621\n",
      "epoch 353 runtime = 3.7077460289001465\n",
      "start epoch 354\n",
      "epoch 354: running loss = 3.238757848739624\n",
      "epoch 354: running loss = 35.540480852127075\n",
      "epoch 354 runtime = 3.7570717334747314\n",
      "start epoch 355\n",
      "epoch 355: running loss = 3.2386624813079834\n",
      "epoch 355: running loss = 35.56229043006897\n",
      "epoch 355 runtime = 3.7226064205169678\n",
      "start epoch 356\n",
      "epoch 356: running loss = 3.155426025390625\n",
      "epoch 356: running loss = 35.53993368148804\n",
      "epoch 356 runtime = 3.7133612632751465\n",
      "start epoch 357\n",
      "epoch 357: running loss = 3.2179224491119385\n",
      "epoch 357: running loss = 35.51907444000244\n",
      "epoch 357 runtime = 3.7420692443847656\n",
      "start epoch 358\n",
      "epoch 358: running loss = 3.2592849731445312\n",
      "epoch 358: running loss = 35.52020454406738\n",
      "epoch 358 runtime = 3.7339489459991455\n",
      "start epoch 359\n",
      "epoch 359: running loss = 3.238759994506836\n",
      "epoch 359: running loss = 35.62401533126831\n",
      "epoch 359 runtime = 3.707001209259033\n",
      "start epoch 360\n",
      "epoch 360: running loss = 3.217806100845337\n",
      "epoch 360: running loss = 35.58209753036499\n",
      "epoch 360 runtime = 3.7345097064971924\n",
      "start epoch 361\n",
      "epoch 361: running loss = 3.3217735290527344\n",
      "epoch 361: running loss = 35.53998374938965\n",
      "epoch 361 runtime = 3.7289133071899414\n",
      "start epoch 362\n",
      "epoch 362: running loss = 3.217374801635742\n",
      "epoch 362: running loss = 35.66424036026001\n",
      "epoch 362 runtime = 3.7386341094970703\n",
      "start epoch 363\n",
      "epoch 363: running loss = 3.23869252204895\n",
      "epoch 363: running loss = 35.60348320007324\n",
      "epoch 363 runtime = 3.755809783935547\n",
      "start epoch 364\n",
      "epoch 364: running loss = 3.1969966888427734\n",
      "epoch 364: running loss = 35.561609745025635\n",
      "epoch 364 runtime = 3.7635443210601807\n",
      "start epoch 365\n",
      "epoch 365: running loss = 3.3004262447357178\n",
      "epoch 365: running loss = 35.56075978279114\n",
      "epoch 365 runtime = 3.7292585372924805\n",
      "start epoch 366\n",
      "epoch 366: running loss = 3.175499200820923\n",
      "epoch 366: running loss = 35.45493936538696\n",
      "epoch 366 runtime = 3.7193727493286133\n",
      "start epoch 367\n",
      "epoch 367: running loss = 3.2591183185577393\n",
      "epoch 367: running loss = 35.58075308799744\n",
      "epoch 367 runtime = 3.7605628967285156\n",
      "start epoch 368\n",
      "epoch 368: running loss = 3.2384979724884033\n",
      "epoch 368: running loss = 35.58160901069641\n",
      "epoch 368 runtime = 3.7109313011169434\n",
      "start epoch 369\n",
      "epoch 369: running loss = 3.238647699356079\n",
      "epoch 369: running loss = 35.62271308898926\n",
      "epoch 369 runtime = 3.7309844493865967\n",
      "start epoch 370\n",
      "epoch 370: running loss = 3.259585380554199\n",
      "epoch 370: running loss = 35.55917978286743\n",
      "epoch 370 runtime = 3.7544798851013184\n",
      "start epoch 371\n",
      "epoch 371: running loss = 3.280423402786255\n",
      "epoch 371: running loss = 35.64305090904236\n",
      "epoch 371 runtime = 3.754554271697998\n",
      "start epoch 372\n",
      "epoch 372: running loss = 3.154920816421509\n",
      "epoch 372: running loss = 35.55947208404541\n",
      "epoch 372 runtime = 3.7602081298828125\n",
      "start epoch 373\n",
      "epoch 373: running loss = 3.300607919692993\n",
      "epoch 373: running loss = 35.683486223220825\n",
      "epoch 373 runtime = 3.719024896621704\n",
      "start epoch 374\n",
      "epoch 374: running loss = 3.2177774906158447\n",
      "epoch 374: running loss = 35.5775682926178\n",
      "epoch 374 runtime = 3.749346971511841\n",
      "start epoch 375\n",
      "epoch 375: running loss = 3.238314390182495\n",
      "epoch 375: running loss = 35.433677673339844\n",
      "epoch 375 runtime = 3.7451171875\n",
      "start epoch 376\n",
      "epoch 376: running loss = 3.2384259700775146\n",
      "epoch 376: running loss = 35.58199167251587\n",
      "epoch 376 runtime = 3.7255749702453613\n",
      "start epoch 377\n",
      "epoch 377: running loss = 3.259314775466919\n",
      "epoch 377: running loss = 35.531837463378906\n",
      "epoch 377 runtime = 3.7533156871795654\n",
      "start epoch 378\n",
      "epoch 378: running loss = 3.2175233364105225\n",
      "epoch 378: running loss = 35.54119634628296\n",
      "epoch 378 runtime = 3.7536702156066895\n",
      "start epoch 379\n",
      "epoch 379: running loss = 3.1969261169433594\n",
      "epoch 379: running loss = 35.62437963485718\n",
      "epoch 379 runtime = 3.7372279167175293\n",
      "start epoch 380\n",
      "epoch 380: running loss = 3.217926263809204\n",
      "epoch 380: running loss = 35.54166340827942\n",
      "epoch 380 runtime = 3.7320778369903564\n",
      "start epoch 381\n",
      "epoch 381: running loss = 3.1970856189727783\n",
      "epoch 381: running loss = 35.50010704994202\n",
      "epoch 381 runtime = 3.760305643081665\n",
      "start epoch 382\n",
      "epoch 382: running loss = 3.1344716548919678\n",
      "epoch 382: running loss = 35.60428833961487\n",
      "epoch 382 runtime = 3.7538814544677734\n",
      "start epoch 383\n",
      "epoch 383: running loss = 3.238447904586792\n",
      "epoch 383: running loss = 35.51872372627258\n",
      "epoch 383 runtime = 3.740647315979004\n",
      "start epoch 384\n",
      "epoch 384: running loss = 3.2179157733917236\n",
      "epoch 384: running loss = 35.454415798187256\n",
      "epoch 384 runtime = 3.727415084838867\n",
      "start epoch 385\n",
      "epoch 385: running loss = 3.217916488647461\n",
      "epoch 385: running loss = 35.66759634017944\n",
      "epoch 385 runtime = 3.7606050968170166\n",
      "start epoch 386\n",
      "epoch 386: running loss = 3.2179267406463623\n",
      "epoch 386: running loss = 35.54156422615051\n",
      "epoch 386 runtime = 3.7839200496673584\n",
      "start epoch 387\n",
      "epoch 387: running loss = 3.1762142181396484\n",
      "epoch 387: running loss = 35.58256554603577\n",
      "epoch 387 runtime = 3.7519028186798096\n",
      "start epoch 388\n",
      "epoch 388: running loss = 3.1554253101348877\n",
      "epoch 388: running loss = 35.6037061214447\n",
      "epoch 388 runtime = 3.77028226852417\n",
      "start epoch 389\n",
      "epoch 389: running loss = 3.2387330532073975\n",
      "epoch 389: running loss = 35.49967169761658\n",
      "epoch 389 runtime = 3.7281999588012695\n",
      "start epoch 390\n",
      "epoch 390: running loss = 3.2387380599975586\n",
      "epoch 390: running loss = 35.70765256881714\n",
      "epoch 390 runtime = 3.75262188911438\n",
      "start epoch 391\n",
      "epoch 391: running loss = 3.1968204975128174\n",
      "epoch 391: running loss = 35.56122064590454\n",
      "epoch 391 runtime = 3.75166916847229\n",
      "start epoch 392\n",
      "epoch 392: running loss = 3.2385494709014893\n",
      "epoch 392: running loss = 35.56132531166077\n",
      "epoch 392 runtime = 3.756556987762451\n",
      "start epoch 393\n",
      "epoch 393: running loss = 3.2587766647338867\n",
      "epoch 393: running loss = 35.51902675628662\n",
      "epoch 393 runtime = 3.743098258972168\n",
      "start epoch 394\n",
      "epoch 394: running loss = 3.2592294216156006\n",
      "epoch 394: running loss = 35.519256591796875\n",
      "epoch 394 runtime = 3.7526795864105225\n",
      "start epoch 395\n",
      "epoch 395: running loss = 3.301069498062134\n",
      "epoch 395: running loss = 35.62263464927673\n",
      "epoch 395 runtime = 3.7215187549591064\n",
      "start epoch 396\n",
      "epoch 396: running loss = 3.217926263809204\n",
      "epoch 396: running loss = 35.56202483177185\n",
      "epoch 396 runtime = 3.70247483253479\n",
      "start epoch 397\n",
      "epoch 397: running loss = 3.2804267406463623\n",
      "epoch 397: running loss = 35.500367641448975\n",
      "epoch 397 runtime = 3.71535325050354\n",
      "start epoch 398\n",
      "epoch 398: running loss = 3.1135995388031006\n",
      "epoch 398: running loss = 35.5002543926239\n",
      "epoch 398 runtime = 3.7055342197418213\n",
      "start epoch 399\n",
      "epoch 399: running loss = 3.1970932483673096\n",
      "epoch 399: running loss = 35.43679594993591\n",
      "epoch 399 runtime = 3.719111919403076\n",
      "start epoch 400\n",
      "epoch 400: running loss = 3.217926263809204\n",
      "epoch 400: running loss = 35.52018713951111\n",
      "epoch 400 runtime = 3.7587742805480957\n",
      "start epoch 401\n",
      "epoch 401: running loss = 3.280416488647461\n",
      "epoch 401: running loss = 35.4790723323822\n",
      "epoch 401 runtime = 3.765489101409912\n",
      "start epoch 402\n",
      "epoch 402: running loss = 3.1966168880462646\n",
      "epoch 402: running loss = 35.6450731754303\n",
      "epoch 402 runtime = 3.730595111846924\n",
      "start epoch 403\n",
      "epoch 403: running loss = 3.217923402786255\n",
      "epoch 403: running loss = 35.540114879608154\n",
      "epoch 403 runtime = 3.7737176418304443\n",
      "start epoch 404\n",
      "epoch 404: running loss = 3.217900514602661\n",
      "epoch 404: running loss = 35.53960394859314\n",
      "epoch 404 runtime = 3.755134344100952\n",
      "start epoch 405\n",
      "epoch 405: running loss = 3.258845090866089\n",
      "epoch 405: running loss = 35.45585322380066\n",
      "epoch 405 runtime = 3.715026617050171\n",
      "start epoch 406\n",
      "epoch 406: running loss = 3.2592945098876953\n",
      "epoch 406: running loss = 35.621702671051025\n",
      "epoch 406 runtime = 3.764292001724243\n",
      "start epoch 407\n",
      "epoch 407: running loss = 3.2386038303375244\n",
      "epoch 407: running loss = 35.582250118255615\n",
      "epoch 407 runtime = 3.7503244876861572\n",
      "start epoch 408\n",
      "epoch 408: running loss = 3.1760470867156982\n",
      "epoch 408: running loss = 35.70593976974487\n",
      "epoch 408 runtime = 3.7603282928466797\n",
      "start epoch 409\n",
      "epoch 409: running loss = 3.2593605518341064\n",
      "epoch 409: running loss = 35.66635584831238\n",
      "epoch 409 runtime = 3.7753543853759766\n",
      "start epoch 410\n",
      "epoch 410: running loss = 3.238757371902466\n",
      "epoch 410: running loss = 35.43666911125183\n",
      "epoch 410 runtime = 3.7520406246185303\n",
      "start epoch 411\n",
      "epoch 411: running loss = 3.2383031845092773\n",
      "epoch 411: running loss = 35.602423906326294\n",
      "epoch 411 runtime = 3.746755838394165\n",
      "start epoch 412\n",
      "epoch 412: running loss = 3.1970443725585938\n",
      "epoch 412: running loss = 35.58157920837402\n",
      "epoch 412 runtime = 3.758291721343994\n",
      "start epoch 413\n",
      "epoch 413: running loss = 3.2380239963531494\n",
      "epoch 413: running loss = 35.68411135673523\n",
      "epoch 413 runtime = 3.7614097595214844\n",
      "start epoch 414\n",
      "epoch 414: running loss = 3.2175588607788086\n",
      "epoch 414: running loss = 35.53915023803711\n",
      "epoch 414 runtime = 3.734426259994507\n",
      "start epoch 415\n",
      "epoch 415: running loss = 3.3003385066986084\n",
      "epoch 415: running loss = 35.60158848762512\n",
      "epoch 415 runtime = 3.7682299613952637\n",
      "start epoch 416\n",
      "epoch 416: running loss = 3.217755079269409\n",
      "epoch 416: running loss = 35.58148431777954\n",
      "epoch 416 runtime = 3.72375750541687\n",
      "start epoch 417\n",
      "epoch 417: running loss = 3.2802364826202393\n",
      "epoch 417: running loss = 35.58211040496826\n",
      "epoch 417 runtime = 3.753931760787964\n",
      "start epoch 418\n",
      "epoch 418: running loss = 3.237619161605835\n",
      "epoch 418: running loss = 35.580331802368164\n",
      "epoch 418 runtime = 3.7654712200164795\n",
      "start epoch 419\n",
      "epoch 419: running loss = 3.1970932483673096\n",
      "epoch 419: running loss = 35.58072328567505\n",
      "epoch 419 runtime = 3.7358200550079346\n",
      "start epoch 420\n",
      "epoch 420: running loss = 3.280283212661743\n",
      "epoch 420: running loss = 35.601115465164185\n",
      "epoch 420 runtime = 3.75199556350708\n",
      "start epoch 421\n",
      "epoch 421: running loss = 3.2595932483673096\n",
      "epoch 421: running loss = 35.496448040008545\n",
      "epoch 421 runtime = 3.7353763580322266\n",
      "start epoch 422\n",
      "epoch 422: running loss = 3.217487335205078\n",
      "epoch 422: running loss = 35.477383613586426\n",
      "epoch 422 runtime = 3.7459747791290283\n",
      "start epoch 423\n",
      "epoch 423: running loss = 3.259504556655884\n",
      "epoch 423: running loss = 35.54036498069763\n",
      "epoch 423 runtime = 3.7518537044525146\n",
      "start epoch 424\n",
      "epoch 424: running loss = 3.2178401947021484\n",
      "epoch 424: running loss = 35.62279391288757\n",
      "epoch 424 runtime = 3.767502784729004\n",
      "start epoch 425\n",
      "epoch 425: running loss = 3.2175018787384033\n",
      "epoch 425: running loss = 35.51905608177185\n",
      "epoch 425 runtime = 3.7410504817962646\n",
      "start epoch 426\n",
      "epoch 426: running loss = 3.2385244369506836\n",
      "epoch 426: running loss = 35.58031725883484\n",
      "epoch 426 runtime = 3.715043067932129\n",
      "start epoch 427\n",
      "epoch 427: running loss = 3.197036027908325\n",
      "epoch 427: running loss = 35.58035755157471\n",
      "epoch 427 runtime = 3.7438549995422363\n",
      "start epoch 428\n",
      "epoch 428: running loss = 3.2177703380584717\n",
      "epoch 428: running loss = 35.642321825027466\n",
      "epoch 428 runtime = 3.7353596687316895\n",
      "start epoch 429\n",
      "epoch 429: running loss = 3.1756811141967773\n",
      "epoch 429: running loss = 35.49584078788757\n",
      "epoch 429 runtime = 3.7246923446655273\n",
      "start epoch 430\n",
      "epoch 430: running loss = 3.280106544494629\n",
      "epoch 430: running loss = 35.60043978691101\n",
      "epoch 430 runtime = 3.745584011077881\n",
      "start epoch 431\n",
      "epoch 431: running loss = 3.300964593887329\n",
      "epoch 431: running loss = 35.53958797454834\n",
      "epoch 431 runtime = 3.7682173252105713\n",
      "start epoch 432\n",
      "epoch 432: running loss = 3.2593982219696045\n",
      "epoch 432: running loss = 35.539632081985474\n",
      "epoch 432 runtime = 3.722132444381714\n",
      "start epoch 433\n",
      "epoch 433: running loss = 3.2179267406463623\n",
      "epoch 433: running loss = 35.727203130722046\n",
      "epoch 433 runtime = 3.739224433898926\n",
      "start epoch 434\n",
      "epoch 434: running loss = 3.2590677738189697\n",
      "epoch 434: running loss = 35.642271757125854\n",
      "epoch 434 runtime = 3.7244653701782227\n",
      "start epoch 435\n",
      "epoch 435: running loss = 3.237917900085449\n",
      "epoch 435: running loss = 35.60015606880188\n",
      "epoch 435 runtime = 3.7188403606414795\n",
      "start epoch 436\n",
      "epoch 436: running loss = 3.196519136428833\n",
      "epoch 436: running loss = 35.5803656578064\n",
      "epoch 436 runtime = 3.7666385173797607\n",
      "start epoch 437\n",
      "epoch 437: running loss = 3.238101005554199\n",
      "epoch 437: running loss = 35.6014928817749\n",
      "epoch 437 runtime = 3.76155424118042\n",
      "start epoch 438\n",
      "epoch 438: running loss = 3.1970930099487305\n",
      "epoch 438: running loss = 35.49702024459839\n",
      "epoch 438 runtime = 3.74743390083313\n",
      "start epoch 439\n",
      "epoch 439: running loss = 3.21758770942688\n",
      "epoch 439: running loss = 35.60092329978943\n",
      "epoch 439 runtime = 3.727663516998291\n",
      "start epoch 440\n",
      "epoch 440: running loss = 3.280128240585327\n",
      "epoch 440: running loss = 35.621771574020386\n",
      "epoch 440 runtime = 3.7566633224487305\n",
      "start epoch 441\n",
      "epoch 441: running loss = 3.1966657638549805\n",
      "epoch 441: running loss = 35.51961016654968\n",
      "epoch 441 runtime = 3.729128837585449\n",
      "start epoch 442\n",
      "epoch 442: running loss = 3.259181261062622\n",
      "epoch 442: running loss = 35.517415285110474\n",
      "epoch 442 runtime = 3.72758150100708\n",
      "start epoch 443\n",
      "epoch 443: running loss = 3.2801530361175537\n",
      "epoch 443: running loss = 35.705294132232666\n",
      "epoch 443 runtime = 3.72186279296875\n",
      "start epoch 444\n",
      "epoch 444: running loss = 3.2384872436523438\n",
      "epoch 444: running loss = 35.704946517944336\n",
      "epoch 444 runtime = 3.7447750568389893\n",
      "start epoch 445\n",
      "epoch 445: running loss = 3.2584340572357178\n",
      "epoch 445: running loss = 35.600136518478394\n",
      "epoch 445 runtime = 3.732715368270874\n",
      "start epoch 446\n",
      "epoch 446: running loss = 3.238551378250122\n",
      "epoch 446: running loss = 35.49630808830261\n",
      "epoch 446 runtime = 3.7452876567840576\n",
      "start epoch 447\n",
      "epoch 447: running loss = 3.155426263809204\n",
      "epoch 447: running loss = 35.41380167007446\n",
      "epoch 447 runtime = 3.7876553535461426\n",
      "start epoch 448\n",
      "epoch 448: running loss = 3.217524528503418\n",
      "epoch 448: running loss = 35.60069537162781\n",
      "epoch 448 runtime = 3.729006290435791\n",
      "start epoch 449\n",
      "epoch 449: running loss = 3.2384140491485596\n",
      "epoch 449: running loss = 35.598716497421265\n",
      "epoch 449 runtime = 3.7134406566619873\n",
      "start epoch 450\n",
      "epoch 450: running loss = 3.2595932483673096\n",
      "epoch 450: running loss = 35.580981492996216\n",
      "epoch 450 runtime = 3.7257890701293945\n",
      "start epoch 451\n",
      "epoch 451: running loss = 3.259075403213501\n",
      "epoch 451: running loss = 35.601582050323486\n",
      "epoch 451 runtime = 3.726715326309204\n",
      "start epoch 452\n",
      "epoch 452: running loss = 3.2590959072113037\n",
      "epoch 452: running loss = 35.60076832771301\n",
      "epoch 452 runtime = 3.7342703342437744\n",
      "start epoch 453\n",
      "epoch 453: running loss = 3.175779342651367\n",
      "epoch 453: running loss = 35.47615194320679\n",
      "epoch 453 runtime = 3.7329723834991455\n",
      "start epoch 454\n",
      "epoch 454: running loss = 3.258974075317383\n",
      "epoch 454: running loss = 35.454434633255005\n",
      "epoch 454 runtime = 3.74647855758667\n",
      "start epoch 455\n",
      "epoch 455: running loss = 3.2592594623565674\n",
      "epoch 455: running loss = 35.57868814468384\n",
      "epoch 455 runtime = 3.7595155239105225\n",
      "start epoch 456\n",
      "epoch 456: running loss = 3.217259407043457\n",
      "epoch 456: running loss = 35.537407636642456\n",
      "epoch 456 runtime = 3.738426685333252\n",
      "start epoch 457\n",
      "epoch 457: running loss = 3.2178592681884766\n",
      "epoch 457: running loss = 35.642435789108276\n",
      "epoch 457 runtime = 3.726475238800049\n",
      "start epoch 458\n",
      "epoch 458: running loss = 3.259150743484497\n",
      "epoch 458: running loss = 35.51508927345276\n",
      "epoch 458 runtime = 3.7690751552581787\n",
      "start epoch 459\n",
      "epoch 459: running loss = 3.2585887908935547\n",
      "epoch 459: running loss = 35.55948209762573\n",
      "epoch 459 runtime = 3.7244057655334473\n",
      "start epoch 460\n",
      "epoch 460: running loss = 3.1956403255462646\n",
      "epoch 460: running loss = 35.55869650840759\n",
      "epoch 460 runtime = 3.742649793624878\n",
      "start epoch 461\n",
      "epoch 461: running loss = 3.2175886631011963\n",
      "epoch 461: running loss = 35.57986044883728\n",
      "epoch 461 runtime = 3.754183053970337\n",
      "start epoch 462\n",
      "epoch 462: running loss = 3.176259994506836\n",
      "epoch 462: running loss = 35.66438412666321\n",
      "epoch 462 runtime = 3.7314648628234863\n",
      "start epoch 463\n",
      "epoch 463: running loss = 3.1969845294952393\n",
      "epoch 463: running loss = 35.663569688797\n",
      "epoch 463 runtime = 3.734309196472168\n",
      "start epoch 464\n",
      "epoch 464: running loss = 3.1969244480133057\n",
      "epoch 464: running loss = 35.53811573982239\n",
      "epoch 464 runtime = 3.722688674926758\n",
      "start epoch 465\n",
      "epoch 465: running loss = 3.2377681732177734\n",
      "epoch 465: running loss = 35.63919758796692\n",
      "epoch 465 runtime = 3.7745184898376465\n",
      "start epoch 466\n",
      "epoch 466: running loss = 3.2166926860809326\n",
      "epoch 466: running loss = 35.61852145195007\n",
      "epoch 466 runtime = 3.7452712059020996\n",
      "start epoch 467\n",
      "epoch 467: running loss = 3.2387351989746094\n",
      "epoch 467: running loss = 35.57872033119202\n",
      "epoch 467 runtime = 3.7470149993896484\n",
      "start epoch 468\n",
      "epoch 468: running loss = 3.300973892211914\n",
      "epoch 468: running loss = 35.55752873420715\n",
      "epoch 468 runtime = 3.748843193054199\n",
      "start epoch 469\n",
      "epoch 469: running loss = 3.196209192276001\n",
      "epoch 469: running loss = 35.55721378326416\n",
      "epoch 469 runtime = 3.7674269676208496\n",
      "start epoch 470\n",
      "epoch 470: running loss = 3.238759994506836\n",
      "epoch 470: running loss = 35.49573516845703\n",
      "epoch 470 runtime = 3.7136919498443604\n",
      "start epoch 471\n",
      "epoch 471: running loss = 3.2381591796875\n",
      "epoch 471: running loss = 35.59855556488037\n",
      "epoch 471 runtime = 3.7765274047851562\n",
      "start epoch 472\n",
      "epoch 472: running loss = 3.1755428314208984\n",
      "epoch 472: running loss = 35.61949157714844\n",
      "epoch 472 runtime = 3.7259912490844727\n",
      "start epoch 473\n",
      "epoch 473: running loss = 3.259279251098633\n",
      "epoch 473: running loss = 35.57907676696777\n",
      "epoch 473 runtime = 3.750335931777954\n",
      "start epoch 474\n",
      "epoch 474: running loss = 3.258977174758911\n",
      "epoch 474: running loss = 35.43322968482971\n",
      "epoch 474 runtime = 3.7270894050598145\n",
      "start epoch 475\n",
      "epoch 475: running loss = 3.237236976623535\n",
      "epoch 475: running loss = 35.66073966026306\n",
      "epoch 475 runtime = 3.7397232055664062\n",
      "start epoch 476\n",
      "epoch 476: running loss = 3.259394884109497\n",
      "epoch 476: running loss = 35.661028146743774\n",
      "epoch 476 runtime = 3.724395513534546\n",
      "start epoch 477\n",
      "epoch 477: running loss = 3.2587709426879883\n",
      "epoch 477: running loss = 35.41081953048706\n",
      "epoch 477 runtime = 3.7631733417510986\n",
      "start epoch 478\n",
      "epoch 478: running loss = 3.1762592792510986\n",
      "epoch 478: running loss = 35.55755925178528\n",
      "epoch 478 runtime = 3.7099661827087402\n",
      "start epoch 479\n",
      "epoch 479: running loss = 3.1967875957489014\n",
      "epoch 479: running loss = 35.5351996421814\n",
      "epoch 479 runtime = 3.768582344055176\n",
      "start epoch 480\n",
      "epoch 480: running loss = 3.238051176071167\n",
      "epoch 480: running loss = 35.57881498336792\n",
      "epoch 480 runtime = 3.7437219619750977\n",
      "start epoch 481\n",
      "epoch 481: running loss = 3.237795114517212\n",
      "epoch 481: running loss = 35.53580164909363\n",
      "epoch 481 runtime = 3.743959903717041\n",
      "start epoch 482\n",
      "epoch 482: running loss = 3.2593891620635986\n",
      "epoch 482: running loss = 35.57625222206116\n",
      "epoch 482 runtime = 3.7692019939422607\n",
      "start epoch 483\n",
      "epoch 483: running loss = 3.238471269607544\n",
      "epoch 483: running loss = 35.57710814476013\n",
      "epoch 483 runtime = 3.7266104221343994\n",
      "start epoch 484\n",
      "epoch 484: running loss = 3.238497495651245\n",
      "epoch 484: running loss = 35.59674263000488\n",
      "epoch 484 runtime = 3.7370855808258057\n",
      "start epoch 485\n",
      "epoch 485: running loss = 3.155416250228882\n",
      "epoch 485: running loss = 35.640015840530396\n",
      "epoch 485 runtime = 3.7562060356140137\n",
      "start epoch 486\n",
      "epoch 486: running loss = 3.2176027297973633\n",
      "epoch 486: running loss = 35.556806802749634\n",
      "epoch 486 runtime = 3.7291107177734375\n",
      "start epoch 487\n",
      "epoch 487: running loss = 3.258704423904419\n",
      "epoch 487: running loss = 35.5968291759491\n",
      "epoch 487 runtime = 3.7249300479888916\n",
      "start epoch 488\n",
      "epoch 488: running loss = 3.3016350269317627\n",
      "epoch 488: running loss = 35.523988246917725\n",
      "epoch 488 runtime = 3.7307631969451904\n",
      "start epoch 489\n",
      "epoch 489: running loss = 3.2178456783294678\n",
      "epoch 489: running loss = 35.4518723487854\n",
      "epoch 489 runtime = 3.7122395038604736\n",
      "start epoch 490\n",
      "epoch 490: running loss = 3.1759910583496094\n",
      "epoch 490: running loss = 35.42478132247925\n",
      "epoch 490 runtime = 3.7568418979644775\n",
      "start epoch 491\n",
      "epoch 491: running loss = 3.196336507797241\n",
      "epoch 491: running loss = 35.24198842048645\n",
      "epoch 491 runtime = 3.7229793071746826\n",
      "start epoch 492\n",
      "epoch 492: running loss = 3.2251832485198975\n",
      "epoch 492: running loss = 34.97606563568115\n",
      "epoch 492 runtime = 3.7205708026885986\n",
      "start epoch 493\n",
      "epoch 493: running loss = 3.1581971645355225\n",
      "epoch 493: running loss = 34.86147165298462\n",
      "epoch 493 runtime = 3.728212594985962\n",
      "start epoch 494\n",
      "epoch 494: running loss = 3.127737045288086\n",
      "epoch 494: running loss = 34.453572273254395\n",
      "epoch 494 runtime = 3.7324421405792236\n",
      "start epoch 495\n",
      "epoch 495: running loss = 3.1407177448272705\n",
      "epoch 495: running loss = 33.84949445724487\n",
      "epoch 495 runtime = 3.764289617538452\n",
      "start epoch 496\n",
      "epoch 496: running loss = 2.9697535037994385\n",
      "epoch 496: running loss = 32.28629469871521\n",
      "epoch 496 runtime = 3.7445363998413086\n",
      "start epoch 497\n",
      "epoch 497: running loss = 2.800771474838257\n",
      "epoch 497: running loss = 31.279311180114746\n",
      "epoch 497 runtime = 3.769259214401245\n",
      "start epoch 498\n",
      "epoch 498: running loss = 2.800935983657837\n",
      "epoch 498: running loss = 30.285978317260742\n",
      "epoch 498 runtime = 3.7678685188293457\n",
      "start epoch 499\n",
      "epoch 499: running loss = 2.7592999935150146\n",
      "epoch 499: running loss = 29.7660551071167\n",
      "epoch 499 runtime = 3.742504835128784\n",
      "start epoch 500\n",
      "epoch 500: running loss = 2.693035125732422\n",
      "epoch 500: running loss = 29.40340566635132\n",
      "epoch 500 runtime = 3.762160301208496\n",
      "start epoch 501\n",
      "epoch 501: running loss = 2.6185081005096436\n",
      "epoch 501: running loss = 29.08334469795227\n",
      "epoch 501 runtime = 3.722623109817505\n",
      "start epoch 502\n",
      "epoch 502: running loss = 2.6937084197998047\n",
      "epoch 502: running loss = 28.6585431098938\n",
      "epoch 502 runtime = 3.7406578063964844\n",
      "start epoch 503\n",
      "epoch 503: running loss = 2.594391107559204\n",
      "epoch 503: running loss = 28.48161029815674\n",
      "epoch 503 runtime = 3.746612548828125\n",
      "start epoch 504\n",
      "epoch 504: running loss = 2.600583553314209\n",
      "epoch 504: running loss = 28.4384663105011\n",
      "epoch 504 runtime = 3.740847110748291\n",
      "start epoch 505\n",
      "epoch 505: running loss = 2.5710432529449463\n",
      "epoch 505: running loss = 28.006553173065186\n",
      "epoch 505 runtime = 3.7616145610809326\n",
      "start epoch 506\n",
      "epoch 506: running loss = 2.610252618789673\n",
      "epoch 506: running loss = 28.205671072006226\n",
      "epoch 506 runtime = 3.7639269828796387\n",
      "start epoch 507\n",
      "epoch 507: running loss = 2.5898351669311523\n",
      "epoch 507: running loss = 27.829596281051636\n",
      "epoch 507 runtime = 3.764218330383301\n",
      "start epoch 508\n",
      "epoch 508: running loss = 2.6093971729278564\n",
      "epoch 508: running loss = 27.99867057800293\n",
      "epoch 508 runtime = 3.75044322013855\n",
      "start epoch 509\n",
      "epoch 509: running loss = 2.5490710735321045\n",
      "epoch 509: running loss = 27.988027572631836\n",
      "epoch 509 runtime = 3.7666633129119873\n",
      "start epoch 510\n",
      "epoch 510: running loss = 2.5479812622070312\n",
      "epoch 510: running loss = 27.986485481262207\n",
      "epoch 510 runtime = 3.7515649795532227\n",
      "start epoch 511\n",
      "epoch 511: running loss = 2.506931781768799\n",
      "epoch 511: running loss = 27.921737909317017\n",
      "epoch 511 runtime = 3.758702278137207\n",
      "start epoch 512\n",
      "epoch 512: running loss = 2.5269510746002197\n",
      "epoch 512: running loss = 28.060667037963867\n",
      "epoch 512 runtime = 3.715996503829956\n",
      "start epoch 513\n",
      "epoch 513: running loss = 2.4447665214538574\n",
      "epoch 513: running loss = 27.894742488861084\n",
      "epoch 513 runtime = 3.738121747970581\n",
      "start epoch 514\n",
      "epoch 514: running loss = 2.567688226699829\n",
      "epoch 514: running loss = 27.75081443786621\n",
      "epoch 514 runtime = 3.742971658706665\n",
      "start epoch 515\n",
      "epoch 515: running loss = 2.474799633026123\n",
      "epoch 515: running loss = 27.744945526123047\n",
      "epoch 515 runtime = 3.709914207458496\n",
      "start epoch 516\n",
      "epoch 516: running loss = 2.5066168308258057\n",
      "epoch 516: running loss = 27.502559900283813\n",
      "epoch 516 runtime = 3.736328363418579\n",
      "start epoch 517\n",
      "epoch 517: running loss = 2.5544698238372803\n",
      "epoch 517: running loss = 27.475348234176636\n",
      "epoch 517 runtime = 3.7242977619171143\n",
      "start epoch 518\n",
      "epoch 518: running loss = 2.4251911640167236\n",
      "epoch 518: running loss = 27.432941198349\n",
      "epoch 518 runtime = 3.7360339164733887\n",
      "start epoch 519\n",
      "epoch 519: running loss = 2.5570127964019775\n",
      "epoch 519: running loss = 27.347338438034058\n",
      "epoch 519 runtime = 3.7414047718048096\n",
      "start epoch 520\n",
      "epoch 520: running loss = 2.5041420459747314\n",
      "epoch 520: running loss = 27.358168601989746\n",
      "epoch 520 runtime = 3.7587392330169678\n",
      "start epoch 521\n",
      "epoch 521: running loss = 2.5070345401763916\n",
      "epoch 521: running loss = 27.334017276763916\n",
      "epoch 521 runtime = 3.708787441253662\n",
      "start epoch 522\n",
      "epoch 522: running loss = 2.588608741760254\n",
      "epoch 522: running loss = 27.16720676422119\n",
      "epoch 522 runtime = 3.732327938079834\n",
      "start epoch 523\n",
      "epoch 523: running loss = 2.507071018218994\n",
      "epoch 523: running loss = 27.470717430114746\n",
      "epoch 523 runtime = 3.748649835586548\n",
      "start epoch 524\n",
      "epoch 524: running loss = 2.404198408126831\n",
      "epoch 524: running loss = 27.198035717010498\n",
      "epoch 524 runtime = 3.715191125869751\n",
      "start epoch 525\n",
      "epoch 525: running loss = 2.5886342525482178\n",
      "epoch 525: running loss = 27.22623372077942\n",
      "epoch 525 runtime = 3.745737075805664\n",
      "start epoch 526\n",
      "epoch 526: running loss = 2.5476295948028564\n",
      "epoch 526: running loss = 27.28572940826416\n",
      "epoch 526 runtime = 3.762681245803833\n",
      "start epoch 527\n",
      "epoch 527: running loss = 2.5682644844055176\n",
      "epoch 527: running loss = 27.134993076324463\n",
      "epoch 527 runtime = 3.708376407623291\n",
      "start epoch 528\n",
      "epoch 528: running loss = 2.4447104930877686\n",
      "epoch 528: running loss = 27.28350520133972\n",
      "epoch 528 runtime = 3.7621843814849854\n",
      "start epoch 529\n",
      "epoch 529: running loss = 2.4444916248321533\n",
      "epoch 529: running loss = 27.308244466781616\n",
      "epoch 529 runtime = 3.723708391189575\n",
      "start epoch 530\n",
      "epoch 530: running loss = 2.445244073867798\n",
      "epoch 530: running loss = 27.303412199020386\n",
      "epoch 530 runtime = 3.7468228340148926\n",
      "start epoch 531\n",
      "epoch 531: running loss = 2.5880911350250244\n",
      "epoch 531: running loss = 27.100261211395264\n",
      "epoch 531 runtime = 3.7732913494110107\n",
      "start epoch 532\n",
      "epoch 532: running loss = 2.4040567874908447\n",
      "epoch 532: running loss = 27.01717734336853\n",
      "epoch 532 runtime = 3.7443881034851074\n",
      "start epoch 533\n",
      "epoch 533: running loss = 2.4654953479766846\n",
      "epoch 533: running loss = 27.201427698135376\n",
      "epoch 533 runtime = 3.7926580905914307\n",
      "start epoch 534\n",
      "epoch 534: running loss = 2.485797882080078\n",
      "epoch 534: running loss = 27.168609142303467\n",
      "epoch 534 runtime = 3.8768351078033447\n",
      "start epoch 535\n",
      "epoch 535: running loss = 2.4655394554138184\n",
      "epoch 535: running loss = 27.020214319229126\n",
      "epoch 535 runtime = 3.8238518238067627\n",
      "start epoch 536\n",
      "epoch 536: running loss = 2.623621702194214\n",
      "epoch 536: running loss = 27.19622015953064\n",
      "epoch 536 runtime = 3.754469871520996\n",
      "start epoch 537\n",
      "epoch 537: running loss = 2.485354423522949\n",
      "epoch 537: running loss = 26.998793363571167\n",
      "epoch 537 runtime = 3.7159855365753174\n",
      "start epoch 538\n",
      "epoch 538: running loss = 2.424739122390747\n",
      "epoch 538: running loss = 27.09867238998413\n",
      "epoch 538 runtime = 3.7128045558929443\n",
      "start epoch 539\n",
      "epoch 539: running loss = 2.486506938934326\n",
      "epoch 539: running loss = 27.145981311798096\n",
      "epoch 539 runtime = 3.7383599281311035\n",
      "start epoch 540\n",
      "epoch 540: running loss = 2.4866280555725098\n",
      "epoch 540: running loss = 27.12482523918152\n",
      "epoch 540 runtime = 3.742525100708008\n",
      "start epoch 541\n",
      "epoch 541: running loss = 2.506540060043335\n",
      "epoch 541: running loss = 27.141765117645264\n",
      "epoch 541 runtime = 3.7433316707611084\n",
      "start epoch 542\n",
      "epoch 542: running loss = 2.5268707275390625\n",
      "epoch 542: running loss = 27.08050775527954\n",
      "epoch 542 runtime = 3.7351789474487305\n",
      "start epoch 543\n",
      "epoch 543: running loss = 2.5062718391418457\n",
      "epoch 543: running loss = 27.140398740768433\n",
      "epoch 543 runtime = 3.722407579421997\n",
      "start epoch 544\n",
      "epoch 544: running loss = 2.4449844360351562\n",
      "epoch 544: running loss = 27.017958402633667\n",
      "epoch 544 runtime = 3.7421088218688965\n",
      "start epoch 545\n",
      "epoch 545: running loss = 2.4855411052703857\n",
      "epoch 545: running loss = 27.118810653686523\n",
      "epoch 545 runtime = 3.728712320327759\n",
      "start epoch 546\n",
      "epoch 546: running loss = 2.3836240768432617\n",
      "epoch 546: running loss = 26.99613642692566\n",
      "epoch 546 runtime = 3.7050259113311768\n",
      "start epoch 547\n",
      "epoch 547: running loss = 2.527625560760498\n",
      "epoch 547: running loss = 27.03793716430664\n",
      "epoch 547 runtime = 3.7290427684783936\n",
      "start epoch 548\n",
      "epoch 548: running loss = 2.4240591526031494\n",
      "epoch 548: running loss = 27.016839027404785\n",
      "epoch 548 runtime = 3.7425334453582764\n",
      "start epoch 549\n",
      "epoch 549: running loss = 2.40372371673584\n",
      "epoch 549: running loss = 27.017387628555298\n",
      "epoch 549 runtime = 3.7573835849761963\n",
      "start epoch 550\n",
      "epoch 550: running loss = 2.465043783187866\n",
      "epoch 550: running loss = 26.893213272094727\n",
      "epoch 550 runtime = 3.739686965942383\n",
      "start epoch 551\n",
      "epoch 551: running loss = 2.5056424140930176\n",
      "epoch 551: running loss = 27.118308067321777\n",
      "epoch 551 runtime = 3.719726085662842\n",
      "start epoch 552\n",
      "epoch 552: running loss = 2.5064244270324707\n",
      "epoch 552: running loss = 27.057501316070557\n",
      "epoch 552 runtime = 3.740835666656494\n",
      "start epoch 553\n",
      "epoch 553: running loss = 2.547677993774414\n",
      "epoch 553: running loss = 27.22179651260376\n",
      "epoch 553 runtime = 3.74769926071167\n",
      "start epoch 554\n",
      "epoch 554: running loss = 2.4648420810699463\n",
      "epoch 554: running loss = 27.05678629875183\n",
      "epoch 554 runtime = 3.7529566287994385\n",
      "start epoch 555\n",
      "epoch 555: running loss = 2.445068597793579\n",
      "epoch 555: running loss = 27.056424617767334\n",
      "epoch 555 runtime = 3.7372775077819824\n",
      "start epoch 556\n",
      "epoch 556: running loss = 2.4449474811553955\n",
      "epoch 556: running loss = 26.99487328529358\n",
      "epoch 556 runtime = 3.7250425815582275\n",
      "start epoch 557\n",
      "epoch 557: running loss = 2.4450790882110596\n",
      "epoch 557: running loss = 27.138505220413208\n",
      "epoch 557 runtime = 3.7392468452453613\n",
      "start epoch 558\n",
      "epoch 558: running loss = 2.4860451221466064\n",
      "epoch 558: running loss = 27.015645742416382\n",
      "epoch 558 runtime = 3.7381505966186523\n",
      "start epoch 559\n",
      "epoch 559: running loss = 2.383894443511963\n",
      "epoch 559: running loss = 26.852146863937378\n",
      "epoch 559 runtime = 3.733372449874878\n",
      "start epoch 560\n",
      "epoch 560: running loss = 2.485694646835327\n",
      "epoch 560: running loss = 27.0981707572937\n",
      "epoch 560 runtime = 3.7264163494110107\n",
      "start epoch 561\n",
      "epoch 561: running loss = 2.465287446975708\n",
      "epoch 561: running loss = 27.076547861099243\n",
      "epoch 561 runtime = 3.7236218452453613\n",
      "start epoch 562\n",
      "epoch 562: running loss = 2.4444968700408936\n",
      "epoch 562: running loss = 27.040261030197144\n",
      "epoch 562 runtime = 3.7521474361419678\n",
      "start epoch 563\n",
      "epoch 563: running loss = 2.5479893684387207\n",
      "epoch 563: running loss = 26.93077254295349\n",
      "epoch 563 runtime = 3.7530269622802734\n",
      "start epoch 564\n",
      "epoch 564: running loss = 2.4649064540863037\n",
      "epoch 564: running loss = 26.97984552383423\n",
      "epoch 564 runtime = 3.752579689025879\n",
      "start epoch 565\n",
      "epoch 565: running loss = 2.4244039058685303\n",
      "epoch 565: running loss = 27.0396888256073\n",
      "epoch 565 runtime = 3.7536420822143555\n",
      "start epoch 566\n",
      "epoch 566: running loss = 2.527571201324463\n",
      "epoch 566: running loss = 26.935559034347534\n",
      "epoch 566 runtime = 3.744617462158203\n",
      "start epoch 567\n",
      "epoch 567: running loss = 2.403740167617798\n",
      "epoch 567: running loss = 26.95503807067871\n",
      "epoch 567 runtime = 3.7581589221954346\n",
      "start epoch 568\n",
      "epoch 568: running loss = 2.4444568157196045\n",
      "epoch 568: running loss = 26.79120945930481\n",
      "epoch 568 runtime = 3.745631217956543\n",
      "start epoch 569\n",
      "epoch 569: running loss = 2.486295461654663\n",
      "epoch 569: running loss = 26.89316749572754\n",
      "epoch 569 runtime = 3.7475781440734863\n",
      "start epoch 570\n",
      "epoch 570: running loss = 2.4445481300354004\n",
      "epoch 570: running loss = 26.789982557296753\n",
      "epoch 570 runtime = 3.7661447525024414\n",
      "start epoch 571\n",
      "epoch 571: running loss = 2.363022804260254\n",
      "epoch 571: running loss = 26.912723302841187\n",
      "epoch 571 runtime = 3.732795238494873\n",
      "start epoch 572\n",
      "epoch 572: running loss = 2.4653823375701904\n",
      "epoch 572: running loss = 26.95523476600647\n",
      "epoch 572 runtime = 3.7262654304504395\n",
      "start epoch 573\n",
      "epoch 573: running loss = 2.4449045658111572\n",
      "epoch 573: running loss = 26.831920623779297\n",
      "epoch 573 runtime = 3.745030164718628\n",
      "start epoch 574\n",
      "epoch 574: running loss = 2.4653232097625732\n",
      "epoch 574: running loss = 26.912651777267456\n",
      "epoch 574 runtime = 3.7495596408843994\n",
      "start epoch 575\n",
      "epoch 575: running loss = 2.4459102153778076\n",
      "epoch 575: running loss = 27.138059616088867\n",
      "epoch 575 runtime = 3.7288625240325928\n",
      "start epoch 576\n",
      "epoch 576: running loss = 2.4443259239196777\n",
      "epoch 576: running loss = 27.015449285507202\n",
      "epoch 576 runtime = 3.7559092044830322\n",
      "start epoch 577\n",
      "epoch 577: running loss = 2.5262348651885986\n",
      "epoch 577: running loss = 26.93363356590271\n",
      "epoch 577 runtime = 3.7166430950164795\n",
      "start epoch 578\n",
      "epoch 578: running loss = 2.465294122695923\n",
      "epoch 578: running loss = 26.89272975921631\n",
      "epoch 578 runtime = 3.771865129470825\n",
      "start epoch 579\n",
      "epoch 579: running loss = 2.4654343128204346\n",
      "epoch 579: running loss = 26.913467168807983\n",
      "epoch 579 runtime = 3.7489795684814453\n",
      "start epoch 580\n",
      "epoch 580: running loss = 2.4245035648345947\n",
      "epoch 580: running loss = 26.994843244552612\n",
      "epoch 580 runtime = 3.747750997543335\n",
      "start epoch 581\n",
      "epoch 581: running loss = 2.444439649581909\n",
      "epoch 581: running loss = 26.891468286514282\n",
      "epoch 581 runtime = 3.7348337173461914\n",
      "start epoch 582\n",
      "epoch 582: running loss = 2.4244024753570557\n",
      "epoch 582: running loss = 26.93271255493164\n",
      "epoch 582 runtime = 3.711920738220215\n",
      "start epoch 583\n",
      "epoch 583: running loss = 2.4447455406188965\n",
      "epoch 583: running loss = 26.912999391555786\n",
      "epoch 583 runtime = 3.725051164627075\n",
      "start epoch 584\n",
      "epoch 584: running loss = 2.444768190383911\n",
      "epoch 584: running loss = 27.056560754776\n",
      "epoch 584 runtime = 3.7376317977905273\n",
      "start epoch 585\n",
      "epoch 585: running loss = 2.424072027206421\n",
      "epoch 585: running loss = 26.9114031791687\n",
      "epoch 585 runtime = 3.7435436248779297\n",
      "start epoch 586\n",
      "epoch 586: running loss = 2.5057129859924316\n",
      "epoch 586: running loss = 26.994410753250122\n",
      "epoch 586 runtime = 3.7221908569335938\n",
      "start epoch 587\n",
      "epoch 587: running loss = 2.506267786026001\n",
      "epoch 587: running loss = 27.03522801399231\n",
      "epoch 587 runtime = 3.738719940185547\n",
      "start epoch 588\n",
      "epoch 588: running loss = 2.4444198608398438\n",
      "epoch 588: running loss = 26.95348310470581\n",
      "epoch 588 runtime = 3.741995096206665\n",
      "start epoch 589\n",
      "epoch 589: running loss = 2.4648277759552\n",
      "epoch 589: running loss = 26.850677967071533\n",
      "epoch 589 runtime = 3.7323546409606934\n",
      "start epoch 590\n",
      "epoch 590: running loss = 2.4238736629486084\n",
      "epoch 590: running loss = 26.88321566581726\n",
      "epoch 590 runtime = 3.7537474632263184\n",
      "start epoch 591\n",
      "epoch 591: running loss = 2.4446511268615723\n",
      "epoch 591: running loss = 26.975356101989746\n",
      "epoch 591 runtime = 3.7161848545074463\n",
      "start epoch 592\n",
      "epoch 592: running loss = 2.486220121383667\n",
      "epoch 592: running loss = 26.772039651870728\n",
      "epoch 592 runtime = 3.751922607421875\n",
      "start epoch 593\n",
      "epoch 593: running loss = 2.4448509216308594\n",
      "epoch 593: running loss = 26.79328179359436\n",
      "epoch 593 runtime = 3.778308868408203\n",
      "start epoch 594\n",
      "epoch 594: running loss = 2.4044692516326904\n",
      "epoch 594: running loss = 26.853377103805542\n",
      "epoch 594 runtime = 3.7813658714294434\n",
      "start epoch 595\n",
      "epoch 595: running loss = 2.4041507244110107\n",
      "epoch 595: running loss = 27.01555371284485\n",
      "epoch 595 runtime = 3.764920473098755\n",
      "start epoch 596\n",
      "epoch 596: running loss = 2.4045770168304443\n",
      "epoch 596: running loss = 26.954675436019897\n",
      "epoch 596 runtime = 3.746368885040283\n",
      "start epoch 597\n",
      "epoch 597: running loss = 2.506429433822632\n",
      "epoch 597: running loss = 26.893075466156006\n",
      "epoch 597 runtime = 3.722421646118164\n",
      "start epoch 598\n",
      "epoch 598: running loss = 2.50636625289917\n",
      "epoch 598: running loss = 26.891862154006958\n",
      "epoch 598 runtime = 3.7514142990112305\n",
      "start epoch 599\n",
      "epoch 599: running loss = 2.3833117485046387\n",
      "epoch 599: running loss = 26.891574144363403\n",
      "epoch 599 runtime = 3.7392728328704834\n",
      "start epoch 600\n",
      "epoch 600: running loss = 2.5260093212127686\n",
      "epoch 600: running loss = 26.871604681015015\n",
      "epoch 600 runtime = 3.7251853942871094\n",
      "start epoch 601\n",
      "epoch 601: running loss = 2.464749574661255\n",
      "epoch 601: running loss = 26.933024406433105\n",
      "epoch 601 runtime = 3.7548885345458984\n",
      "start epoch 602\n",
      "epoch 602: running loss = 2.4853551387786865\n",
      "epoch 602: running loss = 26.974409103393555\n",
      "epoch 602 runtime = 3.7472524642944336\n",
      "start epoch 603\n",
      "epoch 603: running loss = 2.465635061264038\n",
      "epoch 603: running loss = 26.994149684906006\n",
      "epoch 603 runtime = 3.7239270210266113\n",
      "start epoch 604\n",
      "epoch 604: running loss = 2.4245264530181885\n",
      "epoch 604: running loss = 26.728699922561646\n",
      "epoch 604 runtime = 3.7494497299194336\n",
      "start epoch 605\n",
      "epoch 605: running loss = 2.4039599895477295\n",
      "epoch 605: running loss = 26.871371269226074\n",
      "epoch 605 runtime = 3.725243091583252\n",
      "start epoch 606\n",
      "epoch 606: running loss = 2.4250025749206543\n",
      "epoch 606: running loss = 26.994114637374878\n",
      "epoch 606 runtime = 3.7395150661468506\n",
      "start epoch 607\n",
      "epoch 607: running loss = 2.485377073287964\n",
      "epoch 607: running loss = 27.015605449676514\n",
      "epoch 607 runtime = 3.7612249851226807\n",
      "start epoch 608\n",
      "epoch 608: running loss = 2.464848279953003\n",
      "epoch 608: running loss = 26.8513023853302\n",
      "epoch 608 runtime = 3.7515385150909424\n",
      "start epoch 609\n",
      "epoch 609: running loss = 2.3833823204040527\n",
      "epoch 609: running loss = 26.932726621627808\n",
      "epoch 609 runtime = 3.718245267868042\n",
      "start epoch 610\n",
      "epoch 610: running loss = 2.4850780963897705\n",
      "epoch 610: running loss = 26.851022720336914\n",
      "epoch 610 runtime = 3.7377076148986816\n",
      "start epoch 611\n",
      "epoch 611: running loss = 2.383251190185547\n",
      "epoch 611: running loss = 26.912696838378906\n",
      "epoch 611 runtime = 3.722574472427368\n",
      "start epoch 612\n",
      "epoch 612: running loss = 2.465151071548462\n",
      "epoch 612: running loss = 26.891582012176514\n",
      "epoch 612 runtime = 3.712766647338867\n",
      "start epoch 613\n",
      "epoch 613: running loss = 2.464984178543091\n",
      "epoch 613: running loss = 26.87205696105957\n",
      "epoch 613 runtime = 3.7509849071502686\n",
      "start epoch 614\n",
      "epoch 614: running loss = 2.383695125579834\n",
      "epoch 614: running loss = 26.70755624771118\n",
      "epoch 614 runtime = 3.730686902999878\n",
      "start epoch 615\n",
      "epoch 615: running loss = 2.3424603939056396\n",
      "epoch 615: running loss = 26.748328685760498\n",
      "epoch 615 runtime = 3.759840488433838\n",
      "start epoch 616\n",
      "epoch 616: running loss = 2.485173463821411\n",
      "epoch 616: running loss = 26.968265056610107\n",
      "epoch 616 runtime = 3.7426884174346924\n",
      "start epoch 617\n",
      "epoch 617: running loss = 2.424403667449951\n",
      "epoch 617: running loss = 26.70799469947815\n",
      "epoch 617 runtime = 3.761155843734741\n",
      "start epoch 618\n",
      "epoch 618: running loss = 2.485718011856079\n",
      "epoch 618: running loss = 26.932899713516235\n",
      "epoch 618 runtime = 3.7180306911468506\n",
      "start epoch 619\n",
      "epoch 619: running loss = 2.4648447036743164\n",
      "epoch 619: running loss = 26.904345512390137\n",
      "epoch 619 runtime = 3.7043070793151855\n",
      "start epoch 620\n",
      "epoch 620: running loss = 2.4039230346679688\n",
      "epoch 620: running loss = 26.874873399734497\n",
      "epoch 620 runtime = 3.7862846851348877\n",
      "start epoch 621\n",
      "epoch 621: running loss = 2.4242918491363525\n",
      "epoch 621: running loss = 26.99513578414917\n",
      "epoch 621 runtime = 3.739553689956665\n",
      "start epoch 622\n",
      "epoch 622: running loss = 2.40381121635437\n",
      "epoch 622: running loss = 26.913691759109497\n",
      "epoch 622 runtime = 3.77250337600708\n",
      "start epoch 623\n",
      "epoch 623: running loss = 2.588371992111206\n",
      "epoch 623: running loss = 26.931072473526\n",
      "epoch 623 runtime = 3.7449238300323486\n",
      "start epoch 624\n",
      "epoch 624: running loss = 2.4036216735839844\n",
      "epoch 624: running loss = 26.83028483390808\n",
      "epoch 624 runtime = 3.755887508392334\n",
      "start epoch 625\n",
      "epoch 625: running loss = 2.3628835678100586\n",
      "epoch 625: running loss = 26.68700647354126\n",
      "epoch 625 runtime = 3.75647234916687\n",
      "start epoch 626\n",
      "epoch 626: running loss = 2.4449551105499268\n",
      "epoch 626: running loss = 26.8707754611969\n",
      "epoch 626 runtime = 3.7115349769592285\n",
      "start epoch 627\n",
      "epoch 627: running loss = 2.4653749465942383\n",
      "epoch 627: running loss = 26.79002285003662\n",
      "epoch 627 runtime = 3.730257749557495\n",
      "start epoch 628\n",
      "epoch 628: running loss = 2.383380889892578\n",
      "epoch 628: running loss = 26.850146532058716\n",
      "epoch 628 runtime = 3.743162155151367\n",
      "start epoch 629\n",
      "epoch 629: running loss = 2.444471836090088\n",
      "epoch 629: running loss = 26.953683614730835\n",
      "epoch 629 runtime = 3.744968891143799\n",
      "start epoch 630\n",
      "epoch 630: running loss = 2.4647881984710693\n",
      "epoch 630: running loss = 26.912203550338745\n",
      "epoch 630 runtime = 3.7521371841430664\n",
      "start epoch 631\n",
      "epoch 631: running loss = 2.4038026332855225\n",
      "epoch 631: running loss = 26.87144184112549\n",
      "epoch 631 runtime = 3.7490859031677246\n",
      "start epoch 632\n",
      "epoch 632: running loss = 2.48526930809021\n",
      "epoch 632: running loss = 26.80870771408081\n",
      "epoch 632 runtime = 3.7517950534820557\n",
      "start epoch 633\n",
      "epoch 633: running loss = 2.3833553791046143\n",
      "epoch 633: running loss = 26.810128450393677\n",
      "epoch 633 runtime = 3.7742369174957275\n",
      "start epoch 634\n",
      "epoch 634: running loss = 2.546522378921509\n",
      "epoch 634: running loss = 26.870363473892212\n",
      "epoch 634 runtime = 3.758122682571411\n",
      "start epoch 635\n",
      "epoch 635: running loss = 2.485887289047241\n",
      "epoch 635: running loss = 26.706854820251465\n",
      "epoch 635 runtime = 3.7435142993927\n",
      "start epoch 636\n",
      "epoch 636: running loss = 2.4448258876800537\n",
      "epoch 636: running loss = 26.891961097717285\n",
      "epoch 636 runtime = 3.720637559890747\n",
      "start epoch 637\n",
      "epoch 637: running loss = 2.383168935775757\n",
      "epoch 637: running loss = 26.99388861656189\n",
      "epoch 637 runtime = 3.7498443126678467\n",
      "start epoch 638\n",
      "epoch 638: running loss = 2.4833035469055176\n",
      "epoch 638: running loss = 26.86929225921631\n",
      "epoch 638 runtime = 3.725466251373291\n",
      "start epoch 639\n",
      "epoch 639: running loss = 2.4849913120269775\n",
      "epoch 639: running loss = 26.87129020690918\n",
      "epoch 639 runtime = 3.7480452060699463\n",
      "start epoch 640\n",
      "epoch 640: running loss = 2.5054023265838623\n",
      "epoch 640: running loss = 26.830175399780273\n",
      "epoch 640 runtime = 3.7230358123779297\n",
      "start epoch 641\n",
      "epoch 641: running loss = 2.3424789905548096\n",
      "epoch 641: running loss = 26.830631017684937\n",
      "epoch 641 runtime = 3.7094459533691406\n",
      "start epoch 642\n",
      "epoch 642: running loss = 2.4451959133148193\n",
      "epoch 642: running loss = 26.728140592575073\n",
      "epoch 642 runtime = 3.725663661956787\n",
      "start epoch 643\n",
      "epoch 643: running loss = 2.5059566497802734\n",
      "epoch 643: running loss = 26.851433277130127\n",
      "epoch 643 runtime = 3.7129690647125244\n",
      "start epoch 644\n",
      "epoch 644: running loss = 2.445164680480957\n",
      "epoch 644: running loss = 26.70846462249756\n",
      "epoch 644 runtime = 3.7372348308563232\n",
      "start epoch 645\n",
      "epoch 645: running loss = 2.3833889961242676\n",
      "epoch 645: running loss = 26.871198415756226\n",
      "epoch 645 runtime = 3.754509449005127\n",
      "start epoch 646\n",
      "epoch 646: running loss = 2.403536796569824\n",
      "epoch 646: running loss = 26.972940921783447\n",
      "epoch 646 runtime = 3.743520975112915\n",
      "start epoch 647\n",
      "epoch 647: running loss = 2.404042959213257\n",
      "epoch 647: running loss = 26.626192092895508\n",
      "epoch 647 runtime = 3.732229471206665\n",
      "start epoch 648\n",
      "epoch 648: running loss = 2.423861026763916\n",
      "epoch 648: running loss = 26.768301486968994\n",
      "epoch 648 runtime = 3.7436001300811768\n",
      "start epoch 649\n",
      "epoch 649: running loss = 2.4856135845184326\n",
      "epoch 649: running loss = 26.93154239654541\n",
      "epoch 649 runtime = 3.7538135051727295\n",
      "start epoch 650\n",
      "epoch 650: running loss = 2.4039571285247803\n",
      "epoch 650: running loss = 26.789747953414917\n",
      "epoch 650 runtime = 3.7556960582733154\n",
      "start epoch 651\n",
      "epoch 651: running loss = 2.403676748275757\n",
      "epoch 651: running loss = 26.850043773651123\n",
      "epoch 651 runtime = 3.723085641860962\n",
      "start epoch 652\n",
      "epoch 652: running loss = 2.485461473464966\n",
      "epoch 652: running loss = 26.91022276878357\n",
      "epoch 652 runtime = 3.7269413471221924\n",
      "start epoch 653\n",
      "epoch 653: running loss = 2.4036712646484375\n",
      "epoch 653: running loss = 26.933271169662476\n",
      "epoch 653 runtime = 3.729442596435547\n",
      "start epoch 654\n",
      "epoch 654: running loss = 2.3831818103790283\n",
      "epoch 654: running loss = 26.81062936782837\n",
      "epoch 654 runtime = 3.74700665473938\n",
      "start epoch 655\n",
      "epoch 655: running loss = 2.423938751220703\n",
      "epoch 655: running loss = 26.830098390579224\n",
      "epoch 655 runtime = 3.737363338470459\n",
      "start epoch 656\n",
      "epoch 656: running loss = 2.4244773387908936\n",
      "epoch 656: running loss = 26.768811464309692\n",
      "epoch 656 runtime = 3.72912859916687\n",
      "start epoch 657\n",
      "epoch 657: running loss = 2.424062490463257\n",
      "epoch 657: running loss = 26.871742486953735\n",
      "epoch 657 runtime = 3.7726311683654785\n",
      "start epoch 658\n",
      "epoch 658: running loss = 2.3633928298950195\n",
      "epoch 658: running loss = 26.849598169326782\n",
      "epoch 658 runtime = 3.7522809505462646\n",
      "start epoch 659\n",
      "epoch 659: running loss = 2.444446325302124\n",
      "epoch 659: running loss = 26.869886875152588\n",
      "epoch 659 runtime = 3.71872878074646\n",
      "start epoch 660\n",
      "epoch 660: running loss = 2.3832199573516846\n",
      "epoch 660: running loss = 26.686922311782837\n",
      "epoch 660 runtime = 3.7492096424102783\n",
      "start epoch 661\n",
      "epoch 661: running loss = 2.44425368309021\n",
      "epoch 661: running loss = 26.788367986679077\n",
      "epoch 661 runtime = 3.735025644302368\n",
      "start epoch 662\n",
      "epoch 662: running loss = 2.3841381072998047\n",
      "epoch 662: running loss = 26.74808979034424\n",
      "epoch 662 runtime = 3.7500851154327393\n",
      "start epoch 663\n",
      "epoch 663: running loss = 2.3628313541412354\n",
      "epoch 663: running loss = 26.849892377853394\n",
      "epoch 663 runtime = 3.7150521278381348\n",
      "start epoch 664\n",
      "epoch 664: running loss = 2.465822458267212\n",
      "epoch 664: running loss = 26.829861640930176\n",
      "epoch 664 runtime = 3.747718334197998\n",
      "start epoch 665\n",
      "epoch 665: running loss = 2.464851140975952\n",
      "epoch 665: running loss = 26.93215847015381\n",
      "epoch 665 runtime = 3.7560956478118896\n",
      "start epoch 666\n",
      "epoch 666: running loss = 2.4041361808776855\n",
      "epoch 666: running loss = 26.807392120361328\n",
      "epoch 666 runtime = 3.7440528869628906\n",
      "start epoch 667\n",
      "epoch 667: running loss = 2.5268874168395996\n",
      "epoch 667: running loss = 27.030595302581787\n",
      "epoch 667 runtime = 3.76127290725708\n",
      "start epoch 668\n",
      "epoch 668: running loss = 2.4241459369659424\n",
      "epoch 668: running loss = 26.810105800628662\n",
      "epoch 668 runtime = 3.7155966758728027\n",
      "start epoch 669\n",
      "epoch 669: running loss = 2.4238247871398926\n",
      "epoch 669: running loss = 26.830631971359253\n",
      "epoch 669 runtime = 3.7364554405212402\n",
      "start epoch 670\n",
      "epoch 670: running loss = 2.4648549556732178\n",
      "epoch 670: running loss = 26.788915872573853\n",
      "epoch 670 runtime = 3.7314255237579346\n",
      "start epoch 671\n",
      "epoch 671: running loss = 2.4036247730255127\n",
      "epoch 671: running loss = 26.870397567749023\n",
      "epoch 671 runtime = 3.73809552192688\n",
      "start epoch 672\n",
      "epoch 672: running loss = 2.404430627822876\n",
      "epoch 672: running loss = 26.80950903892517\n",
      "epoch 672 runtime = 3.737985610961914\n",
      "start epoch 673\n",
      "epoch 673: running loss = 2.4039971828460693\n",
      "epoch 673: running loss = 26.912221670150757\n",
      "epoch 673 runtime = 3.7579877376556396\n",
      "start epoch 674\n",
      "epoch 674: running loss = 2.44530987739563\n",
      "epoch 674: running loss = 26.789046049118042\n",
      "epoch 674 runtime = 3.7128779888153076\n",
      "start epoch 675\n",
      "epoch 675: running loss = 2.423895835876465\n",
      "epoch 675: running loss = 26.74737811088562\n",
      "epoch 675 runtime = 3.7483198642730713\n",
      "start epoch 676\n",
      "epoch 676: running loss = 2.3832199573516846\n",
      "epoch 676: running loss = 26.807623386383057\n",
      "epoch 676 runtime = 3.7640933990478516\n",
      "start epoch 677\n",
      "epoch 677: running loss = 2.3628599643707275\n",
      "epoch 677: running loss = 26.810166835784912\n",
      "epoch 677 runtime = 3.7564573287963867\n",
      "start epoch 678\n",
      "epoch 678: running loss = 2.4239606857299805\n",
      "epoch 678: running loss = 26.911495685577393\n",
      "epoch 678 runtime = 3.7228286266326904\n",
      "start epoch 679\n",
      "epoch 679: running loss = 2.4036214351654053\n",
      "epoch 679: running loss = 26.72657871246338\n",
      "epoch 679 runtime = 3.726494789123535\n",
      "start epoch 680\n",
      "epoch 680: running loss = 2.3832719326019287\n",
      "epoch 680: running loss = 26.747700691223145\n",
      "epoch 680 runtime = 3.721087694168091\n",
      "start epoch 681\n",
      "epoch 681: running loss = 2.5471062660217285\n",
      "epoch 681: running loss = 26.931468725204468\n",
      "epoch 681 runtime = 3.7208328247070312\n",
      "start epoch 682\n",
      "epoch 682: running loss = 2.485873222351074\n",
      "epoch 682: running loss = 26.78976011276245\n",
      "epoch 682 runtime = 3.7574243545532227\n",
      "start epoch 683\n",
      "epoch 683: running loss = 2.4448459148406982\n",
      "epoch 683: running loss = 26.788575887680054\n",
      "epoch 683 runtime = 3.7308874130249023\n",
      "start epoch 684\n",
      "epoch 684: running loss = 2.424508571624756\n",
      "epoch 684: running loss = 26.809874057769775\n",
      "epoch 684 runtime = 3.717377185821533\n",
      "start epoch 685\n",
      "epoch 685: running loss = 2.4448018074035645\n",
      "epoch 685: running loss = 26.789448261260986\n",
      "epoch 685 runtime = 3.7188422679901123\n",
      "start epoch 686\n",
      "epoch 686: running loss = 2.404069185256958\n",
      "epoch 686: running loss = 26.68778085708618\n",
      "epoch 686 runtime = 3.7066733837127686\n",
      "start epoch 687\n",
      "epoch 687: running loss = 2.362866163253784\n",
      "epoch 687: running loss = 26.808762788772583\n",
      "epoch 687 runtime = 3.7455031871795654\n",
      "start epoch 688\n",
      "epoch 688: running loss = 2.42411208152771\n",
      "epoch 688: running loss = 26.74841046333313\n",
      "epoch 688 runtime = 3.7300124168395996\n",
      "start epoch 689\n",
      "epoch 689: running loss = 2.464714765548706\n",
      "epoch 689: running loss = 26.768601655960083\n",
      "epoch 689 runtime = 3.7405848503112793\n",
      "start epoch 690\n",
      "epoch 690: running loss = 2.3831746578216553\n",
      "epoch 690: running loss = 26.769005298614502\n",
      "epoch 690 runtime = 3.7015531063079834\n",
      "start epoch 691\n",
      "epoch 691: running loss = 2.4245798587799072\n",
      "epoch 691: running loss = 26.83033013343811\n",
      "epoch 691 runtime = 3.7072062492370605\n",
      "start epoch 692\n",
      "epoch 692: running loss = 2.48539662361145\n",
      "epoch 692: running loss = 26.829368352890015\n",
      "epoch 692 runtime = 3.7210259437561035\n",
      "start epoch 693\n",
      "epoch 693: running loss = 2.403552293777466\n",
      "epoch 693: running loss = 26.78868341445923\n",
      "epoch 693 runtime = 3.734778642654419\n",
      "start epoch 694\n",
      "epoch 694: running loss = 2.424508810043335\n",
      "epoch 694: running loss = 26.768691539764404\n",
      "epoch 694 runtime = 3.7131543159484863\n",
      "start epoch 695\n",
      "epoch 695: running loss = 2.506182909011841\n",
      "epoch 695: running loss = 26.628395318984985\n",
      "epoch 695 runtime = 3.7624449729919434\n",
      "start epoch 696\n",
      "epoch 696: running loss = 2.383298635482788\n",
      "epoch 696: running loss = 26.86839199066162\n",
      "epoch 696 runtime = 3.7451109886169434\n",
      "start epoch 697\n",
      "epoch 697: running loss = 2.444324493408203\n",
      "epoch 697: running loss = 26.818071126937866\n",
      "epoch 697 runtime = 3.7231760025024414\n",
      "start epoch 698\n",
      "epoch 698: running loss = 2.464874267578125\n",
      "epoch 698: running loss = 26.70977473258972\n",
      "epoch 698 runtime = 3.735328197479248\n",
      "start epoch 699\n",
      "epoch 699: running loss = 2.4446182250976562\n",
      "epoch 699: running loss = 26.750187397003174\n",
      "epoch 699 runtime = 3.7252774238586426\n",
      "Total training time = 2614.213509082794\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    params = {'batch_size' : 48, 'shuffle': True, 'num_workers' : 8, 'pin_memory' : True}\n",
    "else:\n",
    "    params = {'batch_size' : 48, 'shuffle': True, 'num_workers' : 8}\n",
    "    \n",
    "\n",
    "max_epochs = 700 \n",
    "data_loader = DataLoader(train_asl_dataset, **params)\n",
    "model = AslNNModel()\n",
    "# move to GPU\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# batch_size, num_channels, w, h\n",
    "#random_data = torch.rand((1, 1, 256, 256))\n",
    "\n",
    "#result = model(random_data)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#imgs, labels = (next(iter(data_loader)))\n",
    "start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"start epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    for i, data in enumerate(data_loader):\n",
    "        imgs, labels = data['image'], data['y']\n",
    "        # move to GPU\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        imgs = imgs.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch {epoch}: running loss = {running_loss}\")\n",
    "    epoch_end = time.time()\n",
    "    print(f\"epoch {epoch} runtime = {epoch_end - epoch_start}\")\n",
    "end = time.time()\n",
    "print(f\"Total training time = {end - start}\")\n",
    "\n",
    "# make sure to save the model so we don't need to train again\n",
    "save = True\n",
    "if save:\n",
    "    save_path = \"./data/asl/asl_train_no_tl_same_model.pkl\"\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.named_parameters)\n",
    "\n",
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = AslNNModel()\n",
    "loaded_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_path = \"./trained_test_cnn.pkl\"\n",
    "#torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "Load the model from disk later for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = HandNNModel()\n",
    "loaded_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Report (from sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def make_stats(y, y_hat, num_classes=10):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    cm_df = pd.DataFrame(cm, columns=[str(i) for i in range(num_classes)])\n",
    "    report = classification_report(y, y_hat)\n",
    "    return cm_df, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1a7815147b0d>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16]\n",
      "Number of Misclassifications = 3\n",
      "Sample acc = 93.75\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23]\n",
      "Number of Misclassifications = 9\n",
      "Sample acc = 81.25\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2]\n",
      "Number of Misclassifications = 3\n",
      "Sample acc = 93.75\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21]\n",
      "Number of Misclassifications = 4\n",
      "Sample acc = 91.66666666666666\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13]\n",
      "Number of Misclassifications = 6\n",
      "Sample acc = 87.5\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19]\n",
      "Number of Misclassifications = 6\n",
      "Sample acc = 87.5\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11]\n",
      "Number of Misclassifications = 6\n",
      "Sample acc = 87.5\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20]\n",
      "Number of Misclassifications = 7\n",
      "Sample acc = 85.41666666666666\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14]\n",
      "Number of Misclassifications = 3\n",
      "Sample acc = 93.75\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12]\n",
      "Number of Misclassifications = 5\n",
      "Sample acc = 89.58333333333334\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  1 22  5  1  7 16 10 11 23  1 12 19  2]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  3  2  5  1  7 16 10 11 23 19 12  3  2]\n",
      "Number of Misclassifications = 4\n",
      "Sample acc = 91.66666666666666\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  1 22  5  1  7 16 10 11 23  1 12 19  2\n",
      " 22 21 14  6  3 17 22 23  2 19  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12 22 17  1  5 19 13 23 16 23  3 22 11 13 18 22 10]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  3  2  5  1  7 16 10 11 23 19 12  3  2\n",
      "  8 21 14  6  3 17  8 23  2 18  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12  8  0  5  5 19 13 23 16 23  3  2 11 13 18  3 10]\n",
      "Number of Misclassifications = 8\n",
      "Sample acc = 83.33333333333334\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  1 22  5  1  7 16 10 11 23  1 12 19  2\n",
      " 22 21 14  6  3 17 22 23  2 19  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12 22 17  1  5 19 13 23 16 23  3 22 11 13 18 22 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14  4 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 10 13 21  6  7 18  1]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  3  2  5  1  7 16 10 11 23 19 12  3  2\n",
      "  8 21 14  6  3 17  8 23  2 18  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12  8  0  5  5 19 13 23 16 23  3  2 11 13 18  3 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14 12 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 21 13 21  6  7 18  1]\n",
      "Number of Misclassifications = 2\n",
      "Sample acc = 95.83333333333334\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  1 22  5  1  7 16 10 11 23  1 12 19  2\n",
      " 22 21 14  6  3 17 22 23  2 19  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12 22 17  1  5 19 13 23 16 23  3 22 11 13 18 22 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14  4 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 10 13 21  6  7 18  1\n",
      "  3  2  7  8 18  2 24  2 19  1 10 15 23  7 11  3 17 10  5 17  0 20 21  8\n",
      " 12 15  6 20 15  6 11  6 11 21  8  6 24  2  1  5 22 10  4 17 23 10  5 17]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  3  2  5  1  7 16 10 11 23 19 12  3  2\n",
      "  8 21 14  6  3 17  8 23  2 18  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12  8  0  5  5 19 13 23 16 23  3  2 11 13 18  3 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14 12 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 21 13 21  6  7 18  1\n",
      "  3  2  7  8 18  2 24  2 19  1 10 15 23  7 11  3 17 24  5 17  0 20 21  8\n",
      " 12 15  6 20 15  6 11  6 11 21  8  6 24  2  3  5 24 10  4 20 23 10  5 17]\n",
      "Number of Misclassifications = 4\n",
      "Sample acc = 91.66666666666666\n",
      "all_y = [ 3 16 20 24 20  1 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16 11  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  6 23 12 16\n",
      " 22  5  8 14  1 11 12  8 16 14 18  8 14 12  7 22  8 22 11  2  6  5 14 18\n",
      " 18 14  2 17 11 14  7 20 12 23 23 15 14  8  0  1  7 13  3  5  3 19 21 23\n",
      " 15 23 19 24 12  6 15 18 21 14  0  2 18  6  0  4  4 23 20 21  0 23 19 22\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  1 19 19 24 13  6  3 11  7 10 21  2 11 13 23  5  0  0  6 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21  6  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21 22  3 24 15  0\n",
      "  4 21 15 13 22 15  3 15 22 12 12 15  4 22 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  4 22  6 12 19  8 19 22 16  2 13 21 13 19  4  2\n",
      "  1  7 19 10 17 19 18 23 20 11 22  7  1 24 23 12  8  5 21  1 23 20 19 19\n",
      "  4  2 20  0 15 22  0  5  4 22 16 24  7  8  0  3 22  3 14  4 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 10 13 16 12 15 16 24 20  0  2 14  8  0 11\n",
      " 16 24 17  5  7 11 22 21 12 14  1 10  3 11  7 20  5 13 20 20 22  0 20 17\n",
      " 20 16 22 11 14 12  8 18 15  2 18 23 12 18  6 18 11  5 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1  1 23 21 24  7 10 23 19\n",
      "  6  3 11 15  8 15 21 10 11 16 20 15 20 19  1 22  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 19 14 21 19  5 22 18  8  1 11  2  1 11 10 19 23 24 20 18 12 19 17 22 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  1 22  5  1  7 16 10 11 23  1 12 19  2\n",
      " 22 21 14  6  3 17 22 23  2 19  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12 22 17  1  5 19 13 23 16 23  3 22 11 13 18 22 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14  4 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 10 13 21  6  7 18  1\n",
      "  3  2  7  8 18  2 24  2 19  1 10 15 23  7 11  3 17 10  5 17  0 20 21  8\n",
      " 12 15  6 20 15  6 11  6 11 21  8  6 24  2  1  5 22 10  4 17 23 10  5 17\n",
      " 22 22 10 10  5 24 17 16 24 17  6 21  8 14  0 14 17 13 17 24 21  1 24  3\n",
      " 17 22  8 17 12 16 17 18 14 16  5  7 23 15  8 23 13  1  4  6  5  7 13 12]\n",
      "all_y_hat = [ 3 16 20 24 20  3 10  0 13  5  7 18 16  5 11  7  0  1  5  3  7 16  3  1\n",
      " 15  6  7  2 13  4 23 14  1 10 20  3 18  4 15  3 17  5 24 20  7 23 12 16\n",
      " 17 12  8 14  5 11 12  8 16 14 18  8 14 12  7 10  8 20 17  2  6  5 14 18\n",
      " 18 14  2 20 11 14  7 20 12 23 23 15 14  8  0  3  7 13  3  5  3  5 21 23\n",
      " 15 23 12 24 12  6 15 18 21 14 14  2 18  6  0  4  4 23 20 21  0 23 19 21\n",
      "  3 10  4  6  4 23  6 13  7 15  2  7  4 16  0  4  5 17 14 14 13 13 11  2\n",
      "  6 19 19 24 13  6  3 20  7 10 21  2 11 13 23  5  0  0  7 20 18  0 12  0\n",
      "  6 17 18 24 17 10 14  5 23  2  2  4  3 24 17 21 15  4 18  6 11 14 21 21\n",
      " 15  3  0 17 24  3 15  4 19 17  6  5 11  0  7  8 10  2 21  3  3 24 15 12\n",
      " 14 21 15 13  2 15  3 15 17 12 12 15  4 17 19 24 19  2  6  0 19  3  5 13\n",
      " 17 16 14  7 12 20 19 16  3  2  6 12 19  8 19 12 16  2 13 21 13 19  4  2\n",
      "  1  7 19 20 17 19 18 23 20 11  3  7  1 24 23 12  8  5 21 21 23 20 19 19\n",
      "  4  2 20  0 15  3  0  5  4 24 16 24  7  8  0  3 10  3 14 18 20 20 12 23\n",
      " 18 20  6 24  5 15 14  4  0  3 21 13 16 12 15 16 24  1  0  2 14  8  0 11\n",
      " 16 24 17  5  7  3  8 21 12 14  1 10  3 11  7 20 18 13 20 20 17  0 20 17\n",
      " 20 16 21 11 14 12  8 18 15  2 18 23 12 18  6  7 11 19 18  1  8 15  2 20\n",
      "  1 20 11  3 17 11  4 21 16 13 15  3  6  6 17  1 14 23 21 24  7 10 23 19\n",
      "  6  3 11 15 19 15 21 10 11 16 20 15 20 19  1 12  8  7 19  7  7  3 13 14\n",
      "  2 13  7 19  1 18 14  8 11  2  2  0 16 20 16 10  0  6 13  8  4  1 12 15\n",
      " 18 14 21 19  5 21 18  8 20 11  2 13 11 10 19 23 24 20 18 12 19 17  3 12\n",
      " 16  2  8 16 18  4 10 12  7 21 24 21  8  4  8 11 13  8 23 16 24 21 24  4\n",
      " 10 10 13 23 17  0 17  5 24 24 12  3  2  5  1  7 16 10 11 23 19 12  3  2\n",
      "  8 21 14  6  3 17  8 23  2 18  1 19 14 18 13  0 24 16 23 14  4 21 11 21\n",
      " 12 21  7 15 15 13 13 12  8  0  5  5 19 13 23 16 23  3  2 11 13 18  3 10\n",
      "  2  8 13 16  5  3 16 21  5  8  4 12 10  4 20 12 14 24 10  6 12 14 12 18\n",
      "  8 18  3 10 10 21  0 15 24  3  2 18 20 10 18  0 16 21 13 21  6  7 18  1\n",
      "  3  2  7  8 18  2 24  2 19  1 10 15 23  7 11  3 17 24  5 17  0 20 21  8\n",
      " 12 15  6 20 15  6 11  6 11 21  8  6 24  2  3  5 24 10  4 20 23 10  5 17\n",
      "  8 24 20 10  5 24 17 16 24 17  7 21 12 14  0 14 17 13 17 24 21  1 24  3\n",
      " 17 21  8 17 12 16 17 12 14 16  4  7 23 15  8 23 13  1  0  6  5  7 13 18]\n",
      "Number of Misclassifications = 10\n",
      "Sample acc = 79.16666666666666\n",
      "Overall Accuracy = 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "total_misclass = 0\n",
    "test_data_loader = DataLoader(test_asl_dataset, **params)\n",
    "all_y = np.array([], dtype=np.uint8)\n",
    "all_y_hat = np.array([], dtype=np.uint8)\n",
    "for i, sample in enumerate(data_loader):\n",
    "    y = sample['y']\n",
    "    y = y.data.numpy()\n",
    "    images = (sample['image'])\n",
    "    images = images.float()\n",
    "    images = (sample['image'])\n",
    "    images = images.float()\n",
    "    model = model.cpu()\n",
    "    predictions = model(images)\n",
    "    predictions = predictions.cpu()\n",
    "    predictions = predictions.data.numpy()\n",
    "    y_hat = np.argmax(predictions, axis=1)\n",
    "    misclass = np.sum(np.where(y != y_hat, 1, 0))\n",
    "    total_samples += y.shape[0]\n",
    "    total_misclass += misclass\n",
    "    all_y = np.append(all_y, y)\n",
    "    all_y_hat = np.append(all_y_hat, y_hat)\n",
    "    #print(f\"all_y = {all_y}\")\n",
    "    #print(f\"all_y_hat = {all_y_hat}\")\n",
    "    print(f\"Number of Misclassifications = {misclass}\")\n",
    "    print(f\"Sample acc = {(y.shape[0]-misclass)/y.shape[0]*100}\")\n",
    "overall_acc = (total_samples - total_misclass)/total_samples\n",
    "print(f\"Overall Accuracy = {overall_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1   2   3   4   5   6   7   8   9  ...  14  15  16  17  18  19  20  \\\n",
      "0   28   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "1    0  18   0   4   0   2   1   0   0   0  ...   0   0   0   0   1   1   1   \n",
      "2    0   0  30   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "3    0   0   0  30   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "4    1   0   0   1  25   0   0   0   0   0  ...   0   0   0   1   0   0   0   \n",
      "5    0   0   0   0   1  26   0   0   0   0  ...   0   0   0   1   1   0   0   \n",
      "6    0   0   0   0   0   0  26   3   0   0  ...   1   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0  30   0   0  ...   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0  28   0  ...   0   0   0   0   1   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0  25  ...   0   0   0   0   0   2   2   \n",
      "10   0   0   0   2   0   0   0   0   0   0  ...   0   0   1   0   0   1   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   1   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0  ...  30   0   0   0   0   0   0   \n",
      "15   0   0   0   0   0   0   0   0   0   0  ...   0  30   0   0   0   0   0   \n",
      "16   1   0   0   0   0   0   0   0   0   0  ...   0   0  27   0   0   2   0   \n",
      "17   0   0   0   0   0   0   0   1   0   0  ...   0   0   0  28   0   0   0   \n",
      "18   0   0   0   1   0   1   0   0   0   0  ...   0   0   0   2  25   0   0   \n",
      "19   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0  29   0   \n",
      "20   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0  30   \n",
      "21   0   0   4   5   0   0   0   0   5   2  ...   0   0   4   0   0   1   4   \n",
      "22   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "23   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "    21  22  23  \n",
      "0    0   0   0  \n",
      "1    0   0   0  \n",
      "2    0   0   0  \n",
      "3    0   0   0  \n",
      "4    0   0   0  \n",
      "5    0   0   0  \n",
      "6    0   0   0  \n",
      "7    0   0   0  \n",
      "8    0   0   0  \n",
      "9    0   0   1  \n",
      "10   0   0   0  \n",
      "11   0   0   0  \n",
      "12   0   0   0  \n",
      "13   0   0   0  \n",
      "14   0   0   0  \n",
      "15   0   0   0  \n",
      "16   0   0   0  \n",
      "17   0   0   0  \n",
      "18   0   0   0  \n",
      "19   0   0   0  \n",
      "20   0   0   0  \n",
      "21   0   0   3  \n",
      "22   0  30   0  \n",
      "23   0   0  30  \n",
      "\n",
      "[24 rows x 24 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        30\n",
      "           1       0.95      0.60      0.73        30\n",
      "           2       0.88      1.00      0.94        30\n",
      "           3       0.70      1.00      0.82        30\n",
      "           4       0.96      0.83      0.89        30\n",
      "           5       0.90      0.87      0.88        30\n",
      "           6       0.96      0.87      0.91        30\n",
      "           7       0.88      1.00      0.94        30\n",
      "           8       0.85      0.93      0.89        30\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       1.00      0.87      0.93        30\n",
      "          12       0.78      0.97      0.87        30\n",
      "          13       0.97      1.00      0.98        30\n",
      "          14       0.91      1.00      0.95        30\n",
      "          15       0.97      1.00      0.98        30\n",
      "          16       1.00      1.00      1.00        30\n",
      "          17       0.84      0.90      0.87        30\n",
      "          18       0.85      0.93      0.89        30\n",
      "          19       0.89      0.83      0.86        30\n",
      "          20       0.81      0.97      0.88        30\n",
      "          21       0.81      1.00      0.90        30\n",
      "          22       0.00      0.00      0.00        30\n",
      "          23       1.00      1.00      1.00        30\n",
      "          24       0.88      1.00      0.94        30\n",
      "\n",
      "    accuracy                           0.89       720\n",
      "   macro avg       0.86      0.89      0.87       720\n",
      "weighted avg       0.86      0.89      0.87       720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_df, report = make_stats(all_y, all_y_hat, num_classes=24)\n",
    "#print(cm_df.to_markdown())\n",
    "print(cm_df)#.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1a7815147b0d>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Misclassifications = 26\n",
      "Sample acc = 45.83333333333333\n",
      "Number of Misclassifications = 20\n",
      "Sample acc = 58.333333333333336\n",
      "Number of Misclassifications = 15\n",
      "Sample acc = 68.75\n",
      "Number of Misclassifications = 19\n",
      "Sample acc = 60.416666666666664\n",
      "Number of Misclassifications = 24\n",
      "Sample acc = 50.0\n",
      "Overall Accuracy = 0.8428571428571429\n"
     ]
    }
   ],
   "source": [
    "test_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/test_asl.csv\", transforms=[recolor, to_tensor])\n",
    "test_data_loader = DataLoader(test_asl_dataset, **params)\n",
    "all_y = np.array([], dtype=np.uint8)\n",
    "all_y_hat = np.array([], dtype=np.uint8)\n",
    "for i, sample in enumerate(test_data_loader):\n",
    "    y = sample['y']\n",
    "    y = y.data.numpy()\n",
    "    images = (sample['image'])\n",
    "    images = images.float()\n",
    "    images = (sample['image'])\n",
    "    images = images.float()\n",
    "    model = model.cpu()\n",
    "    predictions = model(images)\n",
    "    predictions = predictions.cpu()\n",
    "    predictions = predictions.data.numpy()\n",
    "    y_hat = np.argmax(predictions, axis=1)\n",
    "    misclass = np.sum(np.where(y != y_hat, 1, 0))\n",
    "    total_samples += y.shape[0]\n",
    "    total_misclass += misclass\n",
    "    all_y = np.append(all_y, y)\n",
    "    all_y_hat = np.append(all_y_hat, y_hat)\n",
    "    #print(f\"all_y = {all_y}\")\n",
    "    #print(f\"all_y_hat = {all_y_hat}\")\n",
    "    print(f\"Number of Misclassifications = {misclass}\")\n",
    "    print(f\"Sample acc = {(y.shape[0]-misclass)/y.shape[0]*100}\")\n",
    "overall_acc = (total_samples - total_misclass)/total_samples\n",
    "print(f\"Overall Accuracy = {overall_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4  5  6  7  8  9  ...  14  15  16  17  18  19  20  21  22  23\n",
      "0   3  0  0  0  1  0  0  0  0  0  ...   0   0   0   0   1   1   0   0   0   2\n",
      "1   0  8  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   1   0   0   0   0\n",
      "2   1  0  6  0  0  0  0  0  0  0  ...   0   0   0   0   1   0   0   0   0   0\n",
      "3   0  0  0  6  0  0  0  0  0  0  ...   0   0   0   2   0   1   0   0   0   0\n",
      "4   1  0  0  2  3  0  0  0  0  0  ...   0   0   0   1   0   0   0   0   0   0\n",
      "5   0  3  0  0  0  5  0  0  0  0  ...   0   0   0   1   0   1   0   0   0   0\n",
      "6   0  0  0  0  0  0  8  1  0  0  ...   1   0   0   0   0   0   0   0   0   0\n",
      "7   0  0  0  0  0  0  1  8  0  0  ...   0   0   0   0   0   1   0   0   0   0\n",
      "8   0  0  0  0  0  0  0  0  3  0  ...   0   0   1   1   0   0   0   0   0   1\n",
      "9   0  0  0  0  0  0  0  0  0  7  ...   0   0   0   0   0   0   2   0   0   1\n",
      "10  0  0  0  1  0  0  0  0  0  1  ...   0   0   0   0   0   0   0   0   0   0\n",
      "11  0  0  0  2  0  0  0  0  0  1  ...   0   0   0   0   0   0   0   0   1   0\n",
      "12  1  0  0  1  1  1  0  0  0  0  ...   0   0   0   1   0   0   0   0   0   0\n",
      "13  2  1  0  0  1  0  0  0  0  0  ...   0   1   0   0   0   0   0   0   0   0\n",
      "14  0  0  0  0  0  0  0  0  0  0  ...   9   0   0   1   0   0   0   0   0   0\n",
      "15  1  0  0  0  0  0  0  1  0  0  ...   0   7   0   0   0   0   0   0   0   0\n",
      "16  0  1  0  0  0  0  0  0  0  0  ...   0   0   5   1   0   2   0   0   0   1\n",
      "17  1  0  0  1  0  0  0  0  0  0  ...   0   0   0   4   3   0   0   0   0   0\n",
      "18  1  0  0  1  1  0  0  0  0  0  ...   1   0   0   1   4   0   0   0   0   0\n",
      "19  0  0  0  0  0  0  0  0  0  1  ...   0   0   0   0   0   8   0   0   0   0\n",
      "20  0  0  0  1  1  0  0  0  0  2  ...   0   0   0   0   0   1   3   0   1   0\n",
      "21  0  0  0  0  0  0  0  0  1  4  ...   0   0   0   0   0   0   5   0   0   0\n",
      "22  1  0  0  0  0  0  0  2  0  0  ...   0   0   0   1   0   0   0   0   6   0\n",
      "23  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0  10\n",
      "\n",
      "[24 rows x 24 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.30      0.27        10\n",
      "           1       0.62      0.80      0.70        10\n",
      "           2       1.00      0.60      0.75        10\n",
      "           3       0.38      0.60      0.46        10\n",
      "           4       0.38      0.30      0.33        10\n",
      "           5       0.83      0.50      0.62        10\n",
      "           6       0.89      0.80      0.84        10\n",
      "           7       0.67      0.80      0.73        10\n",
      "           8       0.75      0.30      0.43        10\n",
      "          10       0.44      0.70      0.54        10\n",
      "          11       0.73      0.80      0.76        10\n",
      "          12       0.50      0.50      0.50        10\n",
      "          13       0.50      0.50      0.50        10\n",
      "          14       0.50      0.50      0.50        10\n",
      "          15       0.82      0.90      0.86        10\n",
      "          16       0.88      0.70      0.78        10\n",
      "          17       0.83      0.50      0.62        10\n",
      "          18       0.29      0.40      0.33        10\n",
      "          19       0.44      0.40      0.42        10\n",
      "          20       0.50      0.80      0.62        10\n",
      "          21       0.30      0.30      0.30        10\n",
      "          22       0.00      0.00      0.00        10\n",
      "          23       0.75      0.60      0.67        10\n",
      "          24       0.67      1.00      0.80        10\n",
      "\n",
      "    accuracy                           0.57       240\n",
      "   macro avg       0.58      0.57      0.56       240\n",
      "weighted avg       0.58      0.57      0.56       240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cm_df, report = make_stats(all_y, all_y_hat, num_classes=24)\n",
    "#print(cm_df.to_markdown())\n",
    "print(cm_df)#.to_markdown())\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
