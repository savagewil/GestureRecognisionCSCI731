{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transsexual-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import everything we need for the project\n",
    "\n",
    "%matplotlib inline\n",
    "import os, time\n",
    "import csv\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix, classification_report # Helps present results as a confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-accounting",
   "metadata": {},
   "source": [
    "## Writing CSV - ASL Dataset\n",
    "Since this dataset doesn't come with a nice csv, write one ourselves to make loading the data easier later\n",
    "Note: directly setting filepaths to the pre-binarized images so we don't need to perform this operation ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "backed-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"path_to_file\", \"GT\"]\n",
    "csv_path = \"./data/asl/asl_images.csv\"\n",
    "imagepaths = []\n",
    "fnames = []\n",
    "# these are 400x400 BW images\n",
    "root = \"./data/asl/asl_data/binary_frames_rotated\"\n",
    "\n",
    "for dirname, dirs, files in os.walk(root):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".png\"):\n",
    "            fnames.append(fname)\n",
    "        path = os.path.join(dirname, fname)\n",
    "        if path.endswith(\".png\"):\n",
    "            imagepaths.append(path)\n",
    "\n",
    "gt = [fname.split('_')[0] for fname in fnames]\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for i, fpath in enumerate(imagepaths):\n",
    "        writer.writerow([fpath, gt[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-columbia",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "To facilitate using pytorch to build our cnn, we write a custom DataLoader class. This allows for on-demand loading of images, which are used to train our cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solar-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslGestureDataset(Dataset):\n",
    "    \"\"\"Custom loader for the Kaggle Hand Detection Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image filepaths and gt classes\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.images_frame.iloc[idx, 0]\n",
    "        #print(img_name)\n",
    "        img = cv2.imread(img_name)\n",
    "        y = (self.images_frame.iloc[idx, 1])\n",
    "        # normalize to 0-26 for classes (missing j and z b/c dynamic)\n",
    "        y = ord(y)- ord('a')\n",
    "        sample = {'image' : img, 'y' : y, 'fname' : img_name}\n",
    "        \n",
    "        if len(self.transforms) > 0:\n",
    "            for _, transform in enumerate(self.transforms):\n",
    "                sample = transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quick-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Used to rescale an image to a given size. Useful for the CNN\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size after rescaling. If tuple, output is matched to output_size.\n",
    "        If int smaller of width/height is matched to output_size, keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h // w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w // h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        img = cv2.resize(image, (new_h, new_w))\n",
    "        #print(f\"after resize: {img.shape}\")\n",
    "        return {'image' : img, 'y' : y, 'fname' : sample['fname']}\n",
    "    \n",
    "class Recolor(object):\n",
    "    \"\"\"Used to recolor an image using cv2\n",
    "    \n",
    "    Args:\n",
    "        flag (cv2.COLOR_): color to swap to\n",
    "    \"\"\"\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        #print(f\"before recolor: {image.shape}\")\n",
    "        \n",
    "        # cvtColor to gray drops the damned channel dimension but we need it\n",
    "        img_cvt = cv2.cvtColor(image, self.color)\n",
    "        # fucking hack an extra dim to appease pytorch's bitchass\n",
    "        img_cvt = np.expand_dims(img_cvt, axis=-1)\n",
    "        #print(f\"after exansion: {img_cvt.shape}\")\n",
    "        return {'image' : img_cvt, 'y' : y, 'fname' : sample['fname']}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays to pytorch Tensors\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        y = sample['y']\n",
    "        image = sample['image']\n",
    "        \n",
    "        # swap color axis b/c \n",
    "        # numpy img: H x W x C\n",
    "        # torch img: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image' : torch.from_numpy(image), 'y' : y, 'fname' : sample['fname']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dietary-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_class_str(pred):\n",
    "    classes = {0 : \"palm\", 1 : \"L\", 2 : \"fist\", 3 : \"fist_moved\", 4 : \"thumb\", 5 : \"index\", 6 : \"ok\", 7 : \"palm_moved\", 8 : \"c\", 9 : \"down\"}\n",
    "    return classes[pred]\n",
    "\n",
    "def classify_arbitrary_image(model, img):\n",
    "    img_type = type(img)\n",
    "    print(img_type)\n",
    "    if img_type == torch.Tensor:\n",
    "        print(\"tensor\")\n",
    "        img = img.float()\n",
    "        img = img.unsqueeze(1)\n",
    "    elif img_type == np.ndarray:\n",
    "        print(\"np array\")\n",
    "        img = np.expand_dims(img, 1)\n",
    "    else:\n",
    "        print(\"error: something other than a torch.Tensor or an np.ndarray was passed as img\")\n",
    "    prediction = model(img)\n",
    "    prediction = prediction.data.numpy()\n",
    "    y_hat = np.argmax(prediction, axis=1)\n",
    "    return prediction_to_class_str(y_hat[0])\n",
    "\n",
    "def classify_many_images(model, imgs):\n",
    "    # for now, assuming imgs is a list of images that are either np.ndarrays or torch.Tensors\n",
    "    # labels will be given back in order images were given\n",
    "    predictions = []\n",
    "    for img in imgs:\n",
    "        predictions.append(classify_arbitrary_image(model, img))\n",
    "    return predictions\n",
    "\n",
    "def get_model_acc(model, data_loader):\n",
    "    total_samples = 0\n",
    "    total_misclass = 0\n",
    "    all_y = np.array([], dtype=np.uint8)\n",
    "    all_y_hat = np.array([], dtype=np.uint8)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    all_y_hat = np.array([], dtype=np.uint8)\n",
    "    for i, sample in enumerate(train_data_loader):\n",
    "        y = sample['y']\n",
    "        y = y.data.numpy()\n",
    "        images = (sample['image'])\n",
    "        images = images.float()\n",
    "        images = (sample['image'])\n",
    "        images = images.float()\n",
    "        model = model.cpu()\n",
    "        predictions = model(images)\n",
    "        predictions = predictions.cpu()\n",
    "        predictions = predictions.data.numpy()\n",
    "        y_hat = np.argmax(predictions, axis=1)\n",
    "        misclass = np.sum(np.where(y != y_hat, 1, 0))\n",
    "        total_samples += y.shape[0]\n",
    "        total_misclass += misclass\n",
    "        all_y = np.append(all_y, y)\n",
    "        all_y_hat = np.append(all_y_hat, y_hat)\n",
    "        #print(f\"all_y = {all_y}\")\n",
    "        #print(f\"all_y_hat = {all_y_hat}\")\n",
    "        print(f\"Number of Misclassifications = {misclass}\")\n",
    "        print(f\"Sample acc = {(y.shape[0]-misclass)/y.shape[0]*100}\")\n",
    "    overall_acc = (total_samples - total_misclass)/total_samples\n",
    "    print(f\"Overall Accuracy = {overall_acc}\")\n",
    "    return all_y, all_y_hat, overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "celtic-arnold",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HandNNModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (32, 256, 256) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (252, 252, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (121, 121, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (119, 119, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (59, 59, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (57, 57, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (28, 28, 64)\n",
    "        self.fc1 = Linear(28*28*64, 128) # output shape: (28*28*64, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaptive-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AslNNModel(Module):\n",
    "    # same structure as HandNNModel, need to change dimensions\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input shape = (64, 400, 400) - (batch_size, w, h) from dataloader\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=5) # output shape: (496, 496, 32)\n",
    "        self.pool1 = MaxPool2d(2) # output shape: (198, 198, 32)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3) # output shape: (196, 196, 64)\n",
    "        self.pool2 = MaxPool2d(2) # output shape: (98, 98, 64) - torch uses floor by default\n",
    "        self.conv3 = Conv2d(64, 64, kernel_size=3) # output shape: (96, 96, 64)\n",
    "        self.pool3 = MaxPool2d(2) # output shape: (48, 48, 64)\n",
    "        self.fc1 = Linear(48*48*64, 128) # output shape: (48*48*64, 128)\n",
    "        self.fc2 = Linear(128, 26) # 24 possible output classes, but it goes up to idx 26: CUDA screams otherwise, so here we are\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.activation(self.conv1(X))\n",
    "        X = self.pool1(X)\n",
    "        X = self.activation(self.conv2(X))\n",
    "        X = self.pool2(X)\n",
    "        X = self.activation(self.conv3(X))\n",
    "        X = self.pool3(X)\n",
    "        X = torch.flatten(X, 1) # flatten with start_dim = 1\n",
    "        X = self.fc1(X)\n",
    "        X = self.fc2(X)\n",
    "        output = F.softmax(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-italian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_asl_count_df(y):\n",
    "    counts = np.zeros(26)\n",
    "    y = np.array(y)\n",
    "    for i in range(26):\n",
    "        counts[i] = np.sum(np.where(y == i, 1, 0))\n",
    "    idx = [chr(i) for i in range(97, 123)]\n",
    "    columns=[\"Count\"]\n",
    "    df = pd.DataFrame(counts, index=idx, columns=columns)\n",
    "    return df\n",
    "\n",
    "def y_as_np_arr(dataset):\n",
    "    return np.array([sample['y'] for sample in dataset])\n",
    "\n",
    "\n",
    "def make_asl_train_test_split(dataset, counts_df, split_ratio=0.75, train_csv=\"./data/asl/train_asl.csv\", test_csv=\"./data/asl/test_asl.csv\"):\n",
    "    header = [\"path_to_file\", \"GT\"]\n",
    "    train = []\n",
    "    test = []\n",
    "    np.random.seed(0)\n",
    "    train_counts, test_counts = [int(np.ceil(split_ratio*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])], [int(np.floor((1-split_ratio)*counts_df.iloc[idx, 0])) for idx in range(counts_df.shape[0])]\n",
    "    y = y_as_np_arr(dataset)\n",
    "    for idx in range(counts_df.shape[0]):\n",
    "        #curr_train_selections = []\n",
    "        #curr_test_selections = []\n",
    "        only_class_locs = np.where(y==idx)[0]\n",
    "        train_idxes = np.random.choice(only_class_locs, size=train_counts[idx], replace=False)\n",
    "        for data_idx in only_class_locs:\n",
    "            sample = dataset[data_idx]\n",
    "            if data_idx in train_idxes:\n",
    "                #curr_train_selections.append(sample['fname'])\n",
    "                train.append(sample['fname'])\n",
    "            else:\n",
    "                #curr_test_selections.append(sample['fname'])\n",
    "                test.append(sample['fname'])\n",
    "        #train.append(curr_train_selections)\n",
    "        #test.append(curr_test_selections)\n",
    "    train_lasts = [fname.split('/')[-1] for fname in train]\n",
    "    train_gt = [fname.split('_')[0] for fname in train_lasts]\n",
    "    test_lasts = [fname.split('/')[-1] for fname in test]\n",
    "    test_gt = [fname.split('_')[0] for fname in test_lasts]\n",
    "    with open(train_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(train):\n",
    "            writer.writerow([fpath, train_gt[i]])\n",
    "    with open(test_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for i, fpath in enumerate(test):\n",
    "            writer.writerow([fpath, test_gt[i]])\n",
    "    return train, test\n",
    "\n",
    "resize = Rescale((256,256))\n",
    "recolor = Recolor(cv2.COLOR_BGR2GRAY)\n",
    "to_tensor = ToTensor()\n",
    "transforms = [resize, recolor, to_tensor]\n",
    "asl_dataset = AslGestureDataset(csv_file=\"./data/asl/asl_images.csv\", transforms=[recolor, to_tensor])\n",
    "y = y_as_np_arr(asl_dataset)\n",
    "df = make_asl_count_df(y)\n",
    "#train, test = make_asl_train_test_split(asl_dataset, df)\n",
    "#print(test)\n",
    "\n",
    "#lasts = [fname.split('/')[-1] for fname in test]\n",
    "#gt = [fname.split('_')[0] for fname in lasts]\n",
    "#print(len(gt))\n",
    "#train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "#print(len(train_asl_dataset))\n",
    "#plt.imshow(np.squeeze(test[0][0]['image'].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-stuff",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Report (from sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def make_stats(y, y_hat, num_classes=10):\n",
    "    cm = confusion_matrix(y, y_hat)\n",
    "    cm_df = pd.DataFrame(cm, columns=[str(i) for i in range(num_classes)])\n",
    "    report = classification_report(y, y_hat)\n",
    "    return cm_df, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-lying",
   "metadata": {},
   "source": [
    "## Train the model using the Custom Dataloader\n",
    "Below, we will actually train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    params = {'batch_size' : 48, 'shuffle': True, 'num_workers' : 8, 'pin_memory' : True}\n",
    "else:\n",
    "    params = {'batch_size' : 48, 'shuffle': True, 'num_workers' : 8}\n",
    "def train_model(model, data_loader, max_epochs, use_cuda, save_path='trained_model.pkl', save=True):\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    print(model.eval())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"start epoch {epoch}\")\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "        for i, data in enumerate(train_data_loader):\n",
    "            imgs, labels = data['image'], data['y']\n",
    "            # move to GPU\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            imgs = imgs.float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"epoch {epoch}: running loss = {running_loss}\")\n",
    "        epoch_end = time.time()\n",
    "        print(f\"epoch {epoch} runtime = {epoch_end - epoch_start}\")\n",
    "    end = time.time()\n",
    "    print(f\"Total training time = {end - start}\")\n",
    "\n",
    "    # make sure to save the model so we don't need to train again\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-occasions",
   "metadata": {},
   "source": [
    "## Train our Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-denmark",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 400 \n",
    "train_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/train_asl.csv\", transforms=[recolor, to_tensor])\n",
    "test_asl_dataset = AslGestureDataset(csv_file=\"./data/asl/test_asl.csv\", transforms=[recolor, to_tensor])\n",
    "test_data_loader = DataLoader(test_asl_dataset, **params)\n",
    "train_data_loader = DataLoader(train_asl_dataset, **params)\n",
    "asl_model = AslNNModel()\n",
    "\n",
    "save_path='./data/asl/asl_train_no_tl_same_model_tmp.pkl'\n",
    "train_model(asl_model, train_data_loader, max_epochs, use_cuda, save_path='./data/asl/asl_train_no_tl_same_model.pkl')\n",
    "asl_train_all_y, asl_train_all_y_hat, asl_train_acc = get_model_acc(asl_model, train_data_loader)\n",
    "cm_df, report = make_stats(asl_train_all_y, asl_train_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "asl_test_all_y, asl_test_all_y_hat, asl_test_acc = get_model_acc(asl_model, test_data_loader)\n",
    "cm_df, report = make_stats(asl_test_all_y, asl_test_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AslNNModel()\n",
    "loaded_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-assumption",
   "metadata": {},
   "source": [
    "## Using AlexNet\n",
    "This has already been implemented in pytorch, so we are exploring using this CNN model with our datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-league",
   "metadata": {},
   "source": [
    "### Baseline: Literally just loading AlexNet and making predicitions\n",
    "To have a baseline comparison, we will first just use AlexNet (pretrained on ImageNet dataset) to make predictions. We don't expect this to do well at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "alexnet_model = torchvision.models.alexnet(pretrained=True)\n",
    "# update model to have a single layer input, not 3 layer (AlexNet expects 3 channel RGB images as input)\n",
    "alexnet_model.features[0] = Conv2d(1, 64, kernel_size=(11,11), stride=(4,4), padding=(2,2))\n",
    "# update model to have 26 possible output classes, one per letter\n",
    "alexnet_model.classifier[6] = torch.nn.Linear(4096, 26)\n",
    "print(alexnet_model.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_asl_dataset))\n",
    "alexnet_train_all_y, alexnet_train_all_y_hat, alexnet_train_acc = get_model_acc(alexnet_model, train_data_loader)\n",
    "cm_df, report = make_stats(alexnet_train_all_y, alexnet_train_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-undergraduate",
   "metadata": {},
   "source": [
    "### Second Approach: Transfer Learning from the Pretrained AlexNet model \n",
    "This won't be a full train from scratch, but techinically transfer learning from the Pretrained AlexNet model.\n",
    "We will train AlexNet on our ASL dataset, starting from the already trained AlexNet model on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10 \n",
    "train_model(alexnet_model, train_data_loader, max_epochs, use_cuda, save_path='./data/asl/Asl_AdamNet_tl_test.pkl')\n",
    "alexnet_tl_train_all_y, alexnet_tl_train_all_y_hat, alexnet_tl_train_acc = get_model_acc(alexnet_model, train_data_loader)\n",
    "cm_df, report = make_stats(alexnet_tl_train_all_y, alexnet_tl_train_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)\n",
    "\n",
    "test_data_loader = DataLoader(test_asl_dataset, **params)\n",
    "alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, alexnet_tl_test_acc = get_model_acc(alexnet_model, test_data_loader)\n",
    "cm_df, report = make_stats(alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, alexnet_tl_test_acc = get_model_acc(alexnet_model, test_data_loader)\n",
    "cm_df, report = make_stats(alexnet_tl_test_all_y, alexnet_tl_test_all_y_hat, num_classes=24)\n",
    "print(cm_df.to_markdown())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-purchase",
   "metadata": {},
   "source": [
    "### Third Approach: Transfer Learning from the Kaggle Dataset\n",
    "For comparison to the TL trained AlexNet model, we will also be retraining the same style CNN as part 1 of our project, but using TL from our first model.\n",
    "In other words, we will be starting from the pretrained model on the Kaggle Dataset, and training a new model to recognize the ASL alphabet\n",
    "JK I'm not going through this headache - would basically need to redo every layer to match asl model to actually get it working, too much of a pita so not even gonna try\n",
    "We're just gonna say too hard in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-triple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
